GETTING INFO ABOUT: python

================================================================================

New article found for topic: python
URL: https://www.oreilly.com/programming/free/python-for-scientists.csp
TITLE: Python for Scientists
BODY:

        
          Get the free ebook 
        
        More and more, scientists are seeing tech seep into their work. From data collection to team management, various tools exist to make your lives easier. But, where to start? Python is growing in popularity in scientific circles, due to its simple syntax and seemingly endless libraries. This free ebook gets you started on the path to a more streamlined process. With a collection of chapters from our top scientific books, you'll learn about the various options that await you as you strengthen your computational thinking.This free ebook includes chapters from:Python for Data AnalysisEffective Computation in PhysicsBioinformatics Data SkillsPython Data Science Handbook
      

--------------------------------------------------------------------------------

New article found for topic: python
URL: https://www.oreilly.com/programming/free/python-data-for-developers.csp
TITLE: Python Data for Developers
BODY:

        
          Get the free ebook 
        
        Data is everywhere, and not just for data scientists. Developers are increasingly seeing it enter their realm, requiring new skills and problem solving. Python has emerged as a giant in the field, combining an easy-to-learn language with strong libraries and a vibrant community. If you have a programming background (in Python or otherwise), this free ebook will provide a snapshot of the landscape for you to start exploring more deeply.
      

--------------------------------------------------------------------------------


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

New article found for topic: python
URL: /article/idUSKBN1ZG16F
TITLE: 'It was going for my throat': Florida python hunters wrestle invasive snakes
BODY:
OCHOPEE, Fla. (Reuters) - Thomas Aycock’s life flashed before his eyes one night in the Everglades as a 13-foot Burmese python squeezed his arm and a leg in its coils.  Aycock, who was trying to bag the snake by himself, still recalls feeling its tail across his back. “I knew what it was doing, it was going for my throat,” said the 54-year-old Florida Army National Guard major who was able to wrestle free during that incident in the summer of 2018. “I said to myself, ‘It can’t go down like this.’” That scare has not stopped him from returning again and again to the sprawling wetland, devoting almost every spare moment to searching the thick brush and sawgrass for more snakes, as he was doing during this interview.   The state encourages hunters to capture or kill the giant, invasive south Asian snakes that are decimating local wildlife.  Dozens of hunters are prowling the Everglades during Florida’s 10-day Python Bowl, which ends Monday.  Armed with long metal hooks that resemble fireplace pokers and bags, many hunters catch the snakes and take them in live.  Those who take the most longest and heaviest pythons each will win $2,000 in cash.  Other prizes include off-road vehicles. Aycock and his fellow hunters are spending days and nights slowly creeping across the webs of levees that span the Everglades by foot, bicycle and souped-up SUV looking for the glint of an eye or the shine of brown and black scales. First found in the Everglades around the year 2000, the snakes were introduced by pet owners and possibly a snake research facility that was destroyed when Hurricane Andrew struck the region in 1992.  Florida Fish and Wildlife Conservation Commission staff bag an invasive Burmese python in the Everglades Wildlife Management Area, Florida June 23, 2019. FWC/Alicia Wellman/Handout via REUTERSThe behemoths, some of which measure more than 18 feet (5.5 m) long and weigh more than 100 pounds (45 kg), have wreaked havoc on the fragile ecosystem. A 2012 study in Everglades National Park by the United States Geological Survey found 99% fewer raccoons, 98% fewer opossums and 87% fewer bobcats. Massive snakes have even been found trying to eat alligators.  “I saw an opossum last night out on the levee and it was the first small animal I’ve seen in probably five or six months,” Aycock said. Agencies including the South Florida Water Management District and the Florida Fish & Wildlife Conservation Commission have all launched python removal programs in recent years, offering hunters hourly wages and bonuses depending on the size and weight.  According to a 2019 report, contracted python hunters brought in about 1,900 snakes since the program launched in March 2017. The success has been hard fought.  Despite their size and numbers, which some estimate in the hundreds of thousands, Aycock said it can take eight hours on average to find a snake. HUNDREDS CAUGHT From the start of the program to mid-2018, the most current data available, hunters working for both agencies spent 14,000 hours in the field yielding 1,186 snakes.  Some larger females have been found holding up to 100 eggs.  “We’re targeting removal in bird rookeries, in sensitive ecological areas, so regardless of the snakes’ population we know every one removed makes a difference,” said Kristen Sommers, the state’s wildlife impact management section leader. Yet on Wednesday night, finding even one proved impossible for Aycock.  Slideshow (10 Images)The cooler weather meant the cold-blooded serpents stayed hidden and out of sight. “Every python removed out of this ecosystem serves a purpose in restoring this ecosystem,” Aycock said. “We have a good time out here, but it’s also a mission we take seriously and are willing to work at.” Editing by Scott Malone and David GregorioOur Standards:The Thomson Reuters Trust Principles.

--------------------------------------------------------------------------------

New article found for topic: python
URL: /article/idUSFWN2CU0ZV
TITLE: BRIEF-FDA Says Applied Medical Recalls Python Embolectomy, Bard Embolectomy, And Otw Latis Cleaning Catheters
BODY:
May 12 (Reuters) - U.S. FDA: * FDA SAYS APPLIED MEDICAL RECALLS PYTHON EMBOLECTOMY, BARD EMBOLECTOMY, AND OTW LATIS CLEANING CATHETERS DUE TO RISK OF SEPARATION DURING USE * FDA- IDENTIFIED APPLIED MEDICAL’S PYTHON EMBOLECTOMY, BARD EMBOLECTOMY, AND OTW LATIS CLEANING CATHETERS RECALL AS CLASS I RECALL * FDA- GOT 3 MEDICAL DEVICE REPORTS, NO REPORTS OF DEATH/ INJURY RELATED TO PYTHON EMBOLECTOMY, BARD EMBOLECTOMY, OTW LATIS CLEANING CATHETERS RECALL Source text: (bit.ly/2SYq8NU)Our Standards:The Thomson Reuters Trust Principles.

--------------------------------------------------------------------------------

New article found for topic: python
URL: /article/idUSKBN1ZL1QG
TITLE: Monty Python star Terry Jones dies aged 77
BODY:
LONDON (Reuters) - Terry Jones, one of the British Monty Python comedy team and director of religious satire “Life of Brian”, has died at the age of 77 after a long battle with dementia, his family said on Wednesday. Born in Wales in 1942, Jones was also an author, historian and poet. He had been diagnosed in 2015 with a rare form of dementia, FTD. Jones was one of the creators of Monty Python’s Flying Circus, the British TV show that rewrote the rules of comedy with surreal sketches, characters and catchphrases, in 1969. He co-directed the team’s first film “Monty Python and the Holy Grail” with fellow Python Terry Gilliam, and directed the subsequent Life of Brian and “The Meaning of Life.” Python Michael Palin, who met Jones at Oxford University, said he was “kind, generous, supportive and passionate about living life to the full”. “He was far more than one of the funniest writer-performers of his generation, he was the complete Renaissance comedian - writer, director, presenter, historian, brilliant children’s author, and the warmest, most wonderful company you could wish to have.” Jones’ family said his work with Monty Python, books, films, television programmes, poems and other work “will live on forever, a fitting legacy to a true polymath”. Jones wrote comedy sketches with Palin in the 1960s for shows including “The Frost Report” and “Do Not Adjust Your Set” before the pair teamed up with Cambridge graduates Eric Idle, John Cleese, Graham Chapman - who died in 1989 - and U.S. film-maker Terry Gilliam to create Monty Python. One of Jones’ best-known roles was that of Brian’s mother in Life of Brian released in 1979, who screeches at worshippers from an open window: “He’s not the Messiah, he’s a very naughty boy”. Another was the hugely obese Mr Creosote who explodes in a restaurant at the end of an enormous meal after eating a “wafer-thin mint”. Slideshow (6 Images)Cleese said: “It feels strange that a man of so many talents and such endless enthusiasm, should have faded so gently away ..,” adding, in a reference to Chapman “Two down, four to go.” As well as his comedy work, Jones wrote about medieval and ancient history, including a critique of Geoffrey Chaucer’s “The Knight’s Tale”.  He made an emotional public appearance in 2016 when, just weeks after revealing his diagnosis with dementia, he received a Bafta Cymru award for his outstanding contribution to film and television, which was presented by Palin. Additional reporting by Elizabeth Howcroft and Kate Holton; editing by Stephen AddisonOur Standards:The Thomson Reuters Trust Principles.

--------------------------------------------------------------------------------

New article found for topic: python
URL: /article/idUSL8N29R45I
TITLE: Monty Python star Terry Jones dies aged 77
BODY:
LONDON (Reuters) - Terry Jones, one of the British Monty Python comedy team and director of religious satire “Life of Brian”, has died at the age of 77 after a long battle with dementia, his family said on Wednesday. Born in Wales in 1942, Jones was also an author, historian and poet. He had been diagnosed in 2015 with a rare form of dementia, FTD. Jones was one of the creators of Monty Python’s Flying Circus, the British TV show that rewrote the rules of comedy with surreal sketches, characters and catchphrases, in 1969. He co-directed the team’s first film “Monty Python and the Holy Grail” with fellow Python Terry Gilliam, and directed the subsequent Life of Brian and “The Meaning of Life.” Python Michael Palin, who met Jones at Oxford University, said he was “kind, generous, supportive and passionate about living life to the full”. “He was far more than one of the funniest writer-performers of his generation, he was the complete Renaissance comedian - writer, director, presenter, historian, brilliant children’s author, and the warmest, most wonderful company you could wish to have.” Jones’ family said his work with Monty Python, books, films, television programmes, poems and other work “will live on forever, a fitting legacy to a true polymath”. Jones wrote comedy sketches with Palin in the 1960s for shows including “The Frost Report” and “Do Not Adjust Your Set” before the pair teamed up with Cambridge graduates Eric Idle, John Cleese, Graham Chapman - who died in 1989 - and U.S. film-maker Terry Gilliam to create Monty Python. One of Jones’ best-known roles was that of Brian’s mother in Life of Brian released in 1979, who screeches at worshippers from an open window: “He’s not the Messiah, he’s a very naughty boy”. Another was the hugely obese Mr Creosote who explodes in a restaurant at the end of an enormous meal after eating a “wafer-thin mint”. Slideshow (6 Images)Cleese said: “It feels strange that a man of so many talents and such endless enthusiasm, should have faded so gently away ..,” adding, in a reference to Chapman “Two down, four to go.” As well as his comedy work, Jones wrote about medieval and ancient history, including a critique of Geoffrey Chaucer’s “The Knight’s Tale”.  He made an emotional public appearance in 2016 when, just weeks after revealing his diagnosis with dementia, he received a Bafta Cymru award for his outstanding contribution to film and television, which was presented by Palin. Additional reporting by Elizabeth Howcroft and Kate Holton; editing by Stephen AddisonOur Standards:The Thomson Reuters Trust Principles.

--------------------------------------------------------------------------------

New article found for topic: python
URL: /article/idUSS8N27T00I
TITLE: Monty Python actor Terry Jones dies aged 77 - PA Media
BODY:
LONDON, Jan 22 (Reuters) - Terry Jones, one of the British Monty Python comedy team, has died at the age of 77, PA Media said on Wednesday. Born in Wales, Jones was also a film director and historian. He had long suffered from a rare form of dementia, FTD. Reporting by Elizabeth Howcroft and Paul Sandle; editing by
Stephen AddisonOur Standards:The Thomson Reuters Trust Principles.

--------------------------------------------------------------------------------

New article found for topic: python
URL: /article/idUSKBN1YR1AP
TITLE: Brussels puppet theater adds Monty Python humor to nativity tale
BODY:
BRUSSELS (Reuters) - A Brussels puppet theater as old as Belgium itself is staging its Christmas nativity show this year with a dash of Monty Python humor added to the traditional story of Jesus’s birth. Puppeteers rehearse Christmas nativity show at the Royal Toone Theatre in central Brussels, Belgium December 19, 2019.   REUTERS/Francois Lenoir“It’s like a parody, a little bit like Monty Python’s Life of Brian. We don’t mean to be disrespectful, but...it’s not the Bible, really,” said Nicolas Geal, director of the Royal Theater Toone in the Belgian capital. “I don’t know whether I will go to heaven or hell for playing this,” he added with a smile.  Life of Brian, a 1979 film written by Britain’s Monty Python comedy group, is the satirical tale of a Jewish man born on the same day and next door to Jesus, and later mistaken for the Messiah. It drew protests over alleged blasphemy at the time. Playing off the film, the puppet theater’s nativity tale features silly foreign accents, particularly from the Three Wise Men, lots of jokes and slang often with a local flavor.  The theater was formed in 1830, the year Belgium was founded. Geal, who dubs himself Toone VIII, succeeded his father, who moved the theater to its current site in an alleyway near Brussels’ regal Grand Place in the 1960s.  Now boasting 1,400 puppets including some dating to the 19th century, the theater has put on shows ranging from Carmen to Cyrano de Bergerac, Dracula and Hamlet, geared more to an adult than child audience.  Geal said the European tradition of puppet shows dates back to the Middle Ages when the church forbade people to perform nativity scenes themselves, prompting performers to use puppets.  Toone’s marionettes, with a rod attached to the head, are like those used in Sicily where this form of puppet theater is thought to have originated, but given a Belgian twist. “What’s typical of Belgium is the fact I believe that we don’t take ourselves too seriously. Then we have this kind of surrealism,” Geal said. Brussels once had as many as 50 puppet theaters, Geal said, but almost all folded as other forms of entertainment became popular, particularly television, with only the Toone surviving. Additional reporting by Bart Biesemans and Jorrit Donner-Wittkopf; Writing by Philip Blenkinsop; Editing by Mark HeinrichOur Standards:The Thomson Reuters Trust Principles.

--------------------------------------------------------------------------------

New article found for topic: python
URL: /article/idUSL5N26Q0CH
TITLE: Monty Python fans, handkerchiefs on heads, gather to mark anniversary
BODY:
Monty Python fans dressed as the Gumbys gather in an attempt to set the world record for the largest gathering of people dressed as Gumbys as a part of the 50th anniversary of Monty Python's Flying Circus at the Roundhouse in London, Britain October 5, 2019. REUTERS/Simon Dawson   LONDON (Reuters) - Monty Python fans, sporting knotted handkerchiefs on their heads, rolled up trousers and Wellington boots, gathered in London on Saturday for a suitably silly celebration of the 50th anniversary of the comedy troupe. The costumes matched those of the Gumbys who were characters in the “Monty Python’s Flying Circus” series that first aired on BBC television on Oct. 5, 1969.  The Gumbys were noted for their ape-like posture, habit of speaking loudly and slowly, and the catchphrase “my brain hurts”. Dozens of Gumbys strutted outside the Roundhouse music venue before events to celebrate the work of Terry Gilliam, John Cleese, Eric Idle, Terry Jones, Michael Palin and the late Graham Chapman.  Organizers were hoping to set a Guinness World Record for the Largest Gathering of People Dressed as Gumbys. Editing by William Schomberg, Editing by William MacleanOur Standards:The Thomson Reuters Trust Principles.

--------------------------------------------------------------------------------

New article found for topic: python
URL: /article/idUSKBN1WJ1IF
TITLE: Pine no more! Monty Python celebrates 50 years of silliness
BODY:
FILE PHOTO: People attend the Silly Walk Parade, emulating a sketch from British comedy group Monty Python's television series to mark April Fool's day in Budapest, Hungary, April 1, 2019. REUTERS/Bernadett Szabo -/File PhotoLONDON (Reuters) - In what is billed as an “extremely silly” event, hordes of Monty Python fans will gather in full Gumby attire in London on Saturday to celebrate the British comedy troupe’s 50th anniversary. Kitted out in rubber boots, sleeveless sweaters, rolled-up trousers and with knotted handkerchiefs on their heads, they will attempt to set a Guinness World Record for the Largest Gathering of People Dressed as Gumbys. “It’s all so excitingly pointless,” said Python Terry Gilliam, who will host the event. The Gumbys - also noted for their ape-like posture, habit of speaking loudly and slowly, and the catchphrase “my brain hurts” - were recurring characters in the “Monty Python’s Flying Circus” series that first aired on BBC television on Oct. 5, 1969.  The anniversary is shaping up to be a feast of dead parrots, silly walks and singing lumberjacks served up by Gilliam, John Cleese, Eric Idle, Terry Jones, Michael Palin and the late Graham Chapman. A number of events, special screenings and shows are under way, including the release of a restored version of the Monty Python TV series. Biblical spoof “Life of Brian” and their three other feature films are also being shown. The celebrations also showcase the members’ post-Python work such as Cleese’s “Fawlty Towers”. “Python has survived because we live in an increasingly Pythonesque world,” the troupe said in a press release. “Extreme silliness seems more relevant now than it ever was.”  Writing by Angus MacSwan; editing by John StonestreetOur Standards:The Thomson Reuters Trust Principles.

--------------------------------------------------------------------------------

New article found for topic: python
URL: /article/idUSL5N26P2SI
TITLE: Pine no more! Monty Python celebrates 50 years of silliness
BODY:
FILE PHOTO: People attend the Silly Walk Parade, emulating a sketch from British comedy group Monty Python's television series to mark April Fool's day in Budapest, Hungary, April 1, 2019. REUTERS/Bernadett Szabo -/File PhotoLONDON (Reuters) - In what is billed as an “extremely silly” event, hordes of Monty Python fans will gather in full Gumby attire in London on Saturday to celebrate the British comedy troupe’s 50th anniversary. Kitted out in rubber boots, sleeveless sweaters, rolled-up trousers and with knotted handkerchiefs on their heads, they will attempt to set a Guinness World Record for the Largest Gathering of People Dressed as Gumbys. “It’s all so excitingly pointless,” said Python Terry Gilliam, who will host the event. The Gumbys - also noted for their ape-like posture, habit of speaking loudly and slowly, and the catchphrase “my brain hurts” - were recurring characters in the “Monty Python’s Flying Circus” series that first aired on BBC television on Oct. 5, 1969.  The anniversary is shaping up to be a feast of dead parrots, silly walks and singing lumberjacks served up by Gilliam, John Cleese, Eric Idle, Terry Jones, Michael Palin and the late Graham Chapman. A number of events, special screenings and shows are under way, including the release of a restored version of the Monty Python TV series. Biblical spoof “Life of Brian” and their three other feature films are also being shown. The celebrations also showcase the members’ post-Python work such as Cleese’s “Fawlty Towers”. “Python has survived because we live in an increasingly Pythonesque world,” the troupe said in a press release. “Extreme silliness seems more relevant now than it ever was.”  Writing by Angus MacSwan; editing by John StonestreetOur Standards:The Thomson Reuters Trust Principles.

--------------------------------------------------------------------------------

New article found for topic: python
URL: /article/idUSKCN11S04G
TITLE: Python in India demonstrates huge appetite
BODY:
A 20 feet rock python was caught on camera in Junagadh district of India’s western Gujarat state with a swollen stomach after it consumed an antelope on Tuesday (September 20). Residents informed authorities at Girnar Wildlife Sanctuary after they spotted the reptile lying in discomfort in a field. In view of the massive swelling of the python’s stomach, the forest authorities suspect that it gobbled up a full-grown ‘nilgai’ or blue bull. The python - unable to move now - was rescued by the forest personnel and has been put under observation. “We will keep it (python) under observation. We will release it back in the wild once it digests the antelope and the swelling subsides,” said Assistant Conservator of Forest, S.D. Tilala. A blue bull is far larger than an ideal prey for pythons and digesting the mammal could prove to be a great struggle for the reptile. When unable to digest an unusually large prey, pythons are known to regurgitate them.Our Standards:The Thomson Reuters Trust Principles.

--------------------------------------------------------------------------------


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

New article found for topic: python
URL: https://www.brookings.edu/blog/up-front/2015/12/21/the-hutchins-center-explains-budgeting-for-aging-america/
TITLE: The Hutchins Center Explains: Budgeting for aging America
The Hutchins Center Explains: Budgeting for aging America
BODY:

				
	
For decades, we have been hearing that the baby-boom generation was like a pig moving through a python–bigger than the generations before and after. 
That’s true. But that’s also a very misleading metaphor for understanding the demographic forces that are driving up federal spending: They aren’t temporary. The generation born between 1946 and 1964 is the beginning of a demographic transition that will persist for decades after the baby boomers die, the consequence of lengthening lifespans and declining fertility. Putting the federal budget on a sustainable course requires long-lasting fixes, not short-lived tweaks.  
First, a few demographic facts.
As the chart below illustrates, there was a surge in births in the U.S. at the end of World War II, a subsequent decline, and then an uptick as baby boomers began having children.

  
    

Although the population has been rising, the number of births in the U.S. the past few years has been below the peak baby-boom levels, possibly because many couples chose not to have children during bad economic times. More significant, fertility rates–roughly the number of babies born per woman during her lifetime–have fallen well below pre-baby-boom levels.

  
    

Meanwhile, Americans are living longer. In 1950, a man who made it to age 65 could expect to live until 78 and a woman until 81. Social Security’s actuaries project that a man who lived to age 65 in 2010 will reach 84 and a woman age 86.

  
    

Put all this together, and it’s clear that a growing fraction of the U.S. population will be 65 or older.   
The combination of longer life spans and lower fertility rates means the ratio of elderly (over 65) to working-age population (ages 20 to 64) is rising. As the chart below illustrates, the ratio will rise steadily as more baby boomers reach retirement age–and then it levels off.  

  
    



	Related Books
	
			
			

	
		
							
									
					
	
	
						Diversity Explosion
					
									
						By William H. Frey					
													2018
							
				

		
			
			

	
		
							
									
					
	
	
						Society at a Glance 2014
					
									
						By Organization for Economic Cooperation and Development OECD					
													2014
							
				

		
			
			

	
		
							
									
					
	
	
						Brookings-Wharton Papers on Urban Affairs: 2002
					
									
						Edited by William G. Gale and Janet Rothenberg Pack					
													2010
							
				

		
		

Simply put, this doesn’t look like a pig in a python.  
So what do these demographic facts portend for the federal budget?  In simple dollars and cents, the federal government spends more on the old than the young. More older Americans means more federal spending on Social Security and Medicare, the health insurance program for the elderly. On top of that, health care spending per person is likely to continue to grow faster than the overall economy.
The net result: 85 percent of the increase in federal spending that the Congressional Budget Office projects for the next 10 years, based on current policies, will go toward Social Security, Medicare and other major federal health programs, and interest on the national debt.
	
		
				

	
			
		

	
							David Wessel
		
							Director - The Hutchins Center on Fiscal and Monetary Policy					Senior Fellow - Economic Studies
					
						Twitter
		davidmwessel
			
		
			
		
			

	

	
			
		

	
							Louise Sheiner
		
							The Robert S. Kerr Senior Fellow - Economic Studies					Policy Director - The Hutchins Center on Fiscal and Monetary Policy
		
			
		
			

			
	
	Restraining future deficits and the size of the federal debt mean restraining spending on these programs or raising taxes–and probably both. One-time savings or minor tweaks won’t suffice. Nor will limiting the belt-tightening to annually appropriated spending.
The fundamental fiscal problem is not coping with the retirement of the baby boomers and then going back to budgets that resemble those of the past. The fundamental fiscal problem is that retirement of the baby boomers marks a major demographic transition for the nation, one that will require long-lived changes to benefit programs and taxes.

Editor’s Note: This post originally appeared on 
  The Wall Street Journal’s Washington Wire
 on December 18, 2015.
			

--------------------------------------------------------------------------------

New article found for topic: python
URL: https://www.brookings.edu/blog/techtank/2017/11/16/leveraging-the-disruptive-power-of-artificial-intelligence-for-fairer-opportunities/
TITLE: Leveraging the disruptive power of artificial intelligence for fairer opportunities
Leveraging the disruptive power of artificial intelligence for fairer opportunities
BODY:

				According to President Obama’s Council of Economic Advisers (CEA), approximately 3.1 million jobs will be rendered obsolete or permanently altered as a consequence of artificial intelligence technologies. Artificial intelligence (AI) will, for the foreseeable future, have a significant disruptive impact on jobs. That said, this disruption can create new opportunities if policymakers choose to harness them—including some with the potential to help address long-standing social inequities. Investing in quality training programs that deliver premium skills, such as computational analysis and cognitive thinking, provides a real opportunity to leverage AI’s disruptive power.
	
		
				

	
			
		

	
							Makada Henry-Nickie
		
							Fellow - Governance Studies
					
						Twitter
		mhnickie
			
		
			
		
			

			
	
	
AI’s disruption presents a clear challenge: competition to traditional skilled workers arising from the cross-relevance of data scientists and code engineers, who can adapt quickly to new contexts. Data analytics has become an indispensable feature of successful companies across all industries. This reality dictates that companies invest heavily in data analytics to remain competitive and profitable. Consequently, unlikely industries such as retail, banking, finance, and even agricultural firms are aggressively competing for talent with specific computational data science and programming skills. A recent IBM report expertly quantifies the scope and breadth of employers’ hiring demands, noting that “[d]emand for data-driven decision makers, such as data-enabled marketing managers, will comprise one-third of the data savvy professional job market, with a projected increase of 110,000 positions by 2020.” Herein lies a window of opportunity: the rapidly growing technical skills gap.
Investing in high-quality education and training programs is one way that policymakers proactively attempt to address the workforce challenges presented by artificial intelligence. It is essential that we make affirmative, inclusive choices to ensure that marginalized communities participate equitably in these opportunities.
Policymakers should prioritize understanding the demographics of those most likely to lose jobs in the short-run. As opposed to obsessively assembling case studies, we need to proactively identify policy entrepreneurs who can conceive of training policies that equip workers with technical skills of “long-game” relevance. As IBM points out, “[d]ata democratization impacts every career path, so academia must strive to make data literacy an option, if not a requirement, for every student in any field of study.”
Machines are an equal opportunity displacer, blind to color and socioeconomic status.
Machines are an equal opportunity displacer, blind to color and socioeconomic status. Effective policy responses require collaborative data collection and coordination among key stakeholders—policymakers, employers, and educational institutions—to  identify at-risk worker groups and to inform workforce development strategies. Machine substitution is purely an efficiency game in which workers overwhelmingly lose. Nevertheless, we can blunt these effects by identifying critical leverage points.
Investing in innovative education and training is an excellent place to start. Bill Gates’ recent $1.7 billion investment in U.S. public schools is a sign of the way forward, which offers two compelling messages for policymakers. First, innovate and experiment until we identify the right policies. Second, prioritize high-needs schools in poor neighborhoods; they deserve distinct attention to close their opportunity gaps and prepare them to be competitive in the future workforce.
Policymakers can choose to harness AI’s disruptive power to address workforce challenges and redesign fair access to opportunity simultaneously. We should train our collective energies on identifying practical policies that update our current agrarian-based education model, which unfairly disadvantages children from economically segregated neighborhoods. Evidence from a Harvard and New York University research study suggests attending a high-quality high school increases a student’s chances of attending a four-year college; which by extension improves their future income earning potential.
Let me ask a bold question: how much do we lose if we experiment with substituting an entry-level data science class for machine shop or a vocational carpentry program in urban high schools and community colleges? A 2010 pilot partnership between the University of California, Los Angeles and the National Science Foundation is an encouraging sign; the pilot focuses on redesigning computer science curricula in urban high schools to include newer mobile technologies and computational analysis.


	Related
	
			
			
	
		
			
																									
		
	
	
		
					Technology & Innovation
				Trends in the Information Technology sector
		
							Makada Henry-Nickie, Kwadwo Frimpong, and Hao Sun
																Friday, March 29, 2019
					
		
		
			

		
			
			
	
		
			
																									
		
	
	
		
					TechTank
				Regulating free speech on social media is dangerous and futile
		
							Niam Yaraghi
																Friday, September 21, 2018
					
		
		
			

		
			
			
	
		
			
																									
		
	
	
		
					TechTank
				How to increase financial support during COVID-19 by investing in worker training
		
							Gregory Ferenstein
																Wednesday, May 6, 2020
					
		
		
			

		
		


Data science is an applied computational technology best suited to inquisitive minds, making it appropriate for young students. Google’s TensorFlow is an open source machine-learning platform; its free price tag makes the platform an accessible and scalable training resource for schools with constrained budgets. Introducing a data science program into urban schools would be a major paradigm shift for these students. An applied data science program teaching gateway coding skills such as Python, R, SQL, and computational analysis would boost employment possibilities and create meaningful pathways to economic mobility.
I am suggesting that we leverage AI’s transformative power to disrupt diminishing possibilities for marginalized groups, like young men of color, who often do not feature in innovative-themed discussions outside of the social justice arena. Open Source groups such as Code.org and StudentRND exemplify the kinds of transformational approaches that democratize access and opportunity.
Producing a diverse pipeline of tech-savvy workers for Google and Amazon, even if only at the entry level, is a more attainable dream for most cities than competing in a stacked race for Amazon’s HQ2. Broadening adoption of artificial intelligence technologies poses significant workforce challenges, but it also offers the chance to blunt these effects and create opportunities for marginalized groups if we act preemptively.
Google is a donor to the Brookings Institution. The findings, interpretations, and conclusions posted in this piece are solely those of the authors and not influenced by any donation.
			

--------------------------------------------------------------------------------

New article found for topic: python
URL: https://www.brookings.edu/blog/the-avenue/2014/08/06/the-silicon-valley-wage-premium/
TITLE: The Silicon Valley Wage Premium
The Silicon Valley Wage Premium
BODY:

				
	
Software application developers earn large salaries in the United States, $96,260 a year on average. But in metropolitan San Jose they earn $131,270, the highest in the country. There are many partial explanations for this—local cost of living, differences in education levels, experience, and industry—but none of them quite account for it. It turns out that developers living in San Jose have acquired the specific skills most valued by employers.

As the map below shows, there is a huge amount of variation in earnings for software application developers across regional labor markets. In large metropolitan areas like New York, they earn $105,000, but in Louisville, they earn just $72,000.


  Average Salary of Software Application Developers by Metropolitan Area, 2013



  

Similar patterns could be shown for other occupations, of course; for even within the same job title, people vary by education and experience, and regions vary by company and industry mix, productivity and export orientation, which all affect salaries and regional housing prices.

The surprising thing, when it comes to software developers and other skilled occupations too, is that none of these factors can fully account for the San Jose premium. Software developers in San Jose are typically slightly less experienced, and while their levels of education are higher—including their likelihood of having majored in engineering or computer science—the difference is not enough to explain their elevated earnings. Likewise, the cost of living in San Jose is remarkably high, but comparable to other major cities. 



	Related Books
	
			
			

	
		
							T
					
	
	
						Trade Policy Review 2018: Nepal
					
									
						By World Trade Organization WTO					
													2019
							
				

		
			
			

	
		
							T
					
	
	
						Trade Policy Review 2018: Uruguay
					
									
						By World Trade Organization WTO					
													2019
							
				

		
			
			

	
		
							T
					
	
	
						Trade Policy Review 2018: Colombia
					
									
						By World Trade Organization WTO					
													2019
							
				

		
		

So what distinguishes San Jose software developers? 

To figure this out, I analyzed a database of 29 million job vacancies advertised online during 2013 as compiled by the analytics firm Burning Glass. Of these, roughly 1.4 million were for software application developers, making it the most in-demand occupation. In total, 3 million ads also contained salary information, which I could use to estimate the average value of each distinct skill advertised. 
	
		
				

	
			
		

	
							Jonathan Rothwell
		
							Nonresident Senior Fellow - Metropolitan Policy Program
					
						Twitter
		jtrothwell
			
		
			
		
			

			
	
	
In San Jose, the skills advertised for software developers are particularly valuable. The average vacancy requires higher value skills in San Jose than almost any other metropolitan area, even using national rather than local salary values.  

For example, 8.4 percent of ads for software developers in San Jose requested Java, a widely used programming language, associated with an average salary of $98,000 across all U.S. ads mentioning both it and a salary requirement. Yet, for the United States as a whole, just 5.7 percent of software developer ads required Java. In New York City, the share was 6.7, and it was 4.7 in Louisville.

Other high-value programming languages and skills were disproportionately advertised in San Jose, such as Linux, C++, Python and the term “software engineering.”  These skills were much less commonly required for jobs in Louisville and even New York. Only 0.2 percent of software jobs required Python in Louisville and 1.7 in New York City, compared to 2.8 percent in San Jose. It is valued at $100,345.

These and other skills contribute to the high premium enjoyed by Silicon Valley computer workers, but they could be profitably learned by a much larger swath of people, as online educators like Treehouse, Udacity, and Code Fellows aim to demonstrate.
			

--------------------------------------------------------------------------------

New article found for topic: python
URL: https://www.brookings.edu/opinions/inside-the-pentagons-secret-afghan-spy-machine/
TITLE: Inside the Pentagon’s Secret Afghan Spy Machine
Inside the Pentagon’s Secret Afghan Spy Machine
BODY:

				The Pentagon’s top researchers have rushed a classified and controversial intelligence program into Afghanistan. Known as “Nexus 7,” and previously undisclosed as a war-zone surveillance effort, it ties together everything from spy radars to fruit prices in order to glean clues about Afghan instability.

The program has been pushed hard by the leadership of the Defense Advanced Research Projects Agency (DARPA). They see Nexus 7 as both a breakthrough data-analysis tool and an opportunity to move beyond its traditional, long-range research role and into a more active wartime mission. 
But those efforts are drawing fire from some frontline intel operators who see Nexus 7 as little more than a glorified grad-school project, wasting tens of millions on duplicative technology that has nothing to do with stopping the Taliban. 
“There are no models and there are no algorithms,” says one person familiar with the program, echoing numerous others who spoke on condition of anonymity because they are not authorized to discuss the program publicly. Just “200 lines of buggy Python code to do what imagery analysts do every day.” 


	Related Books
	
			
			

	
		
							
									
					
	
	
						Countering Terrorism
					
									
						By Martha Crenshaw and Gary LaFree					
													2017
							
				

		
			
			

	
		
							
									
					
	
	
						Camp David
					
									
						By William B. Quandt					
													2016
							
				

		
			
			

	
		
							
									
					
	
	
						Geopolitics in the 21st Century
					
									
						By Michael E. O’Hanlon					
													2015
							
				

		
		

Read the full article at wired.com >>
	
		
				

	
			
			N
		
	

	
							Noah Shachtman
		
							Former Brookings Expert					Executive Editor - The Daily Beast
					
						Twitter
		NoahShachtman
			
		
			
		
			

			
	
				

--------------------------------------------------------------------------------

New article found for topic: python
URL: https://www.brookings.edu/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/
TITLE: Idea to Retire: Old methods of policy education
Idea to Retire: Old methods of policy education
BODY:

				Public policy and public affairs schools aim to train competent creators and implementers of government policy. While drawing on the principles that gird our economic and political systems to provide a well-rounded education, like law schools and business schools, policy schools provide professional training. They are quite distinct from graduate programs in political science or economics which aim to train the next generation of academics. As professional training programs, they add value by imparting both the skills which are relevant to current employers, and skills which we know will be relevant as organizations and societies evolve. 
The relevance of the skills that policy programs impart to address problems of today and tomorrow bears further discussion. We are living through an era in which societies are increasingly interconnected. The wide-scale adoption of devices such as the smartphone is having a profound impact on our culture, communities, and economy. The use of social and digital media and associated means of communication enabled by mobile devices is changing the tone, content, and geographic scope of our conversations, modifying how information is generated and consumed, and changing the very nature of citizen engagement. 
Information technology-based platforms provisioned by private providers such as Facebook, Google, Uber, and Lyft maintain information about millions of citizens and enable services such as transportation that were mediated in the past solely by the public sector. Surveillance for purposes of public safety via large-scale deployment of sensors also raises fundamental questions about information privacy. From technology-enabled global delivery of work to displacement and replacement of categories of work, some studies estimate that up to 47 percent of U.S. employment might be at risk of computerization with an attendant rise in income inequality. These technology-induced changes will affect every policy domain. How should policy programs best prepare students to address societal challenges in this world that is being transformed by technology? We believe the answer lies in educating students to be “men and women of intelligent action.” 
A model of policy education
We begin with a skills-based model of policy education. These four essential skills address the general problems policy practitioners frequently face:

Design skills to craft policy ideas 
Analytical skills to make smart ex ante decisions 
Interpersonal experience to manage policy implementation  
Evaluative skills to assess outcomes ex post and correct course if necessary

These skills make up the policy analysis toolkit required to be data driven practitioner of “intelligent action” in any policy domain. This toolkit needs to be supplemented by an understanding of how technology is transforming societal challenges, enabling new solutions, or disrupting existing regulatory regimes. This understanding is essential to policy formulation and implementation. 
Pillar 1: Design skills
As with engineering, where design precedes analysis, this first pillar seeks to educate students in thinking creatively about problems in order to devise and develop policy ideas. Using ideas derived from design, divergent and convergent thinking principles are employed to generate, explore, and arrive at a candidate set of solutions. Using Uber as an example, an approach to identify and explore the key policy issues such as convenience, costs, driver working hours, and insurance would involve interviewing and observing both incumbent taxi drivers and Uber drivers. This in turn would lead to a set of alternatives that deserve further and careful consideration.  Using these skills, candidate designs and choices that are generated can be evaluated using the policy analytic toolkit. 
Pillar 2: Analytical skills
At Carnegie Mellon, we are often cited in media and interrogated by peers on our approach to analytical and technology skills education. Curiosity about which skills are the “right” skills to teach policy practitioners are common, but we believe this is the wrong approach. We instead begin from the premise that policy or management decisions should be grounded in evidence.  We then determine the skills required to assemble the types of evidence that will likely be available to policy makers in the future.  In increasingly instrumented environments where citizens and infrastructure produce continuous streams of data, making sense of it all will require a somewhat different set of skills. We believe that a grounding in micro-economics, operations research, statistics, and program evaluation (aka causal inference) to be an essential core to policy programs. 
New coursework will teach students to work with multi-variable data and machine learning with an emphasis on prediction. This material ought to be part of the required coursework in statistics given the importance of prediction in many policy implementation settings. Along the same lines, the ability to work with unstructured data (especially text) and data visualization will become increasingly relevant to all students, not just those students who want to specialize in data analytics. Finally, knowledge of data manipulation and analysis languages such as Python and R for analytic work will be important because data often has to be massaged and cleansed prior to analysis. An important task for programs will be to determine the competencies expected of graduates. 
Pillar 3: Interpersonal experiences
The third pillar of the skills-based model is interpersonal experience, where the practiced habits of good communication and steady negotiation developed with a sound understanding of organizations, their design and their behaviors. We label these purposely as experiences rather than skills because we believe they are best practiced either in the real-world or in simulated real-world settings. It is also in this pillar where practitioners learn the knowledge necessary to become credible experts in their domain. We believe that in addition to core coursework in the area, a supplementary curriculum which provides students with opportunities to gain these experiences is an essential component of our educational model.
Pillar 4: Evaluative skills


	Related Books
	
			
			

	
		
							
									
					
	
	
						Constitution 3.0
					
									
						Edited by Jeffrey Rosen and Benjamin Wittes					
													2013
							
				

		
			
			

	
		
							
									
					
	
	
						The Need for Speed
					
									
						By Robert E. Litan and Hal J. Singer					
													2013
							
				

		
			
			

	
		
							
									
					
	
	
						After the Breakup
					
									
						By Robert W. Crandall					
													2010
							
				

		
		

The ability to carefully diagnose the effectiveness of policy or management interventions is the fourth pillar of our model. It is insufficient to create and execute policy without measurement, and this is where both careful thought to the fundamental issues of measurement and evaluation become important. The ability to make objective judgments on the benefits, liabilities, and unintended consequences of prior policies is the goal of this set of skills. Here, sound statistical and econometric training with an understanding of the principles of causal inference is essential. In addition, program evaluation skills such as cost-benefit and financial analysis help practitioners round out their evaluation skills by considering both non-monetary and economic impacts.
What should be retired?
A skills-based approach might replace certain aspects of existing policy training.  This depends on a number of factors specific to each institution, but three generally applicable observations are clear. First, real-world experiences are a powerful way to encode domain learning as well as project management skills. Through project-based work, students can learn about institutional contexts in specific policy domains and political processes such as budgeting. Second, team-based projects allow students to learn and apply principles of management and organizational behavior. At Carnegie Mellon, we refer to these as “systems synthesis” projects, since they require students to adopt a systemic point of view and to synthesize a number of skills in their policy analysis toolkit. Third, interpersonal skills training can be practiced through activities such as weekend negotiation exercises, hackathons, and speaker series. These activities can be highly intentional and fashioned to reinforce skills rather than as a recess from the “real work” of classroom training. Since students complete graduate programs in such a short time, counseling them to focus on outcomes from day one will allow them to choose a reinforcing set of coursework and real-world experiences. 
In summary, we argue for a model of policy education that views practitioners as future problem solvers. Good policy education must consider the ways in which problems will present themselves, and the ways in which answers will obscure themselves. Rigorous training grounded in the analysis of available evidence and buoyed by real-world interpersonal experiences is a sound approach to relevant, durable policy training.
	
		
				

	
			
			R
		
	

	
							Ramayya Krishnan
		
							Ramayya Krishnan is the dean of H. John Heinz III College of Information Systems and Public Policy at Carnegie Mellon University where he is the W.W. Cooper and Ruth F. Cooper Professor of Management Science and Information Systems.
		
			
		
			

	

	
			
			J
		
	

	
							Jon Nehlsen
		
							Jon Nehlsen is senior director of external relations at H. John Heinz III College of Information Systems and Public Policy at Carnegie Mellon University.
		
			
		
			

			
	
	
  Read other essays in the Ideas to Retire blog series here.
			

--------------------------------------------------------------------------------

New article found for topic: python
URL: https://www.brookings.edu/opinions/skills-success-and-why-your-choice-of-college-matters/
TITLE: Skills, success, and why your choice of college matters
Skills, success, and why your choice of college matters
BODY:

				
	
Amidst growing frustration with the cost of higher education, complaints also abound about its quality. One critique, launched in the book Academically Adrift by two sociologists, finds little evidence that college students score better on measures of critical thinking, writing, and reasoning after attending college. This is something of a paradox, since strong evidence shows that attending college tends to raise earnings power, even for students who start with mediocre preparation. 
Our recent study uses a different approach to assess the value of a college education. We find that the particular skills listed by a college’s alumni on their resumes predict how well graduates from those schools perform in terms of earning a living, meeting debt obligations, and working for high-paying or innovative companies. Since jobs requiring more valuable skills typically require at least some college education, this finding suggests many students are gaining valuable skills from college. But the variation in alumni skills across schools is wide, even after considering the aptitude of the students in terms of their pre-admission test scores. This variation implies that what one studies and where have big effects on economic outcomes.

  


  Skills versus degrees

It is widely known that education raises individual earnings, but education—measured in years of study or level of degree—is a very rough measure of learning. Thus, it is not surprising that studies consistently find that skills are an important predictor of economic outcomes. People with higher test scores—another measure of learning—earn higher wages, even with the same level of education. Likewise, graduates with technical degrees earn more, as do workers in occupations requiring more STEM skills. At the international scale, performance on standardized exams has a much stronger statistical relationship with economic outcomes than do years of education, according to a new OECD study.

  


  How we valued skills by college

Using data from the company Burning Glass, we calculated the average salary listed for distinct skills based on 3 million job vacancy ads. To match these skills to colleges, we used data from LinkedIn’s college profile pages, which show the 25 most common skills (e.g., customer service, Microsoft Excel, Python) listed by alumni from each college. For the average college, we observed 1,150 profiles per skill. (A great advantage of using LinkedIn data is the large sample size.) We obtained data for 2,164 colleges representing profiles for 2.5 million U.S. residents who attended college. By comparison, Academically Adrift surveyed 2,300 college graduates.

  


  Alumni with more valuable skills earn higher salaries

Measured at mid-career (meaning at least 10 years of working), salaries tend to be much higher for alumni who list high-value skills on their resumes. Earnings go up by an average of $2,600 for every decile of skill. Our more detailed empirical work shows that skills predict higher earnings even after controlling for math test scores on the ACT and SAT, as well as other student characteristics like family income.
Cal Tech graduates list the highest-value skills (e.g., Matlab, Python, C++, algorithms, and machine learning) and typically earn $126,000 at mid-career. Other four-year schools with high-value skills and high salaries include Harvey Mudd, MIT, the Polytechnic Institute of New York University, and the Air Force Academy. 
Earnings data from two-year colleges are not as widely available, and the correlation with alumni skills is weaker, but alumni from those schools also seem to benefit from higher skills training. Top schools include the Pittsburg Institute of Aeronautics, Spartan College of Aeronautics and Technology (Tulsa, Okla.), Coleman University (San Diego), Hondros College (Columbus, Ohio), and SUNY College of Technology at Alfred.

  

  


  Alumni with more valuable skills have higher loan repayment rates



	Related Books
	
			
			

	
		
							R
					
	
	
						Restoring the American Dream
					
									
						By Richard D. Kahlenberg					
													2019
							
				

		
			
			

	
		
							
									
					
	
	
						Higher Education to 2030
					
									
						By Organization for Economic Cooperation and Development OECD					
													2019
							
				

		
			
			

	
		
							
									
					
	
	
						The Transformation of Title IX
					
									
						By R. Shep Melnick					
													2018
							
				

		
		

As an alternative to mid-career earnings, we also analyzed how skills predict the ability to make student loan payments immediately after graduation. Here too, more valuable skills translate into labor market success. For example, not a single Harvey Mudd attendee between 2009 and 2011 defaulted on his or her federal loans within three years of leaving. Repayment rates average 95 percent for four-year colleges in the top 10 percent for alumni skills but 87 percent for those in the bottom 10 percent. For two-year colleges, repayment rates are uniformly lower, but colleges offering higher-value skills still have significantly higher repayment rates than those that do not.

  

  


  Alumni with more valuable skills are more likely to work for top organizations

Another outcome measure is whether alumni work for a desirable company or organization. LinkedIn lists the 25 enterprises that employ the most alumni from each school. To quantify the value of working for a given entity, employers were coded for desirability with data from a 2014 survey of 46,000 U.S. college students in 329 universities, developed by Universum, a corporate marketing intelligence company. A total of 212 employers, including government agencies, made it onto a top 100 list for at least one group of student majors. The most desirable employers across majors were Google, Disney, Apple, Microsoft, the FBI, Nike, NASA, the Environmental Protection Agency, the Peace Corps, and Facebook. 
For the top 10 percent of four-year colleges on alumni skills, half of LinkedIn alumni profiles indicate employment at one of the 212 top-rated companies, compared to just one in four for schools in the bottom 10 percent. For two-year schools, nearly two in five alumni (37 percent) of top-tier schools by skill worked for a top company, versus one in five alumni (21 percent) of bottom-tier schools.
For placement at Google specifically, Harvey Mudd has the highest rate—2 percent of all alumni—followed by Stanford, Carnegie Mellon, Caltech, and MIT. Almost all of the colleges with the highest placement rates at Google are in the top 20 percent of alumni skills, including liberal arts colleges like Swarthmore, Pomona, Claremont, McKenna, and Williams.
	
		
				

	
			
		

	
							Jonathan Rothwell
		
							Nonresident Senior Fellow - Metropolitan Policy Program
					
						Twitter
		jtrothwell
			
		
			
		
			

			
	
	
  

  


  Alumni with more valuable skills are more likely to work for innovative organizations

Workers who contribute to the creation and development of new, valuable products can lift the living standards of people around the world. Companies that patent are more likely to be creating these sorts of advanced industry products, and 843 entities, including universities and government agencies, own at least 40 patents granted by the U.S. Patent and Trademark Office in 2014. 
Four-colleges that graduate alumni in the top 10 percent by skill are twice as likely to have graduates working at a top patenting organization than are colleges in the bottom 10 percent (3.3 versus 1.6 percent). Likewise, graduates from two-year colleges are about twice as likely to be working for a patenting entity if their school is in the top 10 percent compared to the bottom (1.9 versus 0.9 percent).
Schools with high placement rates at patenting entities include those listed above, as well as less the U.S. Naval Academy, Lawrence Technological University, the Stevens Institute of Technology, Santa Clara University, Brazosport College, Mount Mercy University, University of Texas-Dallas, the Missouri University of Science and Technology, and San Jose State University.

  

  


  How to judge colleges

Earnings and other economic outcomes should not be equated with social value, and there are plenty of jobs and professions—child care, teaching, social work—that pay modestly but are nevertheless highly valuable to society. Colleges that specialize in this training or instill even moderately valuable skills in the least academically prepared students may be socially important institutions even if their alumni frequently are less affluent.
Nonetheless, earnings clearly matter privately and socially, as does work that supports innovation and highly productive advanced industries. Many colleges offer programs of study in fields that appear to have almost no market value—nor even any social value since the knowledge acquired is never put to use, at least through paid employment. In this sense, how well colleges instill highly valuable skills and prepare students to contribute productively to the economy should be an important consideration when evaluating schools. Colleges that do this for the students least likely to otherwise succeed are offering an even more beneficial service, as we have discussed in our value-added college research.

  
Correction: A previous version of this post showed graphs which reversed the label on 2- and 4-year colleges. The graphs have been corrected.
			

--------------------------------------------------------------------------------

New article found for topic: python
URL: https://www.brookings.edu/articles/modeling-with-data-tools-and-techniques-for-scientific-computing/
TITLE: Modeling with Data: Tools and Techniques for Scientific Computing
Modeling with Data: Tools and Techniques for Scientific Computing
BODY:

				
		PREFACE



				Should you use the book? This book is intended to be a complement to the standard stats textbook, in three ways.

First, descriptive and inferential statistics are kept separate beginning with the first sentence of the first chapter. I believe that the fusing of the two is the number one cause of confusion among statistics students.

Once descriptive modeling is given its own space, and models do not necessarily have to be just preparation for a test, the options blossom. There are myriad ways to convert a subjective understanding of the world into a mathematical model, including simulations, models like the Bernoulli/Poisson distributions from traditional probability theory, ordinary least squares, and who knows what else.

If those options aren’t enough, simple models can be combined to form multilevel models to describe situations of arbitrary complexity. That is, the basic linear model or the Bernoulli/Poisson models may seem too simple for many situations, but they are building blocks that let us produce more descriptive models. The overall approach concludes with multilevel models as in, e.g., Eliason (1993), Pawitan (2001) or Gelman & Hill (2007).

Second, many stats texts aim to be as complete as possible, because completeness and a thick spine give the impression of value-for-money: you get a textbook and a reference book, so everything you need is guaranteed to be in there somewhere. 

But it’s hard to learn from a reference book. So I have made a solid effort to provide a narrative to the important points about statistics, even though that directly implies that this book is incomplete relative to the more encyclopedic texts. For example, moment generating functions are an interesting narrative on their own, but they are tangential to the story here, so I do not mention them.

Computation The third manner in which this book complements the traditional stats textbook is that it acknowledges that if you are working with data full time, then you are working on a computer full time. The better you understand computing, the more you will be able to do with your data, and the faster you will be able to do it.

People like to characterize computing as fast-paced and ever-changing, but much of that is just churn on the syntactic surface. The fundamental concepts, conceived by mathematicians with an eye toward the simplicity and elegance of pencil-and paper math, have been around for as long as anybody can remember. Time spent learning those fundamentals will pay off no matter what exciting new language everybody happens to be using this month.



	Related Books
	
			
			

	
		
							R
					
	
	
						Restoring the American Dream
					
									
						By Richard D. Kahlenberg					
													2019
							
				

		
			
			

	
		
							
									
					
	
	
						Higher Education to 2030
					
									
						By Organization for Economic Cooperation and Development OECD					
													2019
							
				

		
			
			

	
		
							
									
					
	
	
						Leapfrogging Inequality
					
									
						By Rebecca Winthrop; With Adam Barton and Eileen McGivney					
													2018
							
				

		
		

I spent much of my life ignoring the fundamentals of computing and just hacking together projects using the package or language of the month: C++, Mathematica, Octave, Perl, Python, Java, Scheme, SPLUS, Stata, R, and probably a few others that I’ve forgotten. Albee (1960, p 30) explains that “sometimes it’s necessary to go a long distance out of the way in order to come back a short distance correctly;” this is the distance I’ve gone to arrive at writing a book on data-oriented computing using a general and basic computing language. For the purpose of modeling with data, I have found C to be an easier and more pleasant language than the purpose-built alternatives—especially after I worked out that I could ignore much of the advice from books written in the 1980s and apply the techniques I learned from the scripting languages.

WHAT IS THE LEVEL OF THIS BOOK? The short answer is that this is intended for the graduate student or independent researcher, either as a supplement to a standard first-year stats text or for later study. Here are a few more ways to answer that question:

• Ease of use versus ease of initial use: The majority of statistics students are just trying to slog through their department’s stats requirement so they can never look at another data set again. If that is you, then your sole concern is ease of initial use, and you want a stats package and a textbook that focus less on full proficiency and more on immediate intuition.1[1]

	
		
				

	
			
		

	
							Ben Klemens
		
							Senior Statistician - Office of Tax Analysts, U.S. Department of Treasury
		
			
		
			

			
	
	Conversely, this book is not really about solving today’s problem as quickly as physically possible, but about getting a better understanding of data handling, computing, and statistics. Ease of long-term use will follow therefrom.

• Level of computing abstraction: This book takes the fundamentals of computing seriously, but it is not about reinventing the wheels of statistical computing. For example, Numerical Recipes in C (Press et al., 1988) is a classic text describing the algorithms for seeking optima, efficiently calculating determinants, and making random draws from a Normal distribution. Being such a classic, there are many packages that implement algorithms on its level, and this book will build upon those packages rather than replicate their effort.

• Computing experience: You may have never taken a computer science course, but do have some experience in both the basics of dealing with a computer and in writing scripts in either a stats package or a scripting language like Perl or Python.

• Computational detail: This book includes about 80 working sample programs.
Code clarifies everything: English text may have a few ambiguities, but all the details have to be in place for a program to execute correctly. Also, code rewards the curious, because readers can explore the data, find out to what changes a procedure is robust, and otherwise productively break the code.

That means that this book is not computing-system-agnostic. So if you are a devotee of a stats package not used here, then why look at this book? Although I do not shy away from C-specific details of syntax, most of the book focuses on the conceptual issues common to all computing environments. If you never look at C code again after you finish this book, you will still have a better grounding for effectively working in your preferred programming language.

• Linear algebra: You are reasonably familiar with linear algebra, such that an expression like X−1 is not foreign to you. There are a countably infinite number of linear algebra tutorials in books, stats text appendices, and online, so this book does not include yet another.

• Statistical topics: The book’s statistical topics are not particularly advanced or trendy: OLS, maximum likelihood, or bootstrapping are all staples of first-year grad-level stats. But by creatively combining building blocks such as these, you will be able to model data and situations of arbitrary complexity.

  View book »
  


[1] I myself learned a few things from the excellently written narrative in Gonick & Smith (1994).



			

--------------------------------------------------------------------------------

New article found for topic: python
URL: https://www.brookings.edu/articles/forum-debating-bushs-wars/
TITLE: Forum: Debating Bush’s Wars
Forum: Debating Bush’s Wars
BODY:

				
		In the 
		
				Winter 2007–08 issue 
		of Survival, Philip Gordon argued that America’s strategy against terror is failing ‘because the Bush administration chose to wage the wrong war’. Survival invited former Bush speechwriter and Deputy Assistant to the President Peter Wehner and Kishore Mahbubani, Dean and Professor at the Lee Kuan Yew School of Public Policy in Singapore, to reflect on Gordon’s arguments. Their 
		comments are available in the above PDF and Philip Gordon’s response is below.

I am grateful to Peter Wehner and Kishore Mahbubani for taking the time to comment on my essay, ‘Winning the Right War’. Their comments are valuable not only because both are prominent and influential thinkers but because their divergent views on the subject help to frame the debate: Mahbubani essentially agrees with me but wishes I had ‘gone even further’ in my analysis, while Wehner disagrees and complains that I’ve written a ‘one-sided’, ‘ideological’ brief. Standing between them does not make me right, but it does underscore how, more than six years after the start of the ‘war on terror’, some well-informed observers remain almost diametrically opposed over how to wage it. That, in fact, is why I wrote Winning the Right War (the book from which my Survival essay was drawn) and why I wrote it now: with six years of evidence behind us and with an American presidential election looming, the time seems right for a serious debate about the nature of the Islamist terrorist threat and the policies most likely to defeat it. I am grateful to Peter Wehner and Kishore Mahbubani for taking the time to comment on my essay, ‘Winning the Right War’. Their comments are valuable not only because both are prominent and influential thinkers but because their divergent views on the subject help to frame the debate: Mahbubani essentially agrees with me but wishes I had ‘gone even further’ in my analysis, while Wehner disagrees and complains that I’ve written a ‘one-sided’, ‘ideological’ brief. Standing between them does not make me right, but it does underscore how, more than six years after the start of the ‘war on terror’, some well-informed observers remain almost diametrically opposed over how to wage it. That, in fact, is why I wrote (the book from which my essay was drawn) and why I wrote it now: with six years of evidence behind us and with an American presidential election looming, the time seems right for a serious debate about the nature of the Islamist terrorist threat and the policies most likely to defeat it. 
I have no ‘rebuttal’ to Mahbubani since he agrees with the broad thrust of my essay, but I do have some comments since we hardly agree on everything. He takes me to task, for example, for concluding my analysis of the sources of the terrorist threat by saying that ‘none of this means that the United States should simply change its policies to make potential terrorists happy’. He asks whether America shouldn’t ‘change those policies which are contributing to the enormous sense of resentment towards America in the larger Islamic world’. 
I would say of course it should, when it can, and my book gives a number of examples of how it might do so – by banning torture both in law and in practice, closing the Guantanamo prison, changing tactics in and withdrawing troops from Iraq, showing more respect for other countries’ perceptions and interests, and doing more to support the Palestinians, to take just a few. My point, however, was to note that taking actions to lessen resentment of the United States can not simply mean giving in to extremists’ demands. Osama bin Laden wants Israel to disappear, the United States to withdraw entirely from the Middle East, and the region’s current leaders to be replaced with an Islamic caliphate that would impose sharia law, but I doubt Mahbubani would recommend supporting such an agenda in the name of reducing resentment any more than I would. The challenge is to address legitimate social and diplomatic grievances that produce terrorism without caving in to an insatiable terrorist agenda. 
Mahbubani, like many others, clearly feels that America’s strong support for Israel is a major part of the problem. He gives me credit for ‘somewhat courageously’ noting that by ‘unreservedly justifying any Israeli military action as a necessary part of the “global war on terror” … the United States has reinforced the grievances that inspire people to become terrorists’. This is an important point that requires careful attention. As Mahbubani certainly knows, there are good reasons to be sceptical about bin Laden’s claim to be particularly interested in the fate of the Palestinians. Al-Qaeda grew and planned some of its most spectacular attacks during the 1990s, while the Oslo peace process was in full swing, and only started to focus on Israel later, as it realised that the issue had some resonance among its potential recruits. Even if the United States could somehow force Israel and the Palestinians to conclude a lasting peace tomorrow, many of the factors fuelling Muslim support for terrorism would still exist. 
That said, I agree – and argue in Winning the Right War – that the Israel–Arab dispute contributes to the resentment that fuels the Islamist terrorist threat. Just because terrorists and extremists opportunistically exploit the Palestinian issue does not mean that it is not an issue. Indeed, ‘opportunism’ is possible only where real anger and emotion create an opportunity to exploit. For extremists like bin Laden, of course, even a comprehensive Israeli–Palestinian peace would not be enough, since he also insists that America will not live in peace until ‘all the army of infidels depart the land of Muhammad’, which may be a long time in coming. But for many Muslims around the world who may be tempted to join, support or sympathise with al-Qaeda or similar groups, the issue of Israel and its relationship with its neighbours is crucial. And I agree with Mahbubani that pretending that the issue is irrelevant to the ‘war on terror’ is a critical mistake. 
I also broadly agree with Mahbubani that modernising trends in the Muslim world will undermine, more than any US intervention or use of force, fundamentalist Islam, but I have to admit I was surprised by his comments that ‘not a single Islamic society (with the brief and rare exception of the Taliban in Afghanistan) has been taken over by a fundamentalist group’ and that ‘virtually all the Islamic societies are run by modern or modernising elites’. It seems to me that the radicals who took over Iran in 1979 were ‘fundamentalist’ by any meaningful definition, as is the Sudanese regime today. Moreover, it is more than a stretch to view the current leaders of Saudi Arabia, Egypt and Iran as ‘modern or modernising elites’. I do agree that modernisation is going to be a long-term solution to the problem of Islamist terrorism (even though it is also a short-term cause of it) but I wonder if Mahbubani is not being too sanguine about how far along that process has already progressed. Perhaps his forthcoming book will shed some light on this.
Finally, I would like to correct Mahbubani’s assumption that in the current American political context ‘not a single American strategic thinker would dare suggest’ that the United States establish diplomatic relations with Iran. In fact, some of us are recommending just that. I wrote in Winning the Right War that

  


	Related Books
	
			
			

	
		
							
									
					
	
	
						JFK’s Forgotten Crisis
					
									
						By Bruce Riedel					
													2017
							
				

		
			
			

	
		
							
									
					
	
	
						The South Asia Papers
					
									
						By Stephen P. Cohen					
													2016
							
				

		
			
			

	
		
							
									
					
	
	
						What We Won
					
									
						By Bruce Riedel					
													2014
							
				

		
		

the United States should agree to talk to Iran about any issue – and even offer to open up full diplomatic relations. America maintained diplomatic relations with the Soviet Union throughout the Cold War and today it has diplomatic relations with dozens of countries that it does not particularly like; indeed one could argue that such relations are most needed when there are contentious issues to sort out.
  

This is one of the many areas where US policy must change. 
	
		
				

	
			
		

	
							Philip H. Gordon
		
							Former Brookings Expert					Mary and David Boies Senior Fellow in U.S. Foreign Policy - Council on Foreign Relations
		
			
		
			

			
	
	Peter Wehner is apparently less convinced of the need for change. Indeed in his defence of the Bush administration, his main argument is not that fighting Islamist terrorism is hard (which would be fair enough) but rather that the ‘war on terror’ is actually going very well. Our cooperation with allies is ‘unprecedented’, we’re thwarting plots, al-Qaeda is on the ropes, the ‘surge’ in Iraq is succeeding, and the overthrow of Saddam Hussein has persuaded Libya and Iran to give up their nuclear-weapons programmes. His Monty Python citation – ’what else have they done for us?’ – implies that the Bush administration’s ‘war on terror’ has had all sorts of benefits for civilisation, and the problem is merely that the locals are unable to see the light. While Wehner is right to insist that not everything is going wrong, I do not share his assessment of progress in the ‘war on terror’ so far, or his refusal to accept the negative consequences of some US policies.
Take, for example, his rebuttal of my argument that Bush-administration policies have left the United States isolated in the world. I agree that various countries’ intelligence agencies have continued to work with our own (out of self-interest) and also that the Proliferation Security Initiative was an innovative and useful initiative for which the Bush administration deserves credit. But does Wehner really want to deny that US standing in the world has fallen considerably over the past six years and that this has consequences for our foreign policy? In Iraq, the always-thin ‘coalition of the willing’ is shrinking further (the United States now provides 93% of overall forces), and even the British are essentially pulling out. In Afghanistan, NATO allies are refusing to provide adequate forces in part because European leaders find it difficult to persuade their publics to support deployments that are seen as part of Bush’s ‘war on terror’. In the Muslim world, where the war for ‘hearts and minds’ is really being fought, resentment of the United States is at an all-time high, even as some of the autocracies Wehner mentions quietly work with the United States. 
As I wrote in my article and book, this matters not because we should want to be liked by others, but because millions of people around the world are judging US actions to determine whether they want to be on America’s side, fight against it or sit on the fence. If we can’t get most of them into the first category, or at least the last, all the military and intelligence cooperation in the world will not make us safe. I do agree with Wehner that there is often tension ‘between acting in our national interest and gaining higher approval ratings in foreign countries’. I just think we’ve gotten the balance wrong, and that we’ve failed to realise that ‘gaining higher approval ratings in foreign countries’ can itself be in our national interest.
Wehner also wants to claim successes in the effort to destroy al-Qaeda and in the war in Iraq, but neither claim stands up to scrutiny. We have indeed killed and captured a number of al-Qaeda leaders, but it is simply wrong to argue that the organisation is on the ropes or that the Iraq War has not contributed to its recruitment effort. According to the July 2007 US National Intelligence Estimate, al-Qaeda has over the past two years ‘re-established its central organization, training infrastructure and lines of global communication’, and used its association with affiliates in Iraq to ‘energize the broader Sunni extremist community, raise resources, and to recruit and indoctrinate operatives’. In Iraq, the recent improvement in the security situation is obviously good news, but it is also highly tenuous and so far unaccompanied by the political agreements or ethnic reconciliation that will be required if the United States is going to be able safely to end its costly deployment. In any case, Wehner’s conclusion that ‘repairs are being made’ misses the point: even if Iraq does gradually edge toward the uneasy peace that is now at the more optimistic end of expectations, the war will have proven to be a massive setback in the effort to counter Islamist extremism and stabilise the Middle East. The Iraq war has cost hundreds of billions of dollars, killed tens of thousands of people, displaced millions internally and externally, inspired radical Islamists, strengthened Iran, destabilised Iraq’s neighbors, and deeply damaged America’s reputation as a competent, respected, and feared world power. It will take much more than a decline in roadside bombs or suicide attacks, however welcome, to compensate for all that. 
Toward the end of his response, Wehner points out that I failed to notice a mistake in a Glenn Kessler Washington Post article I cited, which referred to a ‘Sunni mosque’ even though the mosque in question was Shia. I regret not noticing that error (which was surely inadvertent, as the mosque was clearly identified in the speech Kessler was citing as ‘one of the most sacred places in Shia Islam’) but the point is that contrary to Wehner’s claim, the difference in this case is incidental. Kessler’s point, and mine, was that it is a mistake to view all sorts of different Muslim groups as one monolithic lump, ‘a thinking enemy’ as Bush put it. This same error is apparent in the passage from journalist Amir Taheri, whom Wehner cites, claiming that ‘Algeria, Egypt, and Turkey have effectively defeated their respective terrorist enemies’ and the ‘Islamofascists have also suffered defeat in Kashmir, Thailand, the Philippines, Indonesia, and Chechnya’. Aside from the fact that it would be news to many of these countries that they have defeated their terrorist enemies, is it really useful to lump those enemies all together despite their vastly different aims? That is the sort of thinking that can lead the United States to invade Saddam Hussein’s Iraq as a response to an al-Qaeda attack on the United States.
I have other disagreements with Wehner (for example, I do not see how he can argue that our demonstration of ‘resolve’ over the past few years has cowed terrorists more than emboldened them), but let me instead end on a point of agreement. In discussing trends in Iraq and elsewhere, Wehner points out evidence of Muslims turning on the extremists in their midst and increasingly rejecting suicide bombing as a justifiable tactic. Like Wehner (and in a way Mahbubani as well), I think these are important developments and that they point the way to a more hopeful future. Al-Qaeda has no positive vision to offer, its tactics are tarnishing the reputation of Muslims everywhere, and it is killing fellow Muslims and civilians all over the world. In the long run, this is not an approach likely to win broad-based support; on the contrary, unless we artificially prolong its life, it will in time be seen as the nihilistic and counterproductive strategy that it is. All this leads me to recommend the grand strategy that I spell out in Winning the Right War: contain the threat through intelligence, judicial, police and sometimes military means; address the diplomatic, economic and social sources of frustration in the Muslim world; re-establish America’s squandered moral authority and the appeal of US society; engage allies and adversaries alike diplomatically; and seek to diminish our dependence on imported oil which is as bad for oil exporters as it is for us. If we do all that, and stop playing into the extremists’ hands, I believe we can have the same sort of success we had the last time we faced a long-term, ideological threat, during the Cold War. And Islamist extremism will end up on the same ash-heap of history that Communism did. 
			

--------------------------------------------------------------------------------

New article found for topic: python
URL: https://www.brookings.edu/opinions/think-bigger-on-north-korea/
TITLE: Think Bigger on North Korea
Think Bigger on North Korea
BODY:

				While the world is fixated on Iraq and the Middle East, North Korea continues to pose at least as great a threat to Western security interests. Six-party talks with the North Koreans in Beijing have just showed that the Bush administration hasn’t yet found a way out of the nuclear crisis. Although negotiations appear likely to resume in a couple of months, their prospects for success seem poor.


The basic dilemma is easy to understand. North Korea will not surrender its nuclear capabilities, which are among its only valuable national assets, unless offered a very good deal for giving them up. President Bush refuses to offer such a deal because he sees the North Korean demand as blackmail. He insists that before any talks about better diplomatic relations or economic interaction occur, North Korea first relinquish—with verification—a nuclear program it had pledged nine years ago to abandon completely. At most, Bush may, as an interim gesture, offer to sign a multilateral accord in which all six parties (the two Koreas, the United States, China, Japan and Russia) pledge not to attack each other.

Meanwhile, in fits and starts North Korea continues its gradual progress toward a larger nuclear capability. Given the regime’s desperate economic straits, its erratic and eccentric and isolated regime and its threats last April that it might even export nuclear materials if circumstances got bad enough, this is extremely bad news.

To the extent the Bush administration has a plan for addressing this crisis, it is a strategy of pressure. It insisted on the six-party negotiating format because that allows the other five parties all to insist that North Korea de-nuclearize. That kind of setting also deprives North Korea of its bluster and brinkmanship tactics.



	Related Books
	
			
			

	
		
							
									
					
	
	
						Dilemmas of a Trading Nation
					
									
						By Mireya Solís					
													2017
							
				

		
			
			

	
		
							
									
					
	
	
						Chinese Politics in the Xi Jinping Era
					
									
						By Cheng Li					
													2016
							
				

		
			
			

	
		
							
									
					
	
	
						Hong Kong in the Shadow of China
					
									
						By Richard C. Bush					
													2016
							
				

		
		

For example, when the delegation from Pyongyang used the recent Beijing meeting to accuse the Bush administration of harboring aggressive designs on North Korea, Russia countered that it was confident the United States had no such intention. The Bush administration also has established a creative concept known as the proliferation security initiative, by which countries such as the United States, Japan, Australia, France and Germany make use of existing national laws to inspect North Korean ships in their waters—complicating North Korea’s efforts to smuggle illicit weapons, drugs and counterfeit currency.

And the military card is still on the table in principle as well.

But the Bush administration’s strategy is unlikely to work. Faced with gradual economic strangulation, North Korea’s stubborn and spiteful regime would probably again let its people starve—and perhaps consider selling dangerous weapons to terrorists—before crying uncle. Moreover, China, South Korea and Japan are far from ready to apply such a “python strategy.” China publicly criticized the United States for having an inflexible stance in last month’s talks.

	
		
				

	
			
		

	
							Michael E. O’Hanlon
		
							Senior Fellow - Foreign Policy					Director of Research - Foreign Policy					The Sydney Stein, Jr. Chair
					
						Twitter
		MichaelEOHanlon
			
		
			
		
			

			
	
	Japan and South Korea both insisted on presenting a more conciliatory package of incentives to North Korea in Beijing than Washington was prepared to countenance. And none of our three key regional partners has any interest whatever in a military option at this point.

Faced with this dilemma, we need to think bigger. We must offer much more to North Korea but demand far more in return. The goal should be to push North Korea, which has shown increasing interest in economic reform, to seriously attempt such reform—building on the precedents offered by China and Vietnam in the past two decades. If North Korea is willing and takes steps, such as cutting its conventional military forces, that are needed to give such a plan any hope of success, we can be generous in return. This would not be giving in to blackmail; it would be a form of assisted suicide for the Stalinist ways of the North Korean regime. Even if Kim Jong Il and his cronies survived the transition, their rule would be radically transformed.

This plan would require the help of all six parties that are now part of the negotiating process. Chinese economists and technicians would teach the North Koreans how to carry out market reforms. Russia would reassure Kim Jong Il and his military commanders that intrusive arms control verification can be done without opening up the country to attack. Japan and South Korea would provide aid and investment; South Korea would also have to make at least modest cuts in its conventional forces in return for much deeper cuts in the oversized North Korean military.

Beyond the nuclear and conventional military issues, North Korea would also agree to verifiable elimination of its chemical weapons and ballistic missiles. It would cease counterfeiting and drug trafficking. It would have to let all Japanese kidnapping victims leave and begin a human rights dialogue with the outside world. It would continue to abstain from terrorism and provocative actions against its neighbors.

The United States would, for its part, ease trade sanctions immediately and ultimately lift them. It would, together with its regional partners and international financial institutions, provide at least $2 billion a year in aid to North Korea. The aid would not be in the form of cash (or new nuclear reactors), and would not be provided in one big dose but would be disbursed incrementally—while we watched to make sure North Korea was also holding up its end of the grand bargain.

Diplomatic ties and security assurances leading to a full peace treaty would also be appropriate.

Of course, this approach might well fail. North Korean leaders may, for example, believe they need nuclear weapons to deter the Bush administration from another preemptive action against another charter member of the axis of evil. But it would be a major mistake to act on that assumption before testing it. And if we try and fail, coercive policies may then become possible, as our regional partners will have a much harder time claiming that diplomacy has not yet been seriously attempted.
			

--------------------------------------------------------------------------------

New article found for topic: python
URL: https://www.brookings.edu/articles/the-new-urban-demographics-race-space-boomer-aging/
TITLE: The New Urban Demographics: Race Space & Boomer Aging
The New Urban Demographics: Race Space & Boomer Aging
BODY:

				America’s urban landscape is changing. The familiar distinctions between central cities and suburbs and between the growing Sunbelt and the more stagnant Frostbelt parts of the country are being complicated by new demographic trends, two in particular. The first trend is the sharp rise in immigration to the United States. Each year about one million people, predominantly Latin American and Asian in origin, arrive in the United States, most settling in urban areas. The second trend involves the baby-boomers. This large cohort of 76 million people?often termed “the pig in the python”?is now aging toward the tail of that python. Most boomers will not move but “age in place”?in the suburbs rather than in the city. Both these trends will have important effects on urban America.

		Beyond the “White-Black, City-Suburb” Typology 
For much of the postwar period, discussions of race and space in urban America revolved around black migration to central cities and “white flight” to the suburbs. The new immigration that is infusing many urban areas with new residents from a variety of backgrounds suggests the need for a new way of thinking about the demographic profiles of cities and suburbs.

The impact of immigration is apparent from a glance at the central counties (those that contain a metropolitan area’s central-city population) showing greatest population gains during 1990?99. The two largest gainers, Maricopa County, home of Phoenix, and Clark County, home of Las Vegas, achieved most of their gains from domestic migration?migrants from other parts of the United States. Yet each of the next five central counties with the greatest population gains?those of the Los Angeles, Houston, San Diego, Miami, and Dallas metropolitan areas?registered a net loss of domestic migrants. Their gains came entirely from international migration and natural increase. Were it not for immigration, the population of these areas, and of several other large central counties, would register far smaller growth or outright declines. Indeed, areas that do not attract nearly as many immigrants?the central counties of Philadelphia, Pittsburgh, St. Louis, Cleveland, and Buffalo, among others?lost population over the 1990s.

Because conventional city-suburb, black-white demographic profiles do not take adequate account of this new immigration, I offer a new typology of the nation’s large metropolitan areas, those with populations greater than 1 million (see table 1).



	Related Books
	
			
			

	
		
							
									
					
	
	
						Know Your Price
					
									
						By Andre M. Perry					
													2020
							
				

		
			
			

	
		
							
									
					
	
	
						Financial Markets and Development
					
									
						Edited by Alison Harwood, Robert E. Litan, and Michael Pomerleano					
													2010
							
				

		
			
			

	
		
							
									
					
	
	
						Brookings Papers on Education Policy: 2000
					
									
						Edited by Diane Ravitch					
													2010
							
				

		
		

The typology begins with a Multiethnic High Immigration category of 12 metropolitan areas with high immigration and a significant Asian or Hispanic presence. The largest of these areas are New York, Los Angeles, Chicago, Washington, D.C., and San Francisco. Four other categories cover areas that have a primarily black minority presence and those that are mostly white; and within each, those growing at a relatively high pace and those growing only modestly. These four categories are: White-Black Fast-Growing (for example, Atlanta), White-Black Slow-Growing (Detroit), Mostly White Fast-Growing (Las Vegas), and Mostly White Slow-Growing (Pittsburgh).

In metropolitan areas where white-black racial dynamics have been a historically important demographic dimension, the slow-growing areas tend to be in the Rustbelt (New Orleans being an exception), and the fast-growing areas are located in the Southeast, which has now begun to attract back significant numbers of African Americans. Mostly White Fast-Growing areas are located primarily in the West and Midwest (Orlando, Nashville, and West Palm Beach excepted). Mostly White Slow-Growing areas are located in the Northeast and Midwest (Louisville excepted).
	
		
				

	
			
		

	
							William H. Frey
		
							Senior Fellow - Metropolitan Policy Program
		
			
		
			

			
	
	
Table 2 gives the composite racial profiles for cities and suburbs in each of these five categories. Multiethnic High Immigration areas clearly have the greatest diversity both in their cities and in their suburbs although the suburbs remain majority white. Among the White-Black categories, the slow-growing areas show a lesser tendency toward black suburbanization.

What these statistics show is that the conventional view of cities as being in decline and as having predominantly black populations fails to take into account recent changes in the urban scene. Today some of the nation’s fastest-growing cities are gaining population from domestic migration and are mostly white. And several large multiethnic metropolitan areas house “majority minorities” in their cities and may soon do so in their suburbs as well.

The old “city-suburb” typology also fails to recognize heterogeneous growth patterns within the suburbs. Many inner and even middle suburbs are experiencing demographic dynamics similar to those of the cities. This is especially the case in some of the largest “melting pot” metropolitan areas. A look at the immigration and domestic migration dynamics for counties within the greater Los Angeles, San Francisco, and New York CMSAs (Consolidated Metropolitan Statistical Areas) shows that it is not just the inner counties whose gains are attributable solely to international migration. Of the 29 counties in the greater New York CMSA, fully 20 registered negative domestic migration during 1990?99 and achieved their only migration growth from immigration from abroad. The same goes for 4 of the 5 counties in the Los Angeles CMSA and for 7 of the 10 counties of the San Francisco CMSA.

The heterogeneity of suburbs was already apparent in 1990. Figure 1 compares the Los Angeles, Atlanta, and Detroit metropolitan areas. The racially diverse suburbs of Los Angeles, with their infusion of immigrants and new ethnic minorities, contrast with the suburbs of Atlanta, which show moderate racial heterogeneity, and those of slower-growing Detroit, where historical racial antagonisms keep the divide between city and suburbs fairly sharp.

Suburban growth patterns continue to favor the outer suburbs. Of the 30 counties that made the fastest gains via net domestic migration during 1990?99, most that are in metropolitan areas lie in the outer suburban reaches of fast-growing metropolitan areas such as Denver, Atlanta, Las Vegas, Dallas, and Houston. Even in the fast-growing, less dense portions of the country, an outer suburban residence remains popular.
Aging Boomers, Cities, and Suburbs
Since the baby-boom generation began entering grade school in the 1950s, it has been followed closely by marketers, policymakers, and political consultants. But the boomers’ sharp disengagement from the residential aspects of city life seems to have escaped the notice of urban watchers.

The aging of the nation’s first suburban generation will increase substantially the number of households in their 50s and early 60s over the next 10 years. Early boomers?born between 1945 and 1955?will be making the transition from empty-nesters to preretirees. Many will retire from regular jobs. Some will leave their suburban homes. But most will “age in place” or perhaps make a local move.

The late boomers?those born between 1955 and 1964?will still be in their prime career and prime earnings ages. Some will be looking to upgrade their housing, again in the same local area. They will have fewer children living at home than did earlier generations at the same age?and therefore more freedom in their location decisions.

Some observers have expressed the hope that these huge boomer generations might be a source for central-city revival. But the hope seems unrealistic, given the current location of this “suburban generation.” With the exception of Hispanics, baby-boomers, now in the 35-54 age groups, are less likely to reside in the city than either today’s elderly or adults now in their 20s and early 30s.

In fact, elderly growth patterns during the 1990s show that the “graying of the suburbs” is well under way. Of the 30 counties with fastest-gaining elderly populations over the 1990s, most are either in nonmetropolitan areas or in middle or outer suburban counties of fast-growing metropolitan areas. Much of this simply reflects the aging-in-place of elderly who have moved to these fast-growing metropolitan areas during their lifetimes.

The projected gains in elderly over the next 25 years will occur in the Mostly White Fast-Growing and White-Black Fast-Growing areas discussed earlier. While some of this growth will be attributable to retirement migration to high amenity areas of the Mountain West and South, most will result from the aging-in-place of boomers who relocated there during their working lives or at least before retirement.

This suggests somewhat different patterns of growth for the new immigrant minorities than for largely white or white and black baby-boomers. Much of the boomer growth and, hence, the projected elderly gains will be in the suburbs of major metropolitan areas and in regions of the country that are not capturing new immigrant minorities. Within metropolitan areas the better-off and healthy “yuppie elderly” will tend to locate on the periphery, and the more disadvantaged segments of the older population will reside closer in.

Central cities and inner suburbs in metropolitan regions that have suffered economic and demographic declines in recent decades will continue to house disproportionate numbers of the nation’s disadvantaged elderly?older elderly people, widows and widowers, female-headed households, those with incomes below or near the poverty level and relatively high levels of disability. As they continue to age in place, they will pose special challenges for local institutions that are often the most financially strapped.

But although the aging of the baby-boomers will have different effects for cities and suburbs, dealing with the social services, health care, and transportation needs of a faster-growing senior population will prove a challenge to cities and suburbs alike.
New Forces at Work
The advent of new immigrants and the aging of the baby-boomers will surely complicate both urban and suburban race-ethnic and aging demographic dynamics.

The demographics of Los Angeles provide just one example of how the two countervailing trends could operate. As a result of the successive outmigration of whites, juxtaposed with the continued waves of immigration of new ethnic minorities, Los Angeles County’s elderly population is still majority white, its working-aged population is only about one-third white, and its child population is predominantly Hispanic and other racial and ethnic groups. Reflecting their age, the growing racial and ethnic groups in Los Angeles will be concerned with issues of affordable housing, good schools, and neighborhoods conducive to the raising of children. And, reflecting their age, whites are likely to be more concerned with health and social support services for an aging dependent population. Whether the same kind of “racial generation gap” will occur in other melting pot metropolitan areas remains to be seen.

Whatever else the new urban population profiles show, the old models of dealing with cities and suburbs will need to be revised to adapt to new demographic forces in America today.


			

--------------------------------------------------------------------------------

New article found for topic: python
URL: https://www.brookings.edu/opinions/appointments-apocalypse/
TITLE: Appointments Apocalypse
Appointments Apocalypse
BODY:

				
Anyone who doubts that the presidential appointments process is on the verge of collapse need only look at three recent events.

On April 30, President Bush’s 101st afternoon in office, the White House dumped 61 names into the Senate confirmation process in a desperate effort to beat the Clinton administration’s dismal mark after its 100th day. Despite smashing the single-day nomination record, Bush had nominated less than 30 percent of the candidates for sub-Cabinet posts by the end of that week.

On May 2, Senate Democrats announced that they were delaying a vote on two Justice Department nominees to express their anger over a change in the process that gives home-state Senators a say about federal judicial nominees. Not to be outdone, Republicans followed suit by placing holds on four Defense nominees to remind Secretary Donald Rumsfeld that he should communicate more frequently with the chamber.

On May 4, the administration borrowed the president of the National Academy of Public Administration to stand in for the deputy director for management at the Office of Management and Budget until the administration can find someone willing to fill the most important Senate-confirmed management post. Robert O’Neill will serve in the OMB post for up to 120 days.

These are just the most recent signs of an appointments apocalypse. Every administration since 1960 has faced increasing delays in winning confirmation of its nominees, creating a rising tide of vacancies in the senior ranks of government. Whereas John F. Kennedy had his Cabinet and sub-Cabinet in place by April 1961, Bill Clinton had to wait until mid-October. Short of a miracle in which candidates say yes sooner and the Senate confirmation python digests them faster, the Bush administration will not be fully in place until next February or March.

Some delays involve red tape. Senate-confirmed nominees must answer more than 230 questions, including the dates and places of birth of all relatives, the dates and purposes of all foreign travel dating back 15 years, and a list of any traffic fines above $150. Background checks by the White House Office of Presidential Personnel, FBI and Office of Government Ethics take longer than ever.

Other delays are political. Convinced that a bad appointment hurts more than a good one helps, presidential recruiters have tightened the screws, forcing candidates for even the most trivial political posts to endure a thicket of review.

Meanwhile, the Senate appears to know no limits when it comes to using legislative holds to extract concessions from the president.

Still other delays involve the growing difficulties in recruiting talented Americans to fill the top jobs. Concerns about the cost of moving to Washington, worries about restarting their careers once the administration is over, perceptions that the appointments process is confusing and unfair, and a sense that lower-level jobs are too far from the action have led many of America’s top corporate and civic leaders to conclude that an appointment would be an honor for anyone but them.



	Related Books
	
			
			

	
		
							
									
					
	
	
						Divided Politics, Divided Nation
					
									
						By Darrell M. West					
													2020
							
				

		
			
			

	
		
							
									
					
	
	
						Bit Player
					
									
						By Stephen Hess					
													2018
							
				

		
			
			

	
		
							
									
					
	
	
						The Last Palace
					
									
						By Norman Eisen					
													2018
							
				

		
		

Luckily, the Governmental Affairs Committee has begun to build a legislative record supporting long-overdue reform. The committee held two days of hearings last month to define the problem and explore potential solutions, including a reform agenda drafted by The Presidential Appointee Initiative’s advisory board.

The committee could cut the bureaucratic delays by streamlining the financial disclosure forms and creating a single form to collect personal information. It could also accelerate background checks by making sure the investigating offices have enough staffers to do their jobs quickly.

The committee could reduce the political delays by winnowing the number of presidential appointments subject to confirmation, while urging the Senate as a whole to set a 14-day limit on holds and a 45-day clock on Senate review. It could also launch a process for reducing the number of political layers in government, which would ease the burdens on a process that can only accommodate 20 to 30 nominees a week.

Finally, the committee could make presidential service more attractive by increasing appointee salaries and providing at least some relocation assistance.

Action cannot come soon enough. Unless something is done to reduce the red tape, increase comity between the branches and make presidential appointments more attractive, this won’t be the last administration that may be still arriving as it is leaving.

Paul C. Light is Vice President and Director of Governmental Studies at the Brookings Institution and [Senior] Adviser to The Presidential Appointee Initiative.
	
		
				

	
			
		

	
							Paul C. Light
		
							Nonresident Senior Fellow - Governance Studies
		
			
		
			

			
	
				

--------------------------------------------------------------------------------


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

GETTING INFO ABOUT: data science

================================================================================

New article found for topic: data science
URL: https://www.oreilly.com/data/free/building-data-science-teams.csp
TITLE: Building Data Science Teams
BODY:

        
          Get the free ebook 
        
        As data science evolves to become a business necessity, the importance of assembling a strong and innovative data teams grows. In this in-depth report, data scientist DJ Patil explains the skills,perspectives, tools and processes that position data science teams for success.
      

--------------------------------------------------------------------------------

New article found for topic: data science
URL: https://www.oreilly.com/data/free/2014-data-science-salary-survey.csp
TITLE: 2014 Data Science Salary Survey
BODY:

        
          Get the free ebook 
        
        As a data professional, you are invited to share your valuable insights. Take the short, anonymous salary survey here: http://www.oreilly.com/data/salarysurvey​2015.csp. It only takes about 5-10 minutes to complete. Thank you.For the second year, O'Reilly Media conducted an anonymous survey to expose the tools successful data analysts and engineers use, and how those tool choices might relate to their salary. We heard from over 800 respondents who work in and around the data space, and from a variety of industries across 53 countries and 41 U.S. states.Findings from the survey include:Average number of tools and median income for all respondentsDistribution of responses by a variety of factors, including age, location, industry, position, and cloud computingDetailed analysis of tool use, including tool clustersCorrelation of tool usage and salaryGain insight from these potentially career-changing findings—download this free report to learn the details, and plug your own variables into the regression model to find out where you fit into the data space.John King is a data analyst at O'Reilly Media. Roger Magoulas is O'Reilly's Research Director.
      

--------------------------------------------------------------------------------

New article found for topic: data science
URL: https://www.oreilly.com/data/free/what-is-data-science.csp
TITLE: What Is Data Science?
BODY:

        
          Get the free ebook 
        
        We've all heard it: according to Hal Varian, statistics is the next sexy job. Five years ago, in What is Web 2.0, Tim O'Reilly said that "data is the next Intel Inside." But what does that statement mean? Why do we suddenly care about statistics and about data? This report examines the many sides of data science -- the technologies, the companies and the unique skill sets.
      

--------------------------------------------------------------------------------

New article found for topic: data science
URL: https://www.oreilly.com/data/free/how-data-science-is-transforming-health-care.csp
TITLE: How Data Science Is Transforming Health Care
BODY:

        
          Get the free ebook 
        
        In the early days of the 20th century, department store magnate JohnWanamaker famously said, "I know that half of my advertising doesn'twork. The problem is that I don't know which half." That remainedbasically true until Google transformed advertising with AdSense basedon new uses of data and analysis. The same might be said about healthcare and it's poised to go through a similar transformation as newtools, techniques, and data sources come on line. Soon we'll makepolicy and resource decisions based on much better understanding ofwhat leads to the best outcomes, and we'll make medical decisionsbased on a patient's specific biology. The result will be betterhealth at less cost.This paper explores how data analysis will help us structure thebusiness of health care more effectively around outcomes, and how itwill transform the practice of medicine by personalizing for eachspecific patient.
      

--------------------------------------------------------------------------------


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

New article found for topic: data science
URL: /article/idUSFWN2BW0FJ
TITLE: BRIEF-Roche Canada Launches Data Science Coalition To Advance Covid-19 Solutions
BODY:
April 8 (Reuters) - Roche Holding AG: * ROCHE CANADA LAUNCHES DATA SCIENCE COALITION TO ADVANCE COVID-19 SOLUTIONS Source text for Eikon: Further company coverage: (Reuters.Briefs@thomsonreuters.com)Our Standards:The Thomson Reuters Trust Principles.

--------------------------------------------------------------------------------

New article found for topic: data science
URL: /article/idUSFWN2CW108
TITLE: BRIEF-Myovant Sciences To Present New Data On Relugolix In Prostate Cancer At ASCO
BODY:
May 14 (Reuters) - Myovant Sciences Ltd: * MYOVANT SCIENCES TO PRESENT NEW DATA ON RELUGOLIX IN PROSTATE CANCER AT AMERICAN SOCIETY OF CLINICAL ONCOLOGY (ASCO) 2020 ANNUAL MEETING Source text for Eikon: Further company coverage:Our Standards:The Thomson Reuters Trust Principles.

--------------------------------------------------------------------------------

New article found for topic: data science
URL: /article/idUSL8N2AJ5PH
TITLE: Macho culture seen putting women off data science jobs
BODY:
LONDON (Thomson Reuters Foundation) - Macho work cultures are driving away young women from data science jobs, said analysts on Thursday, warning a failure to diversify the male-dominated sector could result in biased technology that discriminates against women. Women hold less than a quarter of jobs in data science - where they use tech to analyze trends - and female students are put off by combative recruiting events like coding contests and hackathons, found consulting firm Boston Consulting Group. “We need a woman’s perspective to ensure what we build for our society represents our society,” said Andrea Gallego, a partner at BCG GAMMA, the group’s data arm, who added that the problem extended into other fields. “If we start building models with biased teams, we are going to run into a number of longer term effects including ethics issues and models propagating a bias we’re trying to stop.”  The report comes amid concerns that the male-dominated tech sector is shoring up gender pay gaps and can result in tech which has in-built discrimination against women. As technology transforms the world, data scientists have become among the most in-demand workers, according to reports by the career networking site LinkedIn published last month, which found it was the third fastest-growing job in the United States. But the sector is failing to attract a wider pipeline of female talent in entry level jobs, found the report, based on surveys of more than 9,000 students and recent graduates with data-related degrees in about a dozen countries. Young women were significantly more likely than men to see data science as uncomfortably competitive, said researchers. They were also less likely to feel well-informed about career opportunities in data science, though countries with a higher proportion of women in tech did better at reaching women at the start of their careers. The report called on companies to deal with the sector’s  “image problem” among women, including by building more inclusive and collaborative cultures. Allyson Zimmerman of non-profit group Catalyst, which works to make workplaces more inclusive to women, said the data reflected a macho “bro-grammer” culture in many tech firms. “Many workplaces have been designed by men for men,” said Zimmerman, who heads the firm’s operations in Europe, Africa and the Middle East, adding that firms should be concerned about a lack of female staffers. “The number one way you can compete is through your talent. If you don’t have diverse talent throughout the ranks you are missing out on innovation, better teams, better decisions and better outcomes,” she told the Thomson Reuters Foundation. Reporting by Sonia Elks @soniaelks; Editing by Katy Migiro. Please credit the Thomson Reuters Foundation, the charitable arm of Thomson Reuters, that covers humanitarian news, women's and LGBT+ rights, human trafficking, property rights, and climate change. Visit http://news.trust.orgOur Standards:The Thomson Reuters Trust Principles.

--------------------------------------------------------------------------------

New article found for topic: data science
URL: /article/idUSFWN2CW0IL
TITLE: BRIEF-Urovant Sciences Presents Positive Clinical Efficacy & Safety Data On Lead Drug Candidate Vibegron
BODY:
May 14 (Reuters) - Urovant Sciences Ltd: * UROVANT SCIENCES PRESENTS POSITIVE CLINICAL EFFICACY & SAFETY DATA ON LEAD DRUG CANDIDATE VIBEGRON AT VIRTUAL AMERICAN UROLOGICAL ASSOCIATION ANNUAL MEETING Source text for Eikon: Further company coverage: (Reuters.Briefs@thomsonreuters.com)Our Standards:The Thomson Reuters Trust Principles.

--------------------------------------------------------------------------------

New article found for topic: data science
URL: /article/idUSFWN2DH0H5
TITLE: BRIEF-Alpine Immune Sciences Presents Preclinical Data On Novel Dual BAFF/APRIL Inhibitory Domains
BODY:
June 4 (Reuters) - Alpine Immune Sciences Inc: * ALPINE IMMUNE SCIENCES PRESENTS PRECLINICAL DATA ON NOVEL DUAL BAFF/APRIL INHIBITORY DOMAINS FOR B CELL MEDIATED AUTOIMMUNE DISEASES AT THE EUROPEAN CONGRESS OF RHEUMATOLOGY E-CONGRESS 2020 Source text for Eikon: Further company coverage:Our Standards:The Thomson Reuters Trust Principles.

--------------------------------------------------------------------------------

New article found for topic: data science
URL: /article/idUSFWN2CH13M
TITLE: BRIEF-Gilead Sciences Says Aware Of Positive Data Emerging From NIAID's Study Of Investigational Antiviral Remdesivir
BODY:
April 29 (Reuters) - Gilead Sciences Inc: * GILEAD SCIENCES STATEMENT ON POSITIVE DATA EMERGING FROM NATIONAL INSTITUTE OF ALLERGY AND INFECTIOUS DISEASES’ STUDY OF INVESTIGATIONAL ANTIVIRAL REMDESIVIR FOR COVID-19 * GILEAD SCIENCES INC - UNDERSTAND THAT TRIAL HAS MET ITS PRIMARY ENDPOINT AND THAT NIAID WILL PROVIDE DETAILED INFORMATION AT AN UPCOMING BRIEFING * GILEAD SCIENCES INC - AWARE OF POSITIVE DATA EMERGING FROM NIAID’S STUDY OF INVESTIGATIONAL ANTIVIRAL REMDESIVIR FOR TREATMENT OF COVID-19 * GILEAD - REMDESIVIR IS NOT YET LICENSED/APPROVED ANYWHERE GLOBALLY AND HAS NOT YET BEEN DEMONSTRATED TO BE SAFE OR EFFECTIVE FOR TREATMENT OF COVID-19 * GILEAD SCIENCES - WILL SHARE ADDITIONAL REMDESIVIR DATA FROM OPEN-LABEL PHASE 3 SIMPLE TRIAL IN PATIENTS WITH SEVERE COVID-19 DISEASE SHORTLY * GILEAD SCIENCES - SEES DATA AT END OF MAY FROM 2ND SIMPLE STUDY EVALUATING 5- AND 10-DAY DOSING DURATIONS OF REMDESIVIR IN PATIENTS WITH MODERATE COVID-19 DISEASE * GILEAD - PHASE 3 SIMPLE TRIAL WILL PROVIDE INFORMATION ON IF SHORTER, 5-DAY DURATION OF THERAPY MAY HAVE SIMILAR EFFICACY AND SAFETY AS 10-DAY TREATMENT COURSE IN NIAID TRIAL Source text for Eikon: Further company coverage:Our Standards:The Thomson Reuters Trust Principles.

--------------------------------------------------------------------------------

New article found for topic: data science
URL: /article/idUSFWN2AO0FL
TITLE: BRIEF-Anavex Life Sciences Announces Publication Of Foundational Data For Anavex®2-73 In Multiple Sclerosis
BODY:
Feb 24 (Reuters) - Anavex Life Sciences Corp: * ANAVEX LIFE SCIENCES ANNOUNCES PUBLICATION OF FOUNDATIONAL DATA FOR ANAVEX®2-73 (BLARCAMESINE) IN MULTIPLE SCLEROSIS (MS) * ANAVEX LIFE SCIENCES - STUDY ALSO DESCRIBED THAT ANAVEX2-73 PROVIDES OPTIMAL PROTECTION OF OLIGODENDROGLIA AGAINST GLUTAMATE TOXICITY IN VITRO Source text for Eikon: Further company coverage: (Reuters.Briefs@thomsonreuters.com)Our Standards:The Thomson Reuters Trust Principles.

--------------------------------------------------------------------------------

New article found for topic: data science
URL: /article/idUSFWN2DB0PN
TITLE: BRIEF-Myovant Sciences Announces Additional Data From Phase 3 Study In Advanced Prostate Cancer Patients
BODY:
May 29 (Reuters) - Myovant Sciences Ltd: * MYOVANT SCIENCES ANNOUNCES ADDITIONAL POSITIVE EFFICACY AND CARDIOVASCULAR SAFETY DATA FROM PHASE 3 HERO STUDY OF ONCE-DAILY, ORAL RELUGOLIX IN ADVANCED PROSTATE CANCER AND PUBLICATION IN THE NEW ENGLAND JOURNAL OF MEDICINE * MYOVANT SCIENCES LTD - RELUGOLIX TREATMENT SHOWED A 54% LOWER RISK OF MAJOR ADVERSE CARDIOVASCULAR EVENTS COMPARED TO LEUPROLIDE ACETATE Source text for Eikon: Further company coverage:Our Standards:The Thomson Reuters Trust Principles.

--------------------------------------------------------------------------------

New article found for topic: data science
URL: /article/idUSFWN2D40IS
TITLE: BRIEF-Gilead Sciences Issues Statement On NEJM Publication Of Remdesivir Data From NIAID Study
BODY:
May 22 (Reuters) - Gilead Sciences Inc: * GILEAD SCIENCES STATEMENT ON NEJM PUBLICATION OF REMDESIVIR DATA FROM NIAID STUDY * FINDINGS FROM NIAID TRIAL OF REMDESIVIR IN HOSPITALIZED PATIENTS WITH ADVANCED COVID-19 HAVE BEEN PUBLISHED IN PEER-REVIEWED MEDICAL JOURNAL * FINDINGS SUPPORT USE OF REMDESIVIR IN POPULATION, WITH LARGEST BENEFIT OBSERVED AMONG INDIVIDUALS WHO REQUIRED OXYGEN SUPPLEMENTATION * ANTICIPATE THAT RESULTS FROM PHASE 3 SIMPLE-SEVERE STUDY WILL BE PUBLISHED IN NEAR FUTURE * DATA FROM SIMPLE-SEVERE STUDY SUPPORT TREATMENT OF SOME PATIENTS FOR 5 DAYS RATHER THAN 10 DAYS, DEPENDING ON CLINICAL STATUS * RESULTS FROM PHASE 3 SIMPLE-MODERATE STUDY ARE EXPECTED AT END OF THIS MONTH Source: bit.ly/3d2rxve Further company coverage:Our Standards:The Thomson Reuters Trust Principles.

--------------------------------------------------------------------------------

New article found for topic: data science
URL: /article/idUSASA00FAV
TITLE: BRIEF-Alimera Sciences Reports Fourth Quarter Results
BODY:
Feb 26 (Reuters) - Alimera Sciences Inc: * ALIMERA SCIENCES REPORTS RECORD FOURTH QUARTER AND 2019 RESULTS * Q4 EARNINGS PER SHARE $0.08 * Q4 REVENUE $17.3 MILLION VERSUS REFINITIV IBES ESTIMATE OF $15 MILLION * Q4 EARNINGS PER SHARE ESTIMATE $-0.45 — REFINITIV IBES DATA * AS OF DECEMBER 31, 2019, ALIMERA HAD CASH AND CASH EQUIVALENTS OF APPROXIMATELY $9.4 MILLION Source text for Eikon: Further company coverage:Our Standards:The Thomson Reuters Trust Principles.

--------------------------------------------------------------------------------


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

New article found for topic: data science
URL: https://www.brookings.edu/techstream/how-data-science-can-ease-the-covid-19-pandemic/
TITLE: How data science can ease the COVID-19 pandemic
BODY:

		




Social distancing and stay-at-home orders in the United States have slowed the infection rate of SARS-CoV-2, the pathogen that causes COVID-19. This has halted the immediate threat to the U.S. healthcare system, but consensus on a long-term plan or solution to the crisis remains unclear.  As the reality settles in that there are no quick fixes and that therapies and vaccines will take several months if not years to invent, validate, and mass produce, this is a good time to consider another question: How can data science and technology help us endure the pandemic while we develop therapies and vaccines?



Before policymakers reopen their economies, they must be sure that the resulting new COVID-19 cases will not force local healthcare systems to resort to crisis standards of care. Doing so requires not just prevention and suppression of the virus, but ongoing measurement of virus activity, assessment of the efficacy of suppression measures, and forecasting of near-term demand on local health systems. This demand is highly variable given community demographics, the prevalence of pre-existing conditions, and population density and socioeconomics.



Data science can already provide ongoing, accurate estimates of health system demand, which is a requirement in almost all reopening plans. We need to go beyond that to a dynamic approach of data collection, analysis, and forecasting to inform policy decisions in real time and iteratively optimize public health recommendations for re-opening. While most reopening plans propose extensive testing, contact tracing, and monitoring of population mobility, almost none consider setting up such a dynamic feedback loop. Having such feedback could determine what level of virus activity can be tolerated in an area, given regional health system capacity, and adjust population distancing accordingly.



We propose that by using existing technology and some nifty data science, it is possible to set up that feedback loop, which would maintain healthcare demand under the threshold of what is available in a region. Just as the maker community stepped up to cover for the failures of the government to provide adequate protective gear to health workers, this is an opportunity for the data and tech community to partner with healthcare experts and provide a measure of public health planning that governments are unable to do. Therefore, the question we invite the data science community to focus on is: How can data science help forecast regional health system resource needs given measurements of virus activity and suppression measures such as population distancing?







For the data science effort to work, first and foremost, we need to fix delays in data collection and access introduced by existing reporting processes. Currently, most departments of public health are collecting and reporting metrics that are not helpful, and are reporting them with 48 hour delays, and often with errors. Although there are examples of regional excellence in such reporting, by and large, the recommendations from the health IT community around accurate and fast public health reporting remain ignored. For instance, consider the number of COVID-19 hospitalizations, which is the best indicator of the disease’s burden on the regional health system. At the present time, due to time lags in confirming and reporting cases and a failure to distinguish between current and cumulative hospitalizations, even regions that report hospitalization data often provide only a blurry picture of the burden on the regional health system. Regions should ideally report both suspected and confirmed hospital cases and indicate the date of admission, in addition to the date of report or confirmation.



Even with perfect reporting, there are fundamental delays in what such data can tell us. For example, new admissions to a hospital today reflect virus activity as of 9 to 13 days ago (which depends, in turn, on social distancing interventions from up to 17 days prior). Not factoring in such considerations have led to significant over-estimation of hospitalization needs nationwide. We therefore need to measure virus activity via proxy measures that are indicative early in the lifecycle of the virus. We must benchmark these against the number of new and total COVID-19 hospitalizations as well as ideally the number of new infections, assuming it is accurately measured through large scale testing. Available proxy measures include test positivity rates in health systems, case counts, deaths and perhaps seropositivity rates. Ongoing symptom tracking via smartphone apps, daily web or phone surveys, or cough sounds can identify potential hotspots where virus transmission rates are high. Contact tracing, which currently requires significant human effort, can also help tracking of potential cases if it can be scaled using technology under development by major American tech companies. 



With reliable tracking and benchmarking in place, we can calculate infection prevalence as well as daily growth and transmission rates, which is essential for determining if policies are working. This is a problem not only of data collection but also data analysis. Issues of sensitivity, daily variability, time lags, and confounding need to be studied before such data can be used reliably. For instance, symptom tracking is nonspecific and may have difficulty tracking virus activity at low prevalence. Other emerging data sources such as wastewater and smart thermometer data hold similar promise but will have to grapple with these same issues.



We then need to estimate the regional effects of policy interventions such as shelter-in-place orders (via mobility reduction) and contact tracing (via reductions in new cases), first as simple forecasts and eventually maturing to what-if analyses. Several efforts have quantified the impact of mobility on virus transmission and some have suggested “safe” forms of mobility. While there are many potential ways to quantify population mobility — such as via traffic patterns, internet bandwidth usage by address, and location of credit card swipes — the most scalable mechanism to measure mobility appears to be via tracking of smartphones. Groups such as the COVID-19 Mobility Data Network provide such data daily in anonymized, aggregated reports.



Once the ability to project from mobility to transmission to health system burden is constructed, we can “close the loop” by predicting how much mobility we can afford given measured virus activity and anticipated health system resources in the next two weeks. Researchers have already attempted to calculate “tolerable transmission” in the form of maximum infection prevalence in a given geography that would not overload health systems. Coupling such tolerable transmission estimates with daily assessments of a valid sample of the population (via testing, via daily surveys, via electronic health record-based surveillance) would allow monitoring of changes in transmission which can alert us to the need to intervene, such as by reducing mobility. As new measures such as contact tracing cut transmission rates, these same monitoring systems can tell us that it is safe to increase mobility further. Continuously analyzing current mobility as well as virus activity and projected health system capacity can allow us to set up “keep the distance” alerts that trade off tolerable transmission against allowed mobility. Doing so will allow us to intelligently balance public health and economic needs in real time.



Concretely, then, the crucial “data science” task is to learn the counterfactual function linking last week’s population mobility and today’s transmission rates to project hospital demand two weeks later. Imagine taking past measurements of mobility around April 10 in a region (such as the Santa Clara County’s report from COVID-19 Community Mobility Reports), the April 20 virus transmission rate estimate for the region (such as from http://rt.live), and the April 25 burden on the health system (such as from the Santa Clara County Hospitalization dashboard), to learn a function that uses today’s mobility and transmission rates to anticipate needed hospital resources two weeks later. It is unclear how many days of data of each proxy measurement we need to reliably learn such a function, what mathematical form this function might take, and how we do this correctly with the observational data on hand and avoid the trap of mere function-fitting. However, this is the data science problem that needs to be tackled as a priority. 



Adopting such technology and data science to keep anticipated healthcare needs under the threshold of availability in a region requires multiple privacy trade-offs, which will require thoughtful legislation so that the solutions invented for enduring the current pandemic do not lead to loss of privacy in perpetuity. However, given the immense economic as well as hidden medical toll of the shutdown, we urgently need to construct an early warning system that tells us to enhance suppression measures if the next COVID-19 outbreak peak might overwhelm our regional healthcare system. It is imperative that we focus our attention on using data science to anticipate, and manage, regional health system resource needs based on local measurements of virus activity and effects of population distancing.



Dr. Nigam Shah is an associate professor of Medicine (Biomedical Informatics) at Stanford University and  Associate CIO for data science at Stanford Healthcare.Dr. Jacob Steinhardt is an assistant professor of statistics at University of California, Berkeley.




	

--------------------------------------------------------------------------------

New article found for topic: data science
URL: https://www.brookings.edu/research/what-all-policy-analysts-need-to-know-about-data-science/
TITLE: What all policy analysts need to know about data science
What all policy analysts need to know about data science
BODY:

					
		
				

	
			
		

	
							Alex Engler
		
							Rubenstein Fellow - Governance Studies
					
						Twitter
		@AlexCEngler
			
		
			
		
			

			
	
	
Conversations around data science typically contain a lot of buzzwords and broad generalizations that make it difficult to understand its pertinence to governance and policy. Even when well-articulated, the private sector applications of data science can sound quite alien to public servants. This is understandable, as the problems that Netflix and Google strive to solve are very different than those government agencies, think tanks, and nonprofit service providers are focused on. This does not mean, however, that there is no public sector value in the modern field of data science. With qualifications, data science offers a powerful framework to expand our evidence-based understanding of policy choices, as well as directly improve service delivery.
To better understand its importance to public policy, it’s useful to distinguish between two broad (though highly interdependent) trends that define data science. The first is a gradual expansion of the types of data and statistical methods that can be used to glean insights into policy studies, such as predictive analytics, clustering, big data methods, and the analysis of networks, text, and images. The second trend is the emergence of a set of tools and the formalization of standards in the data analysis process. These tools include open-source programming languages, data visualization, cloud computing, reproducible research, as well as data collection and storage infrastructure.
Source: Alex Engler/The University of Chicago
Perhaps not coincidentally, these two trends align reasonably well with the commonly cited data science Venn diagram. In this diagram, data science is defined as the overlap of computer science (the new tools), statistics (the new data and methods), and critically, the pertinent domain knowledge (in our case, economics and public policy). While it is a simplification, it is still a useful and meaningful starting point. Moving beyond this high-level understanding, the goal of this paper is to explain in depth the first trend, illuminating why an expanded view of data and statistics has meaningful repercussions for both policy analysts and consumers of that analysis.
Traditional evidence-building for policy analysis
Using data to learn about public policy is not at all new. The origins of the social sciences using statistical analysis of observational data goes back at least to the 1950s, and experiments started even further back. Microsimulation models, less common but outsized in their influence, emerged as the third pillar of data-driven policy analysis in the 1970s. Beyond descriptive statistics, this trifecta—experiments, observational statistical analysis, and microsimulation—dominated the quantitative analysis of policy for around 40 years. To this day, they constitute the overwhelming majority of empirical knowledge about policy efficacy. While recent years have seen a substantial expansion in the set of pertinent methods (more on that below), it is still critical to have a strong grasp of experiments, observational causal inference, and microsimulation.
 @import url(https://c24215cec6c97b637db6-9c0895f07c3474f6636f95b6bf3db172.ssl.cf1.rackcdn.com/interactives/2020/accordions-v2/app.css); 
Experiments
Since public policy can’t be conducted in a laboratory, experiments are rare in policy studies. Experiments require random assignment, which for policy means a benefit or program is made available randomly to some people and not to others—hardly a politically popular strategy. Many would also say it is ethically questionable to do this, though randomized experiments have taken firm root in medicine, sacrificing fairness in the short term for progress in the long term. Regardless of the political and ethical barriers, they do happen. Experiments are most often supported by nonprofits or created by an accident of governance, and can produce relatively rigorous results, compared to the other methods discussed here.

Perhaps the most famous experiment in modern public policy is that of the Oregon Medicaid expansion. When Oregon moved to expand access to Medicaid in 2008 (before the Affordable Care Act), the state quickly realized that it could not afford to cover all the individuals eligible under the loosened criteria. Opting to randomly select which residents would be able to receive benefits, Oregon officials created the perfect circumstances for researchers to compare recipients of Medicaid with non-recipients who were otherwise very similar. Professors Katherine Baicker and Amy Finkelstein led the research efforts, resulting in extensive evidence that Medicaid improved some health outcomes and prevented catastrophic medical expenses, while also increasing health-care utilization and costs. Signaling a growing interest in this approach, the recent Nobel Prize in Economics recognized three scholars who have taken experiments (sometimes call randomized control trials, or RCTs) into the developing world to examine how to best tackle global poverty.


Statistical analysis of observational data
Due to the financial and political difficulties that experiments present, they remain rare, and much more research is based on the statistical analysis of observational data. Observational data refers to information collected without the presence of an explicit experiment—it comes from surveys, government administrative data, nonprofit service delivery, and other sources. Usually by obtaining and combining several datasets, researchers look for various opportunities to examine the causal effects of policy changes with statistical methods. These statistical methods, broadly called causal inference statistics (or quasi-experiments), take advantage of differences within populations, or policy changes over time and geography to estimate how effective a service or intervention is.

Individually, the strength of the evidence from a single study is limited. (This is true in any field, and it suggests prudence when changing your beliefs based on results from one study.) However, since observational data is far easier to gather and analyze than experimental data, it is possible to find many opportunities to re-examine the same policy questions. Eventually, it’s possible to examine many papers on the same subject, called a meta-analysis. Meta-analysis of observational studies have convincingly argued that increased school spending improves student outcomes, gun access leads to higher risk of suicide and homicide, and that taxes on sugary beverages are associated with lower demand for those beverages.

Although at times difficult to interpret, this slow accumulation of many observational analyses by different research groups often becomes the most informative and trustworthy source of information about potential policy changes.


Microsimulation
Although microsimulation is a lesser-known type of modeling, it remains a critical one. The news is frequently covered in estimates from microsimulation methods, such as how effective taxes would change under the Tax Cuts and Jobs Act and how many people would lose health insurance under the curtailing of the Affordable Care Act. Even a substantial part of the (in)famous Congressional Budget Office scoring of the cost of federal legislation employs microsimulation.

The Urban Institute-Brookings Institution Tax Policy Center model is perhaps the easiest to understand intuitively. The model starts with a sample of anonymized administrative data from the Internal Revenue Service, which contains lots of information about taxpayers that is specific to each person. (This puts the “micro” in microsimulation.) The model itself then does the same thing as online tax preparation software: It runs through the rules of the tax code and calculates how much this person should be paying in taxes. However, the model contains many different knobs that can be turned and switches that can be flicked, each one changing something about the way the tax code works. By altering some of these inputs, the model creates a simulation, that is, an alternative possible outcome from the real world of tax policy.

These models are highly complex, and usually take years to build. They also require a lot of information about how a set of public policies are currently affecting a population, so the data typically comes from government administration records. However, once they are built, they offer a quick and flexible lens into potential policy changes. In reality, the behavioral consequences—how people and firms react to new policy—are large enough that few experts are ever really convinced that estimates from these models are precisely correct. That said, microsimulation methods can ground policy discussions to reasonable predictions, make assumptions explicit, and give a reasonable sense of what complex and interacting policy changes might do. Compared to letting pundits invent numbers out of thin air, microsimulation offer a dramatically more rigorous approach to estimating policy outcomes.




The expanded methods of data science for policy analysis
Nothing discussed above falls outside the field of data science. These approaches all use data, programming, and statistics to infer meaningful conclusions about the world. Still, the term “data science” has some value, as it connotes a broader set of methods and data types than is traditional to the field of policy analysis. While many of these methods have existed for a long time, the proliferation of new and diverse data sources means this expanded toolkit should be more widely understood and applied by policy analysts. Many of the methods detailed below fall into the field of machine learning, but in this case, that terminology complicates the issue without adding much clarity.
 @import url(https://c24215cec6c97b637db6-9c0895f07c3474f6636f95b6bf3db172.ssl.cf1.rackcdn.com/interactives/2020/accordions-v2/app.css); 

Predictive analytics

There is a growing recognition that many government and nonprofit services can be improved with predictive analytics. In Chicago, predictive models are used to reduce the exposure of young children to lead paint, which has extensive and permanent health consequences. Before this effort, and still in most places across the United States, exposed lead paint is typically only discovered after children fall ill.

The Chicago model uses historical inspection data to find correlations between exposed lead paint and other information (like the age of the buildings, when they were last renovated, if they have been vacant, as well as demographic data). This model can then be used to evaluate the level of risk of lead paint in homes that are going to accommodate newborn children. Using those predictions, the Chicago Department of Public Health can more strategically prioritize lead paint inspections, saving many children from hazardous exposure.

This is a generalizable approach for service providers who have a valuable intervention, limited resources, and uncertainty in where their investments would be most beneficial. As another example, the Center for Data Insights at MDRC—a nonprofit, nonpartisan education and social policy research organization—is exploring how to use prediction modeling to better allocate employment services to former inmates. (Disclosure: I am a data science consultant on this project.) If there is trustworthy historical data and an opportunity to affect who gets an intervention, predictive analytics can be highly beneficial by getting services delivered to those who need it most.


Clustering

In public policy, subgroups of a larger population can be very important. Some students score highly on tests, while other score poorly. Some people earn a lot of money from their jobs, while others earn very little. However, although it is tempting to think of groups as separated along a single variable, like the examples above, this is infrequently the case. Some people may earn little money from their jobs, but are in fact in graduate school, have highly educated parents with a strong social support system, suggesting that their income potential is quite high. In some cases, they may be more similar to people earning lots of money than those who earn little but do not have those other supports.

Clustering methods allow for the discovery of these underlying groups across many variables that might otherwise remain hidden or avoid our qualitative intuition. The Pew Research Center has demonstrated this by using clustering methods to examine our assumptions about the political spectrum. Pew researchers applied clustering methods to a survey with 23 questions about political opinions. They discovered that the traditional liberal-moderate-conservative spectrum does not effectively compass the many dimensions of political views that Americans hold. Instead, they argued for seven distinct political subgroups. As just one difference, this more nuanced political analysis notes two groups of conservatives: one more socially conservative, but also critical of Wall-Street; and another more socially moderate, pro-immigration, but also pro-Wall Street.

This is closer to how the world really works—subgroups are complex and nothing is unidimensional. It’s imperative to consider how many variables may be interacting to define the most meaningful differences in whatever populations are being analyzed.


Big data

Sometimes, though certainly not always, simply having more data enables better or different policy analysis. Over the past 12 years, a joint academic initiative at MIT Sloan and Harvard Business School has been using online prices to measure macroeconomic indicators. By scraping data from the internet, the Billion Prices Project has collected the prices of 15 million items from over a thousand retailers. This massive dataset has enabled them to create measures of inflation in 20 countries, updated on a daily basis. For the sake of comparison, the Bureau of Labor Statistics’ (BLS) Consumer Price Index is monthly. Although there are many challenges to this new approach, it’s worth keeping in mind that the traditional process used by the BLS (government employees surveying or physically examining prices) is far more expensive, complicated by its own problems (e.g., growing survey non-response), and painstakingly slow.

While big data can offer new insights, there are important statistical differences when analyzing big data. Most notably, it is generally harder to get data that accurately represents the whole of a population (like a country or a state). Cloud computing and modern software may easily enable analyzing multi-billion row datasets, but that makes it no easier to know who the data is relevant to. Phone records can illuminate strategies to improve traffic patterns, but does it overlook people without mobile phones? Credit card transactions can reveal lifestyle differences across socio-economic groups, but what could be missing without seeing cash transactions and cash-only consumers? This remains a large problem for effectively using big data that is not originally meant for social science (sometimes called “found data”). As a result, it’s a priority to continue the development of methods that can adjust these datasets to be representative and accurate, especially since this new data can offer so much.


Text analysis

Modern text analysis, or natural language processing, offers new ways to glean meaningful insights into the huge bodies of writing that societies produce. For example, consider the impressions that a community has of its law enforcement officers. Trust in the legitimacy of a police force can lead to more lawful behavior, as well as community collaboration to solve and reduce crimes. However, community impressions of police can be hard to measure. This is why the Urban Institute turned to Twitter. Researchers at Urban’s Justice Policy Center analyzed the sentiment of 65 million tweets, finding spikes in negative sentiment after violent police-citizen interactions. It’s worth nothing that this analysis is affected by the big data considerations detailed above.

In another instance, my colleagues in Brookings’s Metropolitan Policy Program looked for overlapping patterns in the text of job descriptions and AI patents. This allows them to create a quantitative estimate of how good AI might be at various job tasks, and thereby, how potentially automatable those jobs might be. This approach creates a new way to reason about the effects of automation that is less dependent on the qualitative judgement of experts.


Network analysis

In their book, “Network Propaganda,” three researchers of Harvard’s Berkman-Klein Center created networks of online media sources, like The New York Times and Fox News. They then measured the sources’ relationships to one another with the hyperlinks in their news content, as well as the social media sharing patterns of their audience. Critically, their research has shown how isolated and self-amplifying far-right media sources have become, leading them to grow more extreme and less tethered to the truth. This is the exact type of insight that network analysis can help deliver: Around whom is the network most dependent? Who is on the fringes and sidelines? How are relationships between actors changing?

While the internet and social media has created huge networks of people, any group of things with relationships to one another can be considered a network and analyzed as such. The states have long been considered laboratories of democracy, where experimental policies can be tested and successful ones shared. This also can be conceived of as a network, with states being connected to one another through the diffusion of similar legislation. Recent research of this kind has provided further evidence of California’s status as a leader in policy innovation. This might be unsurprising, but the same research also highlights Kentucky as the most influential state for the diffusion of public policy from the 1960s until California's emergence in the mid-1990s.


Image analysis

Image data has proliferated in recent years, originating in everything from cell phones, traffic cameras, smart devices, and even constellations of new satellites. In many parts of the word, especially poor countries, it can be very hard to develop an accurate understanding of poverty—but analyzing image data can help. Unsurprisingly, knowing where impoverished people are is vital to targeting services and investments to help improve their socio-economic outcomes. This is why the World Bank has been developing methods to use high-definition satellite data to create geographically specific measures of poverty, especially relevant to the 57 countries with almost no survey data on poverty. Data science methods can look at satellite imagery and recognize cars, identify roofing materials, distinguish between paved and unpaved roads, and measure building height and density. In turn, these new variables can be used to estimate fairly accurate poverty measures that are substantial improvements over outdated (or nonexistent) estimates.

Satellite data has also been used to monitor international conflicts and as evidence of human rights abuses. Other efforts have proposed using social media photos of political protests to measure their size and degree of violence, though this is likely not ready for implementation. Currently, image analysis is largely limited to facial and object recognition; it is not close to genuine understanding of photos. Still, as imagery proliferates and related modeling techniques improve, this data will offer powerful new ways to examine the state of the world.



Why data science matters to public policy and governance
Evaluating data is becoming a core component of government oversight. The actions of private companies are more frequently in databases than file cabinets, and having that digital information obscured from regulators will undermine our societal safeguards. Government agencies should already be acting to evaluate problematic AI-hiring software and seeking to uncover biases in models that determine who gets health interventions. As algorithmic decision-making becomes more common, it will be necessary to have a core of talented civic data scientists to audit their use in regulated industries.
Even for public servants who never write code themselves, it will be critical to have enough data science literacy to meaningfully interpret the proliferation of empirical research. Despite recent setbacks—such as proposed cuts to evidence-building infrastructure in the Trump administration’s budget proposal—evidence-based policymaking is not going anywhere in the long term. There are already 125 federal statistical agencies, and the Foundations of Evidence Based Policymaking Act, passed early last year, expands the footprint and impact of evidence across government programs.
“Even for public servants who never write code themselves, it will be critical to have enough data science literacy to meaningfully interpret the proliferation of empirical research.”
Further, the mindset of a data scientist is tremendously valuable for public servants: It forces people to confront uncertainty, consider counterfactuals, reason about complex patterns, and wonder what information is missing. It makes people skeptical of anecdotes, which, while often emotionally powerful, are not sufficient sources of information on which to build expansive policies. The late and lauded Alice Rivlin knew all this in 1970, when she published “Systemic Thinking for Social Action.” Arguing for more rigor and scientific processes in government decision-making, Rivlin wrote a pithy final line: “Put more simply, to do better, we must have a way of distinguishing better from worse.”
How to encourage data-scientific thinking and evidence-based policies
The tools and data to distinguish better from worse are more available than ever before, and more policymakers must know how to use and interpret them. A continued expansion of evidence-based decision-making relies on many individuals in many different roles, adopting practices that encourage data-scientific thinking. Managers in government agencies can hire analysts with a rigorous understanding of data in addition to a background in policy. They can also work to open up their datasets, contributing to Data.gov and the broader evidence-based infrastructure. Grant-making organizations have a critical role, too. They should be mandating an evaluation budget—at least 5% of a grant—to collect data and see if the programs they are funding actually work. When they fund research, it should require replicable research and open-data practices.


			
			Related Content
		
		
		
	
		
			
																									
		
	
	
		
					Technology & Innovation
				The case for AI transparency requirements
		
							Alex Engler
																Wednesday, January 22, 2020
					
		
		
			


	
		
			
																									
		
	
	
		
					Technology & Innovation
				What is artificial intelligence?
		
							Darrell M. West
																Thursday, October 4, 2018
					
		
		
			


	
		
			
																									
		
	
	
		
					Technology & Innovation
				5 questions policymakers should ask about facial recognition, law enforcement, and algorithmic bias
		
							Rashawn Ray
																Thursday, February 20, 2020
					
		
		
			

	
	

For policy researchers looking to expand their sense of what is possible, keep an eye on the data science blogs at the Urban Institute and the Pew Research Center, which get into the weeds on how they are using emerging tools to build and disseminate new knowledge. And for current policy analysts who want to deepen their skills, they should consider applying to the Computational Social Science Summer Institute, a free two-week intensive to learn data skills in the context of social problems and policy data. Though much of it is not directly relevant to policy, there is a tremendous amount of online content for self-learners, too. I recommend looking into free online courses and learning to program in R. For those interested in a bigger investment, look to the joint data science and public policy graduate programs, like those at Georgetown University, the University of Chicago, and the University of Pennsylvania.

The Brookings Institution is a nonprofit organization devoted to independent research and policy solutions. Its mission is to conduct high-quality, independent research and, based on that research, to provide innovative, practical recommendations for policymakers and the public. The conclusions and recommendations of any Brookings publication are solely those of its author(s), and do not reflect the views of the Institution, its management, or its other scholars.
Google provides general, unrestricted support to the Institution. The findings, interpretations, and conclusions in this report are not influenced by any donation. Brookings recognizes that the value it provides is in its absolute commitment to quality, independence, and impact. Activities supported by its donors reflect this commitment.
			

--------------------------------------------------------------------------------

New article found for topic: data science
URL: https://www.brookings.edu/blog/brown-center-chalkboard/2016/03/10/big-data-meet-behavioral-science/
TITLE: Big data, meet behavioral science
Big data, meet behavioral science
BODY:

				America’s community colleges offer the promise of a more affordable pathway to a bachelor’s degree. Students can pay substantially less for the first two years of college, transfer to a four-year college or university, and still earn their diploma in the same amount of time. At least in theory. Most community college students—80 percent of them—enter with the intention to transfer, but only 20 percent actually do so within five years of entering college. This divide represents a classic case of what behavioralists call an intention-action gap. 
Why would so many students who enter community colleges intending to transfer fail to actually do so? Put yourself in the shoes of a 20-something community college student. You’ve worked hard for the past couple years, earning credits and paying a lot less in tuition than you would have if you had enrolled immediately in a four-year college or university. But now you want to transfer, so that you can complete your bachelor’s degree. How do you figure out where to go? Ideally you’d probably like to find a college that would take most of your credits, where you’re likely to graduate from, and where the degree is going to count for something in the labor market. A college advisor could probably help you figure this out, but at many community colleges there are at least 1,000 other students assigned to your advisor, so you might have a hard time getting a quality meeting.  Some states have articulation agreements between two- and four-year institutions that guarantee admission for students who complete certain course sequences and perform at a high enough level. But these agreements are often dense and inaccessible. 
The combination of big data and behavioral insights has the potential to help students navigate these complex decisions and successfully follow through on their intentions. Big data analytic techniques allow us to identify concrete transfer pathways where students are positioned to succeed; behavioral insights ensure we communicate these options in a way that maximizes students’ engagement and responsiveness. 
The transfer decision is of course only one of many such challenges students and families face in education, where information to help people make informed choices is theoretically available, but often complex and poorly communicated. Parents of toddlers in many states can now access early childcare rating information through Quality Rating Improvement Systems, but in most locations these systems have low visibility. High school students in many schools have a range of courses available to them, but they may not get sufficient counseling on which course sequences best position them for postsecondary education. 
A growing body of innovative research has demonstrated that, by applying behavioral science insights to the way we communicate with students and families about the opportunities and resources available to them, we can help people navigate these complex decisions and experience better outcomes as a result. A combination of simplified information, reminders, and access to assistance have improved achievement and attainment up and down the education pipeline, nudging parents to practice early-literacy activities with their kids or check in with their high schoolers about missed assignments, and encouraging students to renew their financial aid for college. 
Big data analytic techniques have the potential to contribute to a next generation of behavioral science interventions in education—strategies that help people overcome complex matching problems like the transfer decision by giving personalized information about educational pathways that both build on the students’ prior experience and best position them for future success. Think of this as Netflix’s movie recommendation algorithm applied to education. There have already been some initial ventures in this direction. In one seminal study, researchers capitalized on student-level academic achievement data from the College Board and publicly-available data on college characteristics to send high-achieving, low-income students semi-customized packets of high-quality colleges and universities where students had a good chance of being admitted based on their academic performance. But there’s also real potential for much greater personalization. 
Take a community college student’s transfer decision as an example. Working with a state higher education system, researchers and data scientists could use a combination of articulation agreements and data from prior cohorts of students to identify optimal transfer pathways for different student groups and course-taking histories. Using data on current students’ course-taking experiences and contact information they provide to their college, the system could proactively send students messages that identifies specific institutions where they have already met the transfer requirements, or where they are on track to do so, and where prior cohorts of students from their same college have been successful upon transferring. Behavioral scientists can help design this outreach in a way that maximizes student engagement and responsiveness.  
These types of big data techniques are already being used in some education sectors. For instance, a growing number of colleges use predictive analytics to identify struggling students who need additional assistance, so faculty and administrators can intervene before the student drops out. But frequently there is insufficient attention, once the results of these predictive analyses are in hand, about how to communicate the information in a way that is likely to lead to behavior change among students or educators. And much of the predictive analytics work has been on the side of plugging leaks in the pipeline (e.g. preventing drop-outs from higher education), rather than on the side of proactively sending students and families personalized information about educational and career pathways where they are likely to flourish. 
From pre-K through college, students and families have to navigate one complex decision after another—about which schools to attend, what courses to take, which programs to apply for. These decisions are particularly daunting for socioeconomically-disadvantaged families who have less experience navigating complicated educational decisions and less access to professional advising. Families like these also have to devote a substantial portion of their cognitive bandwidth to just making ends meet financially. By leveraging big data techniques and behavioral science insights, we can help families identify educational opportunities that position them for ongoing success. 


	Related Books
	
			
			

	
		
							
									
					
	
	
						The Impoverishment of the American College Student
					
									
						By James V. Koch					
													2019
							
				

		
			
			

	
		
							
									
					
	
	
						Bridging the Higher Education Divide
					
									
						By The Century Foundation					
													2013
							
				

		
			
			

	
		
							
									
					
	
	
						First Thing We Do, Let’s Deregulate All the Lawyers
					
									
						By Clifford Winston, Robert W. Crandall, and Vikram Maheshri					
													2011
							
				

		
		

	
		
				

	
			
			B
		
	

	
							Ben Castleman
		
							Associate Professor, Education and Public Policy - University of Virginia
					
						Twitter
		bencastleman
			
		
			
		
			

			
	
				

--------------------------------------------------------------------------------

New article found for topic: data science
URL: https://www.brookings.edu/blog/techtank/2019/11/20/bridging-the-gender-data-gap/
TITLE: Bridging the gender data gap
Bridging the gender data gap
BODY:

				More men than women are killed in car crashes each year, partly because men drive more and engage in riskier driving behavior. On the other hand, women are 17% more likely to be killed and 47% more likely to be injured in crashes than men are. Women are at increased risk simply because they are women: cars are primarily designed, built, and tested by male engineers using male data, so they are built with men in mind. Scaled-down versions of male crash test dummies, meant to represent women, were not used until 2003—and are primarily tested in the passenger seat. In car design, development, and testing, male bodies are the standard and female bodies the outlier. This creates a gender data gap with very real impacts on the lives of Americans.
	
		
				

	
			
			J
		
	

	
							Jeanette Gaudry Haynie
		
							Founder and Executive Director - Athena Leadership Project					Lieutenant Colonel - U.S. Marine Corps Reserve
		
			
		
			

			
	
	
The gender data gap appears in health and medicine, too. Heart disease kills more people in the United States than anything else, accounting for approximately 20% of female deaths and 25% of male deaths. But survival rates for women are substantially worse than they are for men, and the gender data gap is partly to blame. Historically, heart disease research was primarily conducted on male subjects by male scientists and doctors, so male symptoms are considered typical and female symptoms atypical. As a result, women are misdiagnosed up to 50% more often and are more likely to be dismissed without treatment. Even when treated, women are less likely to receive needed medications or advice. And simply including women in research is not enough: when research findings are not sex-disaggregated, men are still considered the norm.[1]
The gender data gap affects military service as well. Retention of women is a perennial, persistent, and well-established problem, but the services have never meaningfully tracked why women leave the military at higher rates. A range of factors—rigid career paths, restrictive policies on parental leave and postpartum fitness, and cultural problems such as the Marines United scandal and high sexual assault rates—are suspected to disproportionately impact women. Yet the primarily-male leadership only recently adopted a recommendation to start tracking retention drivers on a voluntary basis. These efforts will remain inadequate until the scope of analysis is broadened, population pool is increased, and time has passed. Until then, the gap remains robust.
Rethinking the default
The endangerment of American citizens is concerning enough, but the gender data gap also creates a vicious cycle of underrepresentation in leadership across the nation. When leaders are primarily male, they are less likely to consider women as they develop and implement policy. And the policies they enact further neglect women, so the cycle continues. When men become the default, women are an afterthought. Their bodies, needs, and contributions are neglected, and their participation as leaders at every level is limited.
If the male perspective is assumed to be the standard and the female perspective is the “other”, the institutions that rest on that assumption, the decision-making processes leaders employ, and the policy choices that result suffer. This limits government effectiveness, and nowhere is this more critical than in the context of national security. To understand security challenges and develop means to successfully address them, leaders must seek the fullest picture possible. If women are not at the decision-making table and those in command do not consider female perspectives, that picture remains incomplete.
Closing the gap
For this nation to succeed and prosper in the future, the gender data gap must be recognized and it must be narrowed.  There are three main steps to accomplish this.


	Related
	
			
			
	
		
			
																									
		
	
	
		
					TechTank
				COVID-19 trends from Germany show different impacts by gender and age
		
							Shamika Ravi and Mudit Kapoor
																Friday, May 1, 2020
					
		
		
			

		
			
			
	
		
			
																									
		
	
	
		
					TechTank
				Combating COVID-19: Lessons from South Korea
		
							Michael J. Ahn
																Monday, April 13, 2020
					
		
		
			

		
			
			
	
		
			
																									
		
	
	
		
					Technology & Innovation
				Removing regulatory barriers to telehealth before and after COVID-19
		
							Nicol Turner Lee, Jack Karsten, and Jordan Roberts
																Wednesday, May 6, 2020
					
		
		
			

		
		

First, research must explore the gender data gap. Instead of considering the average male the norm, gender diversity in research must be actively sought. By exploring how diverse people are affected by various phenomena, research findings can inform government structures, their functions, and the policies that result.
Second, research must explore how diverse teams and leaders impact organizations, particularly in government. The business case for diversity is being established, and research into diversity in other areas is promising: research into diversity in medicine found that female doctors have higher patient survival rates, and male doctors’ performance improved when they worked with female physicians. But the rest of the country must catch up. Research can demonstrate how diversity, particularly gender diversity, affects American lives.


	Related Books
	
			
			

	
		
							
									
					
	
	
						Countering Terrorism
					
									
						By Martha Crenshaw and Gary LaFree					
													2017
							
				

		
			
			

	
		
							
									
					
	
	
						Camp David
					
									
						By William B. Quandt					
													2016
							
				

		
			
			

	
		
							
									
					
	
	
						Geopolitics in the 21st Century
					
									
						By Michael E. O’Hanlon					
													2015
							
				

		
		


Finally, leaders must actively seek diversity. While the gender data gap is narrowed through research, including the perspectives of those who live in the gap can fill in the blanks. There is precedent for this. United Nations Security Council Resolution 1325, adopted 19 years ago, established that women must be included at all levels of leadership to achieve peace and stability. The United States’ own Women, Peace, and Security Act of 2017 states that “the political participation and leadership of women…is critical to sustaining democratic institutions; and… helps promote more inclusive and democratic societies and is critical to country and regional stability.” For policies to work, institutions to function, and security to last, diverse perspectives must be included.
Until the gender data gap is narrowed, the lack of data, and the underrepresentation of women that feeds and is fed by it, will continue to limit the effectiveness of private, public, and non-profit organizations. But the gender data gap will not close on its own—we must actively seek to close it, the sooner the better. As Malala Yousafzai declared in her 2013 address at the United Nations, “We cannot all succeed when half of us are held back.”

[1] For full discussion of the extent of the gender data gap, see Invisible Women: Data Bias in a World Designed for Men by Caroline Criado Perez.
 
			

--------------------------------------------------------------------------------

New article found for topic: data science
URL: https://www.brookings.edu/blog/techtank/2018/05/17/artificial-intelligence-and-data-analytics-in-india/
TITLE: Artificial intelligence and data analytics in India
Artificial intelligence and data analytics in India
BODY:

				Advances in artificial intelligence and data analytics are propelling innovation in many parts of the world.[1] China, for example, has committed $150 billion towards its goal of becoming a world leader by 2030.[2] And while the United States government is investing only $1.1 billion in non-classified AI research, its private sector is spending billions in fields from finance and healthcare to retail and defense.[3] This is transforming a number of different sectors.[4]
	
		
				

	
			
		

	
							Shamika Ravi
		
							Non-Resident Senior Fellow - Governance Studies
					
						Twitter
		@ShamikaRavi
			
		
			
		
			

	

	
			
		

	
							Darrell M. West
		
							Vice President and Director - Governance Studies					Founding Director - Center for Technology Innovation
					
						Twitter
		@DarrWest
			
		
			
		
			

			
	
	
Yet India is playing catch-up in these vital areas. It devotes only 0.6 percent of GDP to R&D, well below the 2.74 percent in the United States and 2.07 in China.[5] Its limited investment has slowed innovation and put the country at an economic disadvantage. PricewaterhouseCoopers estimates that worldwide, AI will “increase global GDP by $15.7 trillion, a full 14%, by 2030.” But of this, $7 trillion is likely to accrue to China, $3.7 trillion to North America, and only $957 billion to India.[6]
In the last three years, India has attracted less than $100 million in AI-oriented venture capital financing.[7] According to writer Ananya Bhattacharya, “the sector is dominated by American firms like Accenture, Microsoft, and Adobe, which have their innovation centres here [in India]. Home-grown efforts on the academic, business, and investor fronts are few.”[8] Seventy percent of the AI research in the nation occurs in non-Indian firms. As an illustration, in looking at research publications, 62 percent of it comes from Google and IBM employees working in India and “there is only one Indian company in the top 10.”[9]
There is growing AI interest, however, as India starts to invest additional resources and deploy new AI applications. This year, the national government has doubled its investment in its innovation program known as Digital India to Rs3,063 crore (or $477 million) in order to fund advances in AI, machine learning, and 3-D printing.[10] The Ministry of Commerce and Industry has developed an AI Taskforce to develop policies that encourage innovation in these areas. Its recent report emphasized the need for greater investment, more AI research, revamping of school curricula, and additional innovation by the private sector.[11]
As a sign of the increased activity level, AI applications are emerging in a number of different areas that show considerable promise:
Finance
Fraud and corruption are major challenges for financial institutions and governmental overseers. A report by PricewaterhouseCoopers found that


	Related
	
			
			
	
		
			
																									
		
	
	
		
					Cybersecurity
				Why 5G requires new approaches to cybersecurity
		
							Tom Wheeler and David Simpson
																Tuesday, September 3, 2019
					
		
		
			

		
			
			
	
		
			
																									
		
	
	
		
					TechTank
				Will robots and AI take your job? The economic and political consequences of automation
		
							Darrell M. West
																Wednesday, April 18, 2018
					
		
		
			

		
			
			
	
		
			
																									
		
	
	
		
					TechTank
				Decades later, electronic monitoring of offenders is still prone to failure
		
							Jack Karsten and Darrell M. West
																Thursday, September 21, 2017
					
		
		
			

		
		

“large financial bodies such as payment regulators handle billions of transactions each day across different channels such as ATM withdrawals, credit card payments, and e-commerce transactions. Advanced analytical techniques and ML [machine learning] algorithms, combined with human expertise allow institutions to flag transactions as potentially fraudulent at the time of occurrence and hence contain the damage as early as possible.”[12]
These become particularly relevant given the recent discoveries of major fraudulent transactions in several large public and private sector banks in India. AI and ML techniques can be employed to create early warning systems and minimize human errors.
Healthcare
Advanced software helps health providers assess symptoms, diagnose disease, and plan appropriate treatments. According to writer Prakash Mallya, “the healthcare industry is relying on AI to fine-tune the accuracy of medical predictions and choose a fitting line of treatment.”[13] AI is a way to improve the quality of care while also containing medical costs. Beyond diagnosis and treatment, AI techniques and ML algorithms can also fine-tune interventions in public health policy across the country. There are large variations in the disease burden and demand for care across states of India, as well as across districts within large states such as Uttar Pradesh. The government can design real-time health interventions targeting specific populations, which is critically needed in Indian health policy.
Manufacturing


	Related Books
	
			
			

	
		
							
									
					
	
	
						Going Mobile
					
									
						By Darrell M. West					
													2014
							
				

		
			
			

	
		
							
									
					
	
	
						Digital Schools
					
									
						By Darrell M. West					
													2013
							
				

		
			
			

	
		
							
									
					
	
	
						Terms of Disservice
					
									
						By Dipayan Ghosh					
													2020
							
				

		
		

AI offers the hope of improving supply chain management and resource utilization. Both of these factors are vital for manufacturing and movement up the value chain. Factories and warehouses have difficulty managing logistics and forecasting the need for particular products. Software can help track supplies and make sure companies have what they need to make their products.[14] This is particularly important given the long term strategic role of the manufacturing sector for job creation and raising overall productivity in the economy.
Crime Prediction
Indian authorities are using AI developed abroad to anticipate crime and intervene before it happens. For example, an Israeli company called Cortica is working with the Best Group “to analyze the terabytes of data streaming from CCTV cameras in public areas. One of the goals is to improve safety in public places, such as city streets, bus stops, and train stations.” The firm is “looking for ‘behavioral anomalies’’ that signal someone is about to commit a violent crime.”[15] Given the various security risks that India faces, application of AI and ML can be instrumental in maintaining law and order as well as neutralizing extremist threats across states of the country.
Precision Agriculture
Farmers are seeking to deploy AI to increase crop yields. Using sensors that measure soil temperature and moisture, these software systems identify the ideal time for planting and harvesting, and help farmers make efficient use of pest control and fertilization. Suhas Wani, Director of the International Crop Research Institute notes “sowing date as such is very critical to ensure that farmers harvest a good crop. And if it fails, it results in loss as a lot of costs are incurred for seeds, as well as the fertilizer applications.”[16] The usage of AI is also potentially significant in weather insurance, where accurate estimates of local weather conditions are instrumental in designing insurance policies for agriculture and other industries.
Based on these examples, it is clear India needs to encourage greater AI investment, see more support from government agencies, make it possible for private firms to develop novel applications, reform education programs in order to generate better AI training, and encourage the venture capital community to invest in India. Adoption of these actions would help India expand its GDP and build greater economic prosperity in the future.

[1] Darrell M. West, The Future of Work: Robots, AI, and Automation, Brookings Institution Press, 2018.
[2] Paul Mozur, “China Sets Goal to Lead in Artificial Intelligence,” New York Times, July 21, 2017, p. B1.
[3] Greg Brockman, “The Dawn of Artificial Intelligence,” Testimony before U.S. Senate Subcommittee on Space, Science, and Competitiveness, November 30, 2016.
[4] Darrell M. West and John R. Allen, “How Artificial Intelligence is Transforming the World,” Brookings Institution report, April 24, 2018.
[5] Darrell M. West, “How the Innovation Economy Leads to Growth,” A Hearing of the Joint Economic Committee of the U.S. Congress, April 25, 2018 and Ananya Bhattacharya, “India Hopes to Become an AI Powerhouse By Copying China’s Model,” Quartz, February 13, 2018.
[6] PricewaterhouseCoopers, “Sizing the Prize: What’s the Real Value of AI for Your Business and How Can You Capitalize?” 2017 and Rekha Menon, Madhu Vazirani, and Pradeep Roy, “Rewire for Growth: Accelerating India’s Economic Growth with Artificial Intelligence,” Accenture, 2017.
[7] Ananya Bhattacharya, “India Hopes to Become an AI Powerhouse By Copying China’s Model,” Quartz, February 13, 2018.
[8] Ananya Bhattacharya, “India Hopes to Become an AI Powerhouse By Copying China’s Model,” Quartz, February 13, 2018.
[9] Smita Sinha, “Where Artificial Intelligence Research in India is Heading,” Analytics India, March, 2018.
[10] Ananya Bhattacharya, “India Hopes to Become an AI Powerhouse By Copying China’s Model,” Quartz, February 13, 2018.
[11] Artificial Intelligence Task Force, “Report of the Artificial Intelligence Task Force”, The Ministry of Commerce and Industry, Government of India, March, 2018.
[12] Arnab Basu and Sudipta Ghosh, “Advance Artificial Intelligence for Growth: Leveraging AI and Robotics for India’s Economic Transformation,” PriceWaterhouseCoopers, April, 2018, p. 13.
[13] Prakash Mallya, “India Wants to Go All in on AI, But Must First Tackle Shortage of Talent and Data,” Forbes, January 2, 2018.
[14] Arnab Basu and Sudipta Ghosh, “Advance Artificial Intelligence for Growth: Leveraging AI and Robotics for India’s Economic Transformation,” PriceWaterhouseCoopers, April, 2018, p. 13.
[15] John Quain, “Crime-Predicting A.I. Isn’t Science Fiction. It’s About to Roll Out in India,” Digital Trends, April 11, 2018.
[16] Microsoft, “Digital Agriculture: Farmers in India Are Using AI to Increase Crop Yields,” undated.
			

--------------------------------------------------------------------------------

New article found for topic: data science
URL: https://www.brookings.edu/blog/brookings-now/2018/02/09/charts-of-the-week-advancing-women-and-girls-in-science/
TITLE: Charts of the week: Advancing women and girls in science
Charts of the week: Advancing women and girls in science
BODY:

				“On this International Day, I urge commitment to end bias, greater investments in science, technology, engineering and math education for all women and girls as well as opportunities for their careers and longer-term professional advancement so that all can benefit from their ground-breaking future contributions.” — UN Secretary-General António Guterres
Three years ago, the UN proclaimed February 11 the International Day of Women and Girls in Science. This new designation was part of a larger effort toward closing gender gaps around the globe, as outline in the 2030 Sustainable Development Goals. Though more women are pursuing careers in science, technology, engineering, and mathematics (STEM), it is clear that gender gaps in these fields—and harmful biases– persist today.
Highlighted below are charts and commentary from Brookings experts on the state of gender equity in STEM fields, and the obstacles that women and girls still face.
U.S. WOMEN EARN MORE COLLEGE DEGREES THAN MEN OVERALL, BUT EARN A MINORITY OF UNDERGRADUATE DEGREES ISSUED IN STEM FIELDS
In their study of gender disparities in education and employment, Ana Maria Munoz-Boudet and Ana Revenga, two experts from the World Bank, found that gender gaps in STEM fields are common around the world. According to the authors, in 2013 only four countries in Europe produced a pool of STEM graduates that were at least 15 percent female. In the United States, despite women earning more degrees than men overall, they account for only 35 percent of the undergraduate degrees issued in STEM fields.

Munoz-Boudet and Revenga also note that in some areas, gender gaps in STEM fields are actually broadening. Between 2004 and 2014, the proportion of women earning engineering or computer science degrees in the United States fell.
STEM FIELD FACULTY REMAINS PREDOMINANTLY MALE
Gender gaps in academia are also apparent at the faculty level. In a post for the Brown Center on Education Policy, University of Missouri Professor Cory Koedel and Diyi Li examined data from over 40 public universities to explore diversity and wage gaps among the faculty. They found that women only account for 18.1-31.1 percent of faculty in STEM fields, but as much as 47.1-53.2 percent of faculty in non-STEM fields.
	
							
		
		
		
			
			
				
			
			
					
				
		
				
	
		if ( ! document.getElementById( 'simplechart-widget-js' ) ) {
			window.__simplechart_public_path__ = window.__simplechart_public_path__ ||
				"https:\/\/www.brookings.edu\/wp-content\/plugins\/wordpress-simplechart\/js\/app\/";
		}
	

More research from the Brown Center illustrates how gender harassment and hostility in academia are keeping women from ascending the ranks in fields like economics.
WOMEN ARE UNDERREPRESENTED THROUGHOUT THE INNOVATION PIPELINE
Recognizing that woman are still underrepresented in STEM fields, experts from the Hamilton Project at Brookings explored what effect that has on patenting and innovation. Women earn  57 percent of all four-year degrees, but only 35 percent of STEM bachelor’s degrees. Following degree completion, they account for just 22 percent of the STEM workforce, and are responsible for only 16 percent of granted patents.

WOMEN REMAIN UNDERREPRESENTED IN THE MOST COMMON DIGITAL AND TECH JOBS
Finally, in their study of occupational data and digitalization in the American workforce, experts from the Metropolitan Policy Program found a “mixed and sometimes surprising view” of male and female workers’ digital skills and employment. Data show that women are now slightly ahead of men as a whole when it comes to developing the digital skills increasingly essential for employment, but remain grossly underrepresented in some of the most common tech jobs such as computer programming and information systems management.

	
		
				

	
			
		

	
							Chris McKenna
		
							Office of Communications
		
			
		
			

			
	
	
			

--------------------------------------------------------------------------------

New article found for topic: data science
URL: https://www.brookings.edu/blog/techtank/2019/06/26/why-data-ownership-is-the-wrong-approach-to-protecting-privacy/
TITLE: Why data ownership is the wrong approach to protecting privacy
Why data ownership is the wrong approach to protecting privacy
BODY:

				“It’s my data.” It’s an idea often expressed about information privacy.
Indeed, in congressional hearings last year, Mark Zuckerberg said multiple times that “people own all of their own content” on Facebook. A survey by Insights Network earlier this year found that 79% of consumers said they want compensation when their data is shared. Musician and tech entrepreneur will.i.am took to the website of The Economist to argue that payment for data is a way to “redress the balance” between individuals and “data monarchs.”
	
		
				

	
			
		

	
							Cameron F. Kerry
		
							Ann R. and Andrew H. Tisch Distinguished Visiting Fellow - Governance Studies, Center for Technology Innovation
					
						Twitter
		@Cam_Kerry
			
		
			
		
			

	

	
			
		

	
							John B. Morris Jr.
		
							Nonresident Senior Fellow - Governance Studies, Center for Technology Innovation
					
						Twitter
		@jmorrisjr
			
		
			
		
			

			
	
	
Some policymakers are taking such thinking to heart. Senator John Kennedy (R-LA) introduced a three-page bill, the “Own Your Own Data Act of 2019,” which declares that “each individual owns and has an exclusive property right in the data that individual generates on the internet” and requires that social media companies obtain licenses to use this data. Senators Mark Warner (D-VA) and Josh Hawley (R-MO) are filing legislation to require Facebook, Google, and other large collectors of data to disclose the value of personal data they collect, although the bill would not require payments. In California, Governor Gavin Newsome wants to pursue a “data dividend” designed to “share in the wealth that is created from [people’s] data.”
Treating our data as our property has understandable appeal. It touches what the foundational privacy thinker Alan Westin identified as an essential aspect of privacy, a right “to control, edit, manage, and delete information about [individuals] and decide when, how, and to what extent information is communicated to others.” It expresses the unfairness people feel about an asymmetrical marketplace in which we know little about the data we share but the companies that receive the data can profit by extracting marketable information.
The trouble is, it’s not your data; it’s not their data either.  Treating data like it is property fails to recognize either the value that varieties of personal information serve or the abiding interest that individuals have in their personal information even if they choose to “sell” it. Data is not a commodity. It is information. Any system of information rights—whether patents, copyrights, and other intellectual property, or privacy rights—presents some tension with strong interest in the free flow of information that is reflected by the First Amendment. Our personal information is in demand precisely because it has value to others and to society across a myriad of uses.
Treating personal information as property to be licensed or sold may induce people to trade away their privacy rights for very little value while injecting enormous friction into free flow of information. The better way to strengthen privacy is to ensure that individual privacy interests are respected as personal information flows to desirable uses, not to reduce personal data to a commodity.
Intersecting interests in personal information
To unpack the private and public interests inherent in personal information, consider the simple case of your name. It is a fundamental part of individual identity, and it is the first item on any list of “personally identifiable information” protected by privacy rules. We each have vital interest in our names and the assets and identities we link to them.
But this interest is not exclusive. Our names are also a way that others recognize us, associated by friends and family with who and what we are, and used by society for voting rolls, property registries, financial accounts, and countless other social, economic, and civic contexts. This makes our names essential social and economic currency that is valuable to others.
It is because of this social and economic significance that naming is regulated. We are assigned names at birth, registered on birth certificates. It takes official approval to change these names. In this way, as personal as our identities are to us, they are also instruments of a well-functioning society.
Other personal information that we necessarily share alongside our names also involves intersecting interests. The transactions we conduct through a bank become part of the business records of the bank, as the Supreme Court recognized in United States v. Miller. But the bank’s interest in these records does not necessarily mean it has absolute control to do what it wants with this information, as the Gramm-Leach-Bliley financial privacy law makes explicit. Courts have relied on Miller to conclude that records of your cell phone calls and other business records can be obtained from the parties that hold these records.
In Carpenter v. United States last year, however, the Court required a warrant for cell phone location data that service providers retain for their use. Although not overturning Miller, the Court also recognized that that the phone users retain expectations of privacy in location data generated from their phones. The same intersecting interests apply to much of the other information we share as we interact with digital services—e-commerce transactions, mobile applications, messaging services and other communications, and pictures or information we share on our social networks among others.
Of course, much of the data we generate is shared more widely than just with recipients like these, but here too the interests in information from the data are not exclusive. The digital economy operates in broad ecosystems of data-sharing for diverse uses. Well-accepted examples are for credit reporting; bank clearing and compliance with know-your-customer rules; fraud monitoring; and security. Many of these functions are recognized explicitly in laws and standards—from the first federal privacy law in 1970, the Fair Credit Reporting Act, to the recent European Union General Data Protection Regulation (GDPR) and California Consumer Privacy Act—while others are invisible. Few sectors exist entirely outside such ecosystems. And as businesses outsource functions and move their data processing onto cloud services and remote sensing of all kinds begins to deploy across environments, an increasing amount of personal data is in the custody of entities unknown to us as consumers.


	Related
	
			
			
	
		
			
																									
		
	
	
		
					Privacy
				Why protecting privacy is a losing game today—and how to change the game
		
							Cameron F. Kerry
																Thursday, July 12, 2018
					
		
		
			

		
			
			
	
		
			
																									
		
	
	
		
					Technology & Innovation
				Protecting privacy in an AI-driven world
		
							Cameron F. Kerry
																Monday, February 10, 2020
					
		
		
			

		
			
			
	
		
			
																									
		
	
	
		
					TechTank
				What are the proper limits on police use of facial recognition?
		
							Nila Bala and Caleb Watney
																Thursday, June 20, 2019
					
		
		
			

		
		


The most visible and debated of these ecosystems are in advertising and marketing. They are controversial both because the visibility to consumers makes them aware of tracking and data-sharing but also because mechanisms of ad-tech like advertising identifiers and cookies enable tracking across sites and devices, sharing information without our knowledge and sometimes even despite measures we take to avoid such tracking and sharing. In turn, data brokers aggregate and supplement much of this tracking information with other data derived from apps, social media, and public databases. As intrusive as such collection and uses can be, regulation must take into account First Amendment protection, which extends in some measure to advertising, and must recognize for better or worse that advertising supports a wide range of socially useful products and services, including vital news media and other content providers.
In short, a significant amount of data sharing serves important public interests and values. The friction and disruption from any system of payments for data would undermine these interests and values. Yet, these costs would come with little benefit for individual privacy.
Instead, what is needed is to enable the sharing personal information that is useful for our social, economic, and governmental systems while protecting the vital interests that each of us has in our personal information. It is precisely to these ends that we need to move away from our current transactional approach to privacy
The handling of personal information in medical research and other research involving human subjects, detection of disparate impacts and analysis to identify discrimination, and the U.S. census provide examples where sharing of information provides important benefits for society. Most medical research is subject to the Health Insurance Portability and Accountability of 1996 and the Privacy Rule adopted by the Department of Health and Human Services (HHS) that defines numerous categories of personal information and limits how these can be disclosed and shared. Human subject research comes under HHS “Common Rule” that provides for institutional review boards to evaluate the risks to subjects, including their privacy, and how to address these risks.
The Equal Employment Opportunity Commission requires employers to report data that includes gender and race or ethnicity. Without data as to race and other protected categories, discrimination is impossible to detect. Census data is considered important enough that we compel people to respond, but the law also limits the use of this data to statistical use, a limit so deeply embedded that anyone who handles the data must swear an oath to observe the limit, violation of which is a criminal offense. Consistent with these privacy obligations, the release of census data is vetted mathematically to avoid release of information at levels granular enough to associate data with individuals.
In turn, census data provides a foundation for social science research of every kind, informing government policy at every level, academic research, and business planning and market research.  The census documentation of the U.S. population and society is available for free — an information commons created by public policy. It works fairly and consistently with our values because public policy also protects a baseline of protection for individual privacy interests. Baseline privacy requirements for how companies protect individual privacy can do the same in the commercial arena.
Why property rights in data will not protect individuals effectively
Basing privacy protection on property systems, on the other hand, would reduce privacy to a commodity, double down on a transactional model based on consumer choice, and be enormously complicated to implement. The current notice-and-choice model is failing because it is effectively impossible for users to understand either how their data will be used or the accompanying privacy risks, especially in the constant flow of online engagement in today’s connected world. The result is that people click past privacy notices through to the information or service they want.
Moreover, many of these consumers already agree to provide personal information in exchange for the perception a benefit. It is hard to imagine people will burrow deeper into privacy disclosures or pause at clicking through to get at communications or transactions simply because they are offered what may amount to a few pennies.  It is far from clear that in a market for data, the ordinary user would come out on top—either in relation to economic benefits or privacy harms. On the contrary, by licensing the use of their information in exchange for monetary consideration, they may be worse off than under the current notice-and-choice regime.
Indeed, the uncertainties of valuating any one individual’s data suggest that individuals will receive little payment. Estimates vary but The Financial Times has a calculator that one of us (Kerry) ran for his profile. The default value is $0.007, but as a well-to-do professional who travels a lot, the value of the Kerry data was estimated as $1.78. If pricing is set by service providers, then the resulting system is likely to end up being very similar to the current “take it or leave it” outcomes that are common under notice and choice. If pricing is set by consumers or through negotiation, the complexity of the service-user interactions would be even greater. And this new complexity would likely slow users’ access to information and services that they want—and simply turn “click fatigue” into “negotiation fatigue.”


	Related Books
	
			
			

	
		
							
									
					
	
	
						Terms of Disservice
					
									
						By Dipayan Ghosh					
													2020
							
				

		
			
			

	
		
							
									
					
	
	
						Growth in a Time of Change
					
									
						Edited by Hyeon-Wook Kim and Zia Qureshi					
													2020
							
				

		
			
			

	
		
							
									
					
	
	
						Behavioral Science & Policy, Volume 1, Number 2
					
									
						Edited by Craig Fox and Sim B. Sitkin					
													2016
							
				

		
		


If pricing is set by regulators or a legislature, this could require drawing of lines about the use of information based on judgments that some should be favored and others disfavored (e.g., different pricing for medical research versus commercial data brokerage). Yet this approach might risk an adverse review under the Supreme Court’s First Amendment holding in Sorrell v. IMS Health Inc., which rejected a Vermont limitation on a few particular uses of information about a pharmacy prescription transaction. Although the Sorrell case turned on particular decisions and statements made by the Vermont legislature, any privacy legislation is likely to face challenges under the First Amendment commercial speech doctrine, which encompasses commercial advertising within free speech protection on the basis of a “strong interest in the free flow of commercial information” on the part of both consumers and society. It is far from clear that a property rights approach to privacy would improve the arguments needed to withstand a constitutional challenge on such grounds.
A property-based system also disregards interests besides property that individuals have in personal information. Consumers often benefit from freely providing information for use in a particular context, but they can suffer a range of privacy harms if the information is used in an unrelated context not contemplated when use of the information was licensed. At the same, imbuing property rights in personal information would affect sharing of data in many common contexts. In a simple purchase of a book from an online retailer, for example, the book title and subject could reveal highly personal information about the purchaser, but the retailer surely would have a right to retain and use its own business records about the transaction (including the title of the book).
Under an information-as-property regime, would both the purchaser and the retailer have property rights to information about the transaction? And in such a property regime, couldn’t the retailer simply make as a condition of sale that the purchaser must grant a license to the retailer to use the information for specified uses? And wouldn’t that simply lead to another form of the tyranny of fine print in which the purchaser who wants the convenience of an online purchase would be forced to cede rights to the retailer? It is unclear whether the information as property regime would in fact improve the current state of privacy.
Looking beyond this retail scenario, how would information-as-property apply to a simple online conversation between two individuals who exchange personal information with each other—perhaps even information about personal activities that the two individuals did together? Would both individuals have property interests in the personal information about a joint activity? Would that mean that neither individual could tell anyone else about the joint activity without the permission of the other individual? (Facebook allows users to delete or port their own photographs but not other people’s photos in which they are tagged).
The European Union’s GDPR provides a benchmark for the complexity of introducing property interests into information ecosystems. The regulation required an enormous amount of systems design work to map data flows within organizations, document these for regulators, change user interfaces, and create the back-end systems to enable individual access, correction, and deletion. Microsoft revealed that it put 1,600 engineers on the task implementing the GDPR. Creating a system of micropayments similarly would add major new layers to user interfaces and back-ends systems.
There are apps and proposals that seek to protect privacy through intermediaries. MIT’s big data pioneer Alexander Pentland and Thomas Hardjono have proposed that credit unions and labor unions could use their collective role to operate data cooperatives on behalf of their members, negotiating and managing permissions as well as deriving insights for their users. In a similar vein, CitizenMe is developing a data “exchange” that enables individuals to pool their data for surveys and other uses in exchange for compensation, and also to receive develop analysis of their data. In time, a privacy marketplace may develop platforms like these. In the meantime, the challenges of implementation, adoption, and permissions management across many different contexts and interests in information means these are not a substitute for widely applicable baseline privacy legislation.
Conclusion
Alan Westin’s insight about control has a place of honor in the constellation of individual interests that make up what we call privacy. But it should not be taken literally as the entirety, it cannot be exclusive, and it is not enough to protect interests that persist in personal information even after we share it with others.
Privacy legislation should empower individuals through more layered and meaningful transparency and individual rights to know, correct, and delete personal information in databases held by others. But relying entirely on individual control will not do enough to change a system that is failing individuals, and trying to reinforce control with a property interest is likely to fail society as well. Rather than trying to resolve whether personal information belongs to individuals or to the companies that collect it, a baseline federal privacy law should directly protect the abiding interest that individuals have in that information and also enable the social benefits that flow from sharing information.
			

--------------------------------------------------------------------------------

New article found for topic: data science
URL: https://www.brookings.edu/research/the-opportunities-and-challenges-of-data-analytics-in-health-care/
TITLE: The opportunities and challenges of data analytics in health care
The opportunities and challenges of data analytics in health care
BODY:

				Data analytics tools have the potential to transform health care in many different ways. In the near future, routine doctor’s visits may be replaced by regularly monitoring one’s health status and remote consultations. The inpatient setting will be improved by more sophisticated quality metrics drawn from an ecosystem of interconnected digital health tools. The care patients receive may be decided in consultation with decision support software that is informed not only by expert judgments but also by algorithms that draw on information from patients around the world, some of whom will differ from the “typical” patient. Support may be customized for an individual’s personal genetic information, and doctors and nurses will be skilled interpreters of advanced ways to diagnose, track, and treat illnesses. In a number of different ways, policymakers are likely to have new tools that provide valuable insights into complicated health, treatment, and spending trends.
	
		
				

	
			
		

	
							Paul B. Ginsburg
		
							Director - USC-Brookings Schaeffer Initiative for Health Policy					Leonard D. Schaeffer Chair in Health Policy Studies					Senior Fellow - Economic Studies
		
			
		
			

	

	
			
		

	
							Andrés de Loera-Brust
		
							Research Intern - Center for Health Policy, The Brookings Institution
		
			
		
			

	

	
			
		

	
							Caitlin Brandt
		
							Assistant Director and Senior Research Analyst - USC-Brookings Schaeffer Initiative for Health Policy
		
			
		
			

	

	
			
		

	
							Abigail Durak
		
							Center Coordinator - Center for Health Policy, Brookings
		
			
		
			

			
	
	
However, recent developments in data analytics also suggest barriers to change that might be more substantial in the health care field than in other parts of the economy. Despite the immense promise of health analytics, the industry lags behind other major sectors in taking advantage of cutting-edge tools. Most health care organizations, for example, have yet to devise a clear approach for integrating data analytics into their regular operations. One study even showed that 56 percent of hospitals have no strategies for data governance or analytics.
Compared to other industries, the slow pace of innovation reflects challenges that are unique to health care in implementing and applying “big data” tools. These barriers include the nature of health care decisions, problematic data conventions, institutionalized practices in care delivery, and the misaligned incentives of various actors in the industry. To address these barriers, federal policy should emphasize interoperability of health data and prioritize payment reforms that will encourage providers to develop data analytics capabilities.
Despite the immense promise of health analytics, the industry lags behind other major sectors in taking advantage of cutting-edge tools.
Sensitivity of care decisions
A major barrier to the widespread application of data analytics in health care is the nature of the decisions and the data themselves. Unlike many other industries, health care decisions deal with hugely sensitive information, require timely information and action, and sometimes have life or death consequences. Each of these features creates a barrier to the pervasive use of data analytics.
The immediacy of health care decisions requires regular monitoring of data and extensive staffing and infrastructure to collect and tabulate information. The nature of health care decisions are more immediate and intrinsic than those made in other settings, creating a hesitancy about overhauling any major aspect of care provision. Health care decisions must take into account patient preferences, which at times differ from expert recommendations.


	Related
	
			
			
	
		
			
																									
		
	
	
		
					Technology & Innovation
				Removing regulatory barriers to telehealth before and after COVID-19
		
							Nicol Turner Lee, Jack Karsten, and Jordan Roberts
																Wednesday, May 6, 2020
					
		
		
			

		
			
			
	
		
			
																									
		
	
	
		
					USC-Brookings Schaeffer on Health Policy
				Receive a surprise medical bill? Here are two federal actions that may address surprise bills
		
							Margaret Darling, Caitlin Brandt, Loren Adler, and Mark Hall
																Tuesday, August 8, 2017
					
		
		
			

		
			
			
	
		
			
																									
		
	
	
		
					Technology & Innovation
				The impact of artificial intelligence on international trade
		
							Joshua P. Meltzer
																Thursday, December 13, 2018
					
		
		
			

		
		


The importance and complexity of these decisions means physicians and patients insist on very high standards for data-analytics tools in health care. That has proven very challenging to designers of these tools, as health providers are more accustomed to dealing with either broad knowledge or narrow choices rather than complex predictions that require careful identification of decisions and calibration of predictions. As a result, clinical decision support software has struggled to make better insights than physicians. Even one of the most advanced systems, IBM’s Watson, made a series of “unsafe and incorrect treatment recommendations” because it was calibrated based on synthetic cases rather than real patient data. There is risk even when training software uses real patient data because decision support software may overfit its models and thereby make less useful suggestions, such as prescribing an inappropriate treatment plan. Sometimes, the clinically best medical decision is not always what a patient wants to pursue.
The sensitive nature of health care decisions and data furthermore creates major concerns about privacy. Patients are rightfully concerned about the security of their data and concerned about it being used in ways that are detrimental to them, damage their reputations, or disadvantage them in the rating and marketing decisions of insurers. This isn’t limited to medical record data. Recent news coverage of the capture of the Golden State Killer, for example, has raised new questions about the privacy of direct-to-consumer genetic testing. And while the growth of “wearables” such as FitBit and Nike+ FuelBand have made health status monitoring accessible to patients, these data are not subjected to federal patient privacy laws, allowing these companies to design their own internal privacy policies and share information with third-parties.
Problematic data conventions
Several data conventions in health care hinder the widespread use of data analytics. Currently, health care data are split among different entities and have different formats such that building an insightful, granular database is next to impossible. These qualities greatly increase the cost of using data to provide value, even when all the relevant information has been recorded in some form. Furthermore, even well-structured data are often not available to researchers or providers who could use them in useful ways.
Several data conventions in health care hinder the widespread use of data analytics. Currently, health care data are split among different entities and have different formats such that building an insightful, granular database is next to impossible.
In general, the health care industry has been resistant to making information available as open data commons, which are up-to-date data provided in accessible format and available to all. That resistance comes in part from fear of violating privacy, even though existing strategies for protecting confidentiality greatly mitigate that risk. A larger reason is that data commons are a public good and will naturally be undersupplied by the market. A third data challenge is data quality. For analysis or predictions to have any value, they must be based on good data. One of the most hyped applications of big data in epidemiology, Google Flu Trends, turned out to underperform far more basic models, despite analyzing far more data, because its analysts were extrapolating from the behavior of Google users—an unrepresentative group of people. The experience illustrated that the success of data analytics in health care is dependent upon the availability and utilization of quality data.
Institutional practices
Entrenched practices in the delivery of health care also create several barriers to the full adoption of data analytics. One clear illustration of the challenge is in one of the most promising areas of data analytics: clinical decision support. While data analytics could greatly improve the clinical decision-making process, the development of decision support tools hasn’t paid sufficient attention to how decisions are actually made and the related workflows supporting those decisions. The tools often assume that putting the right information on a single person’s dashboard can induce them to make the right decision, but in reality, most difficult clinical decisions involve many actors and often follow institutional guidelines designed by committees. Data tools that do not fit into existing work and decision-making structures add burdens to physicians and are much less effective than they could be. For example, many attempts to bring data analytics or other information technology into health care have created a large data entry burden for physicians. This had led to high-profile mistakes, physician burnout, and general dissatisfaction with the tools.
As a consequence, most of the major reasons physicians cite for their resistance to adoption of new data tools are related to workflow disruption. For data analytics to truly transform care, the designers of tools need to cognizant of the context their tools will be used in and health care organizations must be willing to reorganize some elements of their practice to empower patients and providers to use data-driven care.
Misaligned incentives
Arguably the largest barrier to the implementation and application of data analytics in health care is the splintered landscape of the industry, with separate components having their own incentives that diverge from what might be best for the entire system. At the moment, physicians or delivery systems may not know that their patients have visited emergency rooms, for example, unless told by the insurer—because claims data are held by the payer. Meanwhile, care providers may hold clinical data that could help insurers better manage their patient’s costs. The responsibility for managing any given patient is split between their insurer and various providers, each with different incentives and needs and neither functioning as an ideal agent for the patient.
Insurers have incentives to invest in better health for their covered population, but these incentives are mitigated by annual contracts with employers or individuals and employee turnover, which moves many enrollees to a different insurer before the payer’s investments in their health pay off. There are also serious concerns with expecting insurers to take the lead on data analytics in health care. First, data tools designed for insurers are likely to center on costs, which may leave some quality-enhancing insights unexplored. Second, insurer data analytics may impose an externality on hospitals and physicians, which have to bear the administrative costs of complying with the data practices of various insurers. Third, insurers may not conduct their data analytics on a clinically useful timetable. Unless they feed data to providers continuously, it may not be timely enough to affect how patients receive care. The limited degree to which insurers provide claims data to providers that they contract with may reflect the expense of doing so, limitations in their legacy IT systems, or a desire to retain more of the care management responsibility.
The responsibility for managing any given patient is split between their insurer and various providers, each with different incentives and needs and neither functioning as an ideal agent for the patient.
Health care providers have their own particular incentives. Under the most common payment schemes, providers typically have little incentive to control patient costs. However, they likely do care about quality of care, even if they are hesitant to change their institutional practices and norms. Despite seeming like a more logical locus for data decisions, hospitals are often unwilling to undertake the costs of developing data capabilities or the disruption of implementing their use into regular practice. Hospitals also have an incentive to slow health information exchange standards because the lack of interoperability binds physicians into referral patterns favorable to them. Similarly, vendors of health information technology often don’t want standardization of data tools and practices because differentiation of their products and high costs for providers that switch vendors create substantial monopoly power for vendors. Finally, patients themselves often don’t support data practices that can improve care for all. The fear of data breaches or misuse leads patients to oppose data sharing arrangements that may have widespread positive externalities. In short, no individual actor in the health care space has the incentives or means to fully embrace the most revolutionary data analytics practices.
Policy recommendations
Because of the systemic challenges described above, we need policy changes that diminish the barriers to health analytics. While there is potential for radical overhaul, the initial priority should be making sure all hospitals can record, use, and share patient data in useful ways. One critical component of that agenda is ensuring interoperability of Electronic Medical Records (EMRs). Federal policy has contributed a great deal to the adoption of EMRs and other health IT practices through incentives under the Medicare program, but providers still struggle with sharing that data. As discussed above, neither hospitals nor EMR vendors have a strong incentive to standardize health information exchanges, despite the fact that interoperable EMRs can improve care and save money. The 2009 Health Information Technology for Economic and Clinical Health (HITECH) Act included health information exchange as one of the required capabilities for certified EMR systems. However, this requirement was included at a later implementation stage, allowing EMR systems to be designed and integrated into health systems without these capabilities, making interoperability even more difficult. In 2016, the 21st Century Cures Act increased incentives and penalties specifically promoting EMR interoperability.
These incentives need not aim to establish one universal EMR. Applications that can access and transfer health data from different kinds of EMRs can achieve interoperability, but they are not used as widely or thoroughly as possible, risking a situation where the applications meant to bridge different EMRs themselves fail to adopt uniform data conventions. Federal policy could standardize the way EMR data are accessed and transferred by applications, like Fast Healthcare Interoperability Resources (FHIR), that exist to facilitate interoperability. It could also revise HITECH and the Health Insurance Portability and Accountability Act (HIPAA) to allow fees for data exchange, thus creating incentives to improve data exchange that could potentially counteract the existing disincentives. Federal support for best practices in data management and use would go a long way in helping the industry develop its own capabilities.
The federal government can also indirectly support the development of health data analytics by continuing to encourage payment based on the value of care, typically through the Medicare program, encouraging alternative payment approaches, and by working to align quality measures and payment approaches with private insurers. Under value-based care models, providers are typically paid some amount per beneficiary based on the package of care they are expected to deliver, with payment at least partially tied to quality-of-care metrics. These models aim to create the incentive for providers to provide high-quality care at lower costs, which often involves closer coordination of care and careful revision of many practices. All these features make hospitals operating under value-based care models better loci for data-backed decisions. Kaiser Permanente has demonstrated the power of a well-integrated data strategy aimed at managing costs and quality. Conversely, improved data analytics capabilities may be precisely what health care providers need to better coordinate and improve value of care. Medicare could improve the usability of its data for a wider audience with a varying degree of analytic capabilities to help more of these providers successfully implement these new health care models. Coupling these systemic health care reforms can allow them to complement each other and reduce administrative confusion.
Federal support for best practices in data management and use would go a long way in helping the industry develop its own capabilities.
One factor that is holding back progress toward value-based payment is risk adjustment—varying the payment on the basis of how challenging one provider’s patients are in comparison to other providers. Much of the energy in improving risk adjustment has focused on contracts between purchasers and insurers—for example, between the Medicare program and Medicare Advantage plans. But the risk adjustment challenges for contracts between insurers and providers are distinct from these and, if ignored, pose grave challenges to some of the best providers, who inevitably attract patients with the most challenging conditions.
Despite the disruptions to conventional practices, all actors in health care should be excited about the possibilities that new data tools will bring. But obtaining this enormous potential is not around the corner and will require overcoming challenges by all of the relevant components of the health care system.
			

--------------------------------------------------------------------------------

New article found for topic: data science
URL: https://www.brookings.edu/blog/order-from-chaos/2018/07/02/wars-of-none-ai-big-data-and-the-future-of-insurgency/
TITLE: Wars of none: AI, big data, and the future of insurgency
Wars of none: AI, big data, and the future of insurgency
BODY:

				When U.S. Special Forces entered Afghanistan in 2001, Facebook didn’t exist, the iPhone had yet to be invented, and “A.I.” often referred to an NBA star. Seventeen years later, American special operations forces continue to ride horseback in rural Afghanistan, but information technology has advanced rapidly. Recent breakthroughs in robotics and artificial intelligence (AI) have captured the popular imagination and prompted sober talk of an impending AI revolution. Yet surprisingly little of that talk has touched on the small wars and insurgencies that have dominated U.S. foreign policy in the 21st century.	
		
				

	
			
		

	
							Chris Meserole
		
							Deputy Director - Artificial Intelligence and Emerging Technology Initiative					Fellow - Foreign Policy
					
						Twitter
		chrismeserole
			
		
			
		
			

			
	
	
The definitive work on emerging technology and insurgency has yet to be written, but two recent books offer suggestions for how the era of big data and AI will affect the United States’ modern conflicts. Small Wars, Big Data: The Information Revolution in Modern Conflict, by Eli Berman, Joseph Felter, and Jacob Shapiro, offers few musings about the future of insurgency, but lays out a compelling theory about the ways in which information shapes insurgent violence. By contrast, Paul Scharre’s excellent new book, Army of None: Autonomous Weapons and the Future of War, offers little in the way of counterinsurgency strategy, but is wholly concerned with how artificial intelligence will reshape armed conflict. Taken together, they begin to sketch out a vision for how AI and big data might alter insurgent dynamics.
The core insight of Small Wars, Big Data is that insurgencies are ultimately competitions over information rather than territory or ideology. Since insurgents can readily blend in with their surrounding populations, regime forces cannot defeat an insurgency unless the local population identifies who and where the insurgents are. The challenge for the state is thus to convince local civilians to provide that information, while the challenge for insurgents is to persuade them not to. In Berman, Felter, and Shapiro’s telling, just about everything that happens in an insurgency—from building schools and hospitals on the one hand, to the indiscriminate slaughter of civilians on the other—can be read as an attempt to coax or intimidate civilians into divulging or withholding what they know.
Small Wars, Big Data is by no means the first to offer that argument. But it is unique in terms of the breadth and depth of the empirical evidence it marshals. From the pioneering research of Stathis Kalyvas in the early 2000s on, political scientists from Lisa Hultman to Laia Balcells have compiled an extraordinary body of empirical work on the “micro-foundations” of insurgent and civil war violence. As leading contributors to that literature themselves, the authors do an admirable job of surveying its findings.
Of particular note is chapter four, which offers a thorough overview of the debate regarding the effects of developments in information technology on insurgent violence. As the authors note, on the one hand, information technology may reduce insurgent attacks by making it easier for states to gather intelligence about the insurgents. On the other, it may instead increase attacks by enabling insurgents to better coordinate with one another. By comparing the rollout of cell phone networks with insurgent violence in Iraq, the authors show that cell phones—by offering low cost, anonymous ways of supplying information about insurgents at little risk to the informant—do appear to reduce and disrupt insurgent activity. That the Taliban in Afghanistan and Boko Haram in Nigeria have both repeatedly targeted cell phone towers, despite the improved communication they enable, suggests that even insurgents themselves fear that information technology has tilted the balance of power to the state.
For all its strengths, Small Wars, Big Data is not without its flaws. One is that it gives short shrift to the kind of “brute force” tactics that Jacqueline Hazelton discussed in a controversial article last summer. The book would have been stronger if it had discussed at greater length why exploiting information technology is more effective at defeating insurgencies than draconian policies like mass incarceration, mass resettlement, or even mass killing. But the other critique is that the book’s title hints at a topic it never addresses: It is primarily a book about the role of information in insurgency, rather than information technology. The title makes for great marketing, but it’s a bit of misnomer.


			
			Related Content
		
		
		
	
		
			
																									
		
	
	
		
					Order from Chaos
				The West is ill-prepared for the wave of “deep fakes” that artificial intelligence could unleash
		
							Chris Meserole and Alina Polyakova
																Friday, May 25, 2018
					
		
		
			


	
		
			
																									
		
	
	
		
					Order from Chaos
				How misinformation spreads on social media—And what to do about it
		
							Chris Meserole
																Wednesday, May 9, 2018
					
		
		
			


	
		
			
																									
		
	
	
		
					Order from Chaos
				Spreading terror: How the Toronto attack echoes recent trends in extremist violence
		
							Chris Meserole
																Wednesday, April 25, 2018
					
		
		
			

	
	
Army of None, by contrast, more than lives up to its billing. Scharre has spent nearly a decade framing the early debate over autonomous weapons in D.C. and the Pentagon, and the experience shows. The book plainly and masterfully lays out the major questions that AI and autonomous weapons raise for the future of armed conflict. Although written for a popular audience, even well-informed academics will find it worthwhile as an introduction to the technical, ethical, and strategic issues that AI-infused weapons systems will introduce.
Scharre’s argument about the coming ubiquity of big data and AI has profound implications for the future of insurgency. Very roughly, two futures are possible. In one, AI and autonomous weapons are both distributed and commoditized, such that insurgents can afford weapons systems that are nearly as capable as those of any given regime. Think of the commercial off-the-shelf drones that the Islamic State has deployed in Iraq and Syria, but with low-level intelligence and object-detection baked in via Tensorflow. Since many of the cutting-edge AI projects are open source and publicly available, it may well be possible to build makeshift lethal autonomous weapons systems that are nearly as good as state-of-the-art systems but at a fraction of the cost. In such a scenario, the balance may shift slightly to insurgents; one shudders to imagine the carnage a Mumbai-style attack could produce if the attackers had lethal drone swarms at their disposal.
However, if the future of artificial intelligence is one that favors scale and centralization, then AI may give the upper hand to regime forces. In this world, regimes with access to the products and infrastructure of a great power, like the United States or China, may rapidly unravel insurgent networks. We might think of this as the “wars of none” scenario, since the regime’s overwhelming informational advantage may come to limit the need for violence altogether.


	Related Books
	
			
			

	
		
							
									
					
	
	
						Terms of Disservice
					
									
						By Dipayan Ghosh					
													2020
							
				

		
			
			

	
		
							
									
					
	
	
						Growth in a Time of Change
					
									
						Edited by Hyeon-Wook Kim and Zia Qureshi					
													2020
							
				

		
			
			

	
		
							
									
					
	
	
						Nuclear Weapons and American Grand Strategy
					
									
						By Francis J. Gavin					
													2020
							
				

		
		

Recall that the central point of Small Wars, Big Data is that insurgencies are primarily contests for information about the identity and location of insurgents, and that even relatively simple technology—such as a text-based tipline—appears to make it much easier for states to gain that information. What happens when tiplines are replaced with real-time surveillance systems equipped with facial recognition? As the cost of ubiquitous surveillance drops, we may see the emergence of a new counterinsurgency strategy: Whereas the “hearts-and-minds” approach tries to coax information out of civilians and the “brute force” approach attempts to coerce it, the “big brother” approach may bypass civilians altogether. There’s little need for informants if you have enough sensors, cameras, and processing power to recognize and track everyone, everywhere.
Although such a scenario may seem far-fetched, early versions are already feasible. In the United States, Anduril Industries, the latest security startup funded by Peter Thiel, is fast at work building an “electronic” wall that has already proven remarkably effective at detecting and monitoring unauthorized border crossings in Texas. In China, meanwhile, authorities have responded to deadly attacks in Xinjiang by widely deploying facial recognition and mass surveillance technology there. And the capabilities of Chinese authorities are only set to grow: Beijing recently announced a massive new investment in SenseTime, an AI startup whose next generation product aims to identify objects and individuals across 100,000 live camera feeds simultaneously.
Whether AI and information technology will empower states or insurgents remains unclear. Most likely it will do both, with AI and information technology supercharging insurgencies in failing or weak countries while constraining the space in which insurgency can occur everywhere else. What seems certain is that the information revolution is poised to revolutionize insurgency soon too.
			

--------------------------------------------------------------------------------

New article found for topic: data science
URL: https://www.brookings.edu/blog/future-development/2018/05/03/using-big-data-to-link-poor-farmers-to-finance/
TITLE: Using big data to link poor farmers to finance
Using big data to link poor farmers to finance
BODY:

				Two billion adults in the world are excluded from credit. The situation is especially bad for small farmers in rural areas who are unable to access loans to invest in their farms, trapped in a vicious circle of low productivity, low yields, and poor income. The Initiative for Smallholder Finance estimates that smallholders globally access just $50 billion of the $200 billion of lending that they require to grow their operations and improve their lives.	
		
				

	
			
		

	
							Roy Parizat
		
							Fund Manager, BioCarbon Fund Initiative for Sustainable Forest Landscapes - World Bank
		
			
		
			

	

	
			
			H
		
	

	
							Heinz-Wilhelm Strubenhoff
		
							Agribusiness Program Manager, World Bank Group
		
			
		
			

			
	
	
The global growth of microfinance banks has created new opportunities for financial inclusion, with outstanding lending of $100 billion to around 200 million clients. Yet the majority of lending from microfinance institutions has been to urban populations and not to the rural poor or small farmers.
Farmers with access to finance can invest in fertilizers and seeds to increase their yields and incomes potentially by more than 25 percent.
There are well known reasons that make lending money to farmers so hard. Farmers are spread out across large thinly populated areas making them costly to reach and serve; they have few realizable assets to pledge as collateral against loans; and they generally have no financial history, which is needed for credit scoring. And on top of all that, agriculture is regarded as a risky business, often at the mercy of the weather. Yet farmers with access to finance can invest in fertilizers and seeds to increase their yields and incomes potentially by more than 25 percent.
New technologies have the potential to give farmers access to the financial tools necessary for such growth.
The rise of mobile phones in rural areas has been dramatic with over 90 percent of Africans able to access cellphone services, and many farmers now use cellphones to make calls, send text messages, and access data and informational services. The adoption of mobile money services has mirrored cellphone growth, allowing users to send and receive funds using a network of local agents.
Figure 1: The rapid rise of mobile money in Africa

Source: Chart adapted from “The Mobile Economy Sub-Saharan Africa 2017”
Big data analytics for credit risk assessment has been on the rise across urban areas of developing countries over the past years. Several startups offer loans to previously un-bankable customers using cellphone data (call data records, mobile money data, and social media data). This data is processed by algorithms, generating a predictive score of a person’s likelihood to repay a loan.
Crop suitability mapping enables lenders to understand how risky a farmers production is, based on where they are located (using geo-mapping techniques, lenders are able to ensure that they only lend to a farmer growing a suitable crop in a suitable location).
Can this new technology really make farmers bankable?
Imagine a poor farmer in Ghana that has been growing beans to sell into the local market. After providing for her family, she doesn’t have enough money left over to purchase high quality seeds or fertilizers, even though such inputs would increase the amount of beans she will grow. She tried borrowing from the local microfinance bank, but since she had never saved with them before, they are not willing to grant her a loan. Suddenly she receives a text message asking if she’d like to apply for a loan to pay for inputs. She clicks “yes,” and receives an automated message saying she has received the loan. The loan automatically goes into her mobile money account, and she visits the supply store to purchase seeds and fertilizers. Next harvest she produces 30 percent more beans than usual. After selling her crops she visits the local mobile money agent to repay the loan and interest. As a result, she is left with more money than normal, so she joins the local microfinance bank to open a savings account with the surplus funds. Now, she is an individual with a bank account opening further opportunities for her and her family.
The story above is not yet happening but is a distinct possibility and illustrates how technology can transform a farmer into a profitable borrower. While cellphone-based lending is starting to happen in Africa, with products such as MShwari in Kenya, this type of lending so far has been mainly for small, short-term loans for urban customers. Farmers who are less tech-savvy and need larger, longer-term loans repayable at the end of the season cannot benefit from existing mobile loan services. Yet by transforming the application process into a simple automated cellphone-based application, transaction costs plummet and remote farmers can access the service. By using cellphone records for credit scores, previously “un-scorable” customers are now ratable. Using agronomic geospatial data ensures that lending is only given to farmers growing the right crops in the right location. Migrating cash management to mobile money platforms dramatically reduces money disbursement and recovery costs. The mix of these technologies could grant access to seasonal lending to smallholder farmers.
Perhaps the nearest example of this technological convergence to date is Safaricom’s DigiFarm and Connected Farmer model in Kenya. The platform offers a range of services to farmers including e-vouchers for quality inputs at discounted prices; access to agriculture advice services via a training module; and advice on how to appropriately use inputs like seed and fertilizer. Digifarm also offers farmers the opportunity to apply, via their cellphones, for a loan to be used to purchase agricultural inputs, although data on total lending and repayment rates is not yet available. The loan module appears to build upon their proven success with the M-Shwari mobile microloan available to Sarfaricom customers.
If Digifarm and other similar services achieve success, we are likely to see many other mobile companies, banks, and startups replicate their models.  Most exciting of all is the impact that this will have on farmers who will have increased opportunities to invest in their endeavors.

	Related
	
			
			
	
		
			
																									
		
	
	
		
					Africa in Focus
				Impact of COVID-19 on micro, small, and medium businesses in Uganda
		
							Corti Paul Lakuma and Nathan Sunday
																Tuesday, May 19, 2020
					
		
		
			

		
			
			
	
		
			
																									
		
	
	
		
					Report
				The Fourth Industrial Revolution and digitization will transform Africa into a global powerhouse
		
							Njuguna Ndung’u and Landry Signé
																Wednesday, January 8, 2020
					
		
		
			

		
			
			
	
		
			
																									
		
	
	
		
					Future Development
				4 lessons for developing countries from advanced economies’ past
		
							Ivailo Izvorski and Kenan Karakülah
																Wednesday, February 20, 2019
					
		
		
			

		
		


			

--------------------------------------------------------------------------------

New article found for topic: data science
URL: https://www.brookings.edu/blog/education-plus-development/2018/05/18/identifying-student-readiness-through-science-learning-progressions-in-the-philippines/
TITLE: Identifying student readiness through science learning progressions in the Philippines
Identifying student readiness through science learning progressions in the Philippines
BODY:

				Learning progressions have been described as roadmaps that help align curriculum, pedagogy, and assessment. These roadmaps point out important locations on the typical journey from novice to expert by describing what someone at each important location on the roadmap knows and can do. The descriptions highlight what is unique about each location, ensuring that differences between locations are emphasized so the transformations in skills and knowledge along the learning journey can be recognized. By mapping the journey, learning progressions help ensure the curriculum developers, teachers, and assessment designers are all working with the same destination in mind.
Increasingly, learning progressions are being used both to construct assessment tools that will reflect the learning pathway, as well as to locate how students are meeting learning goals. Focussing on these two parallel functions can be useful to central education systems in evaluating new curricula.	
		
				

	
			
			P
		
	

	
							Pam Robertson
		
							Research Fellow - University of Melbourne; Assessment Curriculum and Technology Research Centre
		
			
		
			

	

	
			
		

	
							Esther Care
		
							Senior Fellow - Global Economy and Development, Center for Universal Education
					
						Twitter
		Care_Esther
			
		
			
		
			

	

	
			
			M
		
	

	
							Marlene Ferido
		
							Curriculum Program Leader - Assessment Curriculum and Technology Research Centre NISMED, University of the Philippines
		
			
		
			

			
	
	
Different types of curriculum design require learning progressions of different grain sizes. When developing multi-year curricula, designers require coarse-grained learning progressions which describe learning over a long time period. An example from the USA illustrates how levels on a learning progression in science can be mapped to two or three grades each so the curriculum can be designed accordingly. This approach provides the opportunity to design evidence-based curricula which mirror the typical patterns of development for learners across grades because the learning progressions are founded on research.
In the Philippines, we have applied this approach through fine-grained learning progressions that are being used to monitor student progress through the new science curriculum for Grades 7-10. The curriculum presents increasing levels of complexity from one grade to another in an upward spiraling progression, to develop deep understandings of core science concepts and skills. This provides the opportunity to construct tests that reflect this progression, and the test results can be used in multiple ways.
We wanted to know whether students entering a grade had the pre-requisite knowledge, skills, and understanding needed to be able to engage successfully with the science chemistry curriculum at that grade level. To develop the tests used in this research, and with the support of the Philippines Department of Education, the Assessment Curriculum and Technology Research Centre (ACTRC) collaborated with colleagues from University of the Philippines National Institute for Science and Mathematics Education Development (UP NISMED), University of Philippines College of Education, and its Integrated School to audit the curriculum and identify the prerequisite conceptual knowledge and skills that are necessary for students to engage with the chemistry content in each of Grades 7-10. Using the audit information, test items were written to assess mastery of the identified concepts and skills.
As students entered into each of the grade levels, they took the grade-appropriate test, which provided information about their readiness to engage with the curriculum. The results, aligned with the progressions, captured the levels of proficiency of students within each successive grade. Some students were still at early locations along the learning journey, while others had progressed further and employed more sophisticated chemical thinking. The levels on each learning progression were then compared to the pre-requisite skills and knowledge expected by the grade level curriculum and the percentage of students who are ready (and not ready) for the curriculum content were calculated.

			
			Related Content
		
		
		
	
		
			
																									
		
	
	
		
					Education Plus Development
				Using learning progressions in teaching students with disabilities in inclusive classrooms
		
							Kerry Woods
																Thursday, April 26, 2018
					
		
		
			


	
		
			
																									
		
	
	
		
					Education Plus Development
				Teaching preschoolers learning strategies: ‘What’ meets ‘how’
		
							Felicia R. Truong, Abby G. Carlson, and Helyn Kim
																Friday, April 13, 2018
					
		
		
			


	
		
			
																									
		
	
	
		
					Education Plus Development
				Learning progressions: Pathways for 21st century teaching and learning
		
							Helyn Kim and Esther Care
																Tuesday, March 27, 2018
					
		
		
			

	
	

Box 1 shows four levels of performance: A to D on the pre-Grade 8 test. From the expert identification of what level of skill is required to enable students to engage successfully with the Grade 8 chemistry curriculum, students in Level C and above were found to be ready for the Grade 8 curriculum, while students in Level B and below were found to be unready. The differences between the knowledge and skills at Level B and Level C included the crucial distinction between representations of matter where differences can be seen by the naked eye and representations of matter where differences cannot be seen even with a microscope and must be understood conceptually. The results suggest that this fundamental change in how scientific knowledge is represented is one for which many of the students were unprepared when entering Grade 8 chemistry.
Box 1. Levels through the Grade 8 chemistry curriculum



Level D – Students at this level are starting to interpret pH scales to make inferences about acidity basicity. They are beginning to make deductions based on their understanding of scientific terminology (element, compound, mixture, density). They are starting to recognize that gases are compressible.


Level C – Students at this level are starting to use the appropriate scientific terminology to distinguish between sub-microscopic particles (atom, molecule, compound, element, mixture, proton, electron, neutron). They are learning to use chemical symbols and to connect the position of an element on the periodic table with its classification as a metal/non-metal. They are beginning to make inferences from experimental data presented in tables and graphs in order to identify trends and explanations and to draw conclusions.


Level B – Students at this level are beginning to recognize the particulate nature of matter, e.g., that ice and water are the same substance with different arrangements of particles. They are learning to use scientific terminology to describe the properties of metals and non-metals (malleable, ductile, brittle) and recognize that heating causes expansion. Students are starting to read instruments accurately, convert between different units of measurement, choose the correct scale for accurate measurement, and link given graphs and diagrams with written descriptions.


Level A – Students at this level are learning the macroscopic properties of solids, liquids, and gases. They are beginning to classify metals and non-metals based on their properties; that metals conduct heat; that non-metals are not attracted by magnets. Students are starting to use data in tabulated form.



The use of student achievement data marked against the learning progressions continues to provide the information needed to evaluate how the Philippine Science curriculum is performing. In working with the Department of Education in its first few years of implementing the new K–12 curriculum, the study demonstrates that there are some concerns with current implementation. Although the data capture what students can do and cannot do at each level of the progression, they don’t answer why this is happening—that is an issue that the Department of Education is exploring. What the data do provide is the essential information about whether students are achieving in line with the system’s new learning goals.
			

--------------------------------------------------------------------------------

New article found for topic: data science
URL: https://www.brookings.edu/blog/education-plus-development/2018/04/25/drawing-from-improvement-science-to-bridge-education-research-and-practice/
TITLE: Drawing from improvement science to bridge education research and practice
Drawing from improvement science to bridge education research and practice
BODY:

				A three-legged stool can be tough to balance if the legs are uneven. We’ve been thinking a lot about how to do just that with the launch of the  Millions Learning Real-time Scaling Labs starting in Brazil, Jordan, Tanzania, Côte d’Ivoire, and the U.S. city of Philadelphia. How do we balance and give equal weight to our three primary objectives: learn from, document, and support education interventions in the process of scaling? We think that drawing from principles behind adaptive and iterative learning methodologies, such as improvement science, can help.	
		
				

	
			
		

	
							Jenny Perlman Robinson
		
							Senior Fellow - Global Economy and Development, Center for Universal Education
					
						Twitter
		@JennyPerlman
			
		
			
		
			

			
	
	
Our starting point with the scaling labs is to learn more about how effective education interventions expand and deepen their impact—what we refer to as scale—especially among some of the world’s most marginalized communities. The global education community is learning a lot about how best to scale, some of which is seen in our 2016 report Millions Learning: Scaling Up Quality Education in Developing Countries. But there is certainly a lot more to learn, particularly around the core drivers behind adapting an evidenced-based initiative to a new context as well as about the enabling conditions in the environment that allow for good ideas to take root and spread. Through the lab process, we hope to learn even more about this scaling as it unfolds in real-time.
Secondly, we hope to document this process so that we gain deeper insight into the scaling journey, not only to understand what outcomes are achieved but, just as importantly, to understand how they are achieved, and to be able to “tell the story of what happened” so that other decisionmakers can learn from it. We are thinking a lot about how best to capture and share this insight in an actionable way to help inform future policy, financing, and programmatic decisions around scaling in education.
And the final leg of the stool is helping to support scaling efforts at the center of the Real-time Scaling Labs. Rather than merely learning from and documenting what is happening as external, silent observers, instead we are co-creating a peer-to-peer, iterative learning approach with partner institutions around the world to actively support the scaling process. During the three to four-year long lab process, we will routinely convene a diversity of stakeholders involved in the initiative(s) to identify scaling goals, develop and/or refine scaling action plans, and engage in a participatory and iterative process of implementing the scaling plan, reflecting on progress, and making course corrections along the way. The intention is to help strengthen capacity to scale among decision-makers through this participatory and applied research approach.
In designing these Real-time Scaling Labs, we certainly didn’t start from scratch. We spent the past year researching a wide range of adaptive learning and collective impact approaches and considering how these methodologies might be applied to challenges around scaling in global education. One of the most useful approaches that we have drawn from is improvement science, a user-centered, problem-driven, iterative approach designed to accelerate learning-by-doing. Originally pioneered in the healthcare field by the Institute for Healthcare Improvement, improvement science uses iterative cycles to rapidly test change ideas, and adjust based on data in a systemic rigorous way. I had the opportunity to attend the Carnegie Foundation for the Advancement of Teaching’s annual Summit on Improvement in Education earlier this month in San Francisco. At the conference, I heard about three key components of improvement science that really resonated with what we are trying to create with the scaling labs.
1. Possibly incorrect and definitely incomplete
Improvement science begins with the notion that the ideas we wish to test about how to improve education are possibly incorrect and definitely incomplete, and so will need to be reconsidered, adapted, and possibly even abandoned as we learn more. This notion is also at the core of the Real-time Scaling Labs, recognizing that scaling is not a linear process but is, in fact, cyclical, with ongoing challenges to overcome and opportunities to seize. We recognize that the first plan developed to scale an education intervention will most certainly not be the final one; it must change and be adapted throughout the process of scaling. Therefore, if we want to scale effectively and sustainably, we must continue to assess and adapt our strategies in response to ongoing changes that occur within any complex system, reacting to real-time learning rather than strictly adhering to our first assumptions. This underscores the importance of taking a flexible approach to designing, delivering, and financing interventions for scale. Part of this is also recognizing that setbacks will be an unavoidable and important part of the journey as they are critical moments to learn.
2. Every system is perfectly designed to get the results that it gets
Improvement science recognizes that we don’t exist in silos and vacuums. If we want to affect large-scale change, we must start by focusing on the broader system and understanding what in the system is producing the outcomes we currently see. Similarly, the scaling labs begin by convening a diverse group of stakeholders in-person to discuss and assess the nature of the problem the intervention(s) seeks to address and the system that produces it. By taking a systems approach, we are able to understand and address the processes and mechanisms within the system that are hindering the type of improvement we seek and to tackle the underlying causes behind the problems we are trying to solve. Additionally, by deliberately engaging a diverse composition of participants in the labs, we recognize the importance of getting multiple perspectives, as one person can only see part of the system.

Source: The Baseline Company www.theblindelephant.com
3. Control urges for solutionitis
Before jumping to any conclusions about a potential solution, improvement science requires asking deeper questions about the data to truly understand the actual problem to be solved. An excellent speaker in one of the sessions I attended gave the example of the Lincoln and Jefferson Memorials, which were decaying at an increasingly rapid rate. Evidence suggested it was a result of the cleaning process. Rather than rushing to make changes, improvement science led to asking why the monuments were cleaned so frequently and with such harsh chemicals. They found the frequent cleaning was to remove the large amount of bird droppings on the memorials. Why were so many birds flying over? Because of a large spider population on the memorials attracting the pigeons as food sources. And why so many spiders? Because of gnats swarming the monuments at dusk. And why so many gnats? They were drawn by the lighting of the monuments. This series of deep questions led to the decision to turn on the memorial lights one hour later, after sunset, eliminating the root cause of the problem (gnats). This will also be a core tenant of the scaling labs, as we will not be satisfied with identifying scaling challenges and opportunities to leverage at a surface level and jumping immediately to finding solutions, but rather will spend time to collectively understand the root causes and ensure our scaling plans seek to take those into account.

	Related Books
	
			
			

	
		
							
									
					
	
	
						Leapfrogging Inequality
					
									
						By Rebecca Winthrop; With Adam Barton and Eileen McGivney					
													2018
							
				

		
		


In designing the Real-time Scaling Labs, we are fortunate that improvement science has a rich history for us to learn from and experts around the world who are using this approach to tackle some of the most complex social challenges facing our societies, thus helping us to keep our three-legged stool in balance.


			
			Related Content
		
		
		
	
		
			
																									
		
	
	
		
					Education Plus Development
				Real-time scaling, the 2018 World Development Report, and getting millions learning
		
							Jenny Perlman Robinson and Molly Curtiss
																Friday, October 6, 2017
					
		
		
			


	
		
			
																									
		
	
	
		
					Education Plus Development
				How to get millions learning: From evidence to action
		
							Jenny Perlman Robinson and Molly Curtiss
																Wednesday, April 19, 2017
					
		
		
			


	
		
			
																									
		
	
	
		
					Report
				Millions learning: Scaling up quality education in developing countries
		
							Jenny Perlman Robinson, Rebecca Winthrop, and Eileen McGivney
																Wednesday, April 13, 2016
					
		
		
			

	
	

			

--------------------------------------------------------------------------------

New article found for topic: data science
URL: https://www.brookings.edu/opinions/the-smart-society-of-the-future-doesnt-look-like-science-fiction/
TITLE: The “smart society” of the future doesn’t look like science fiction
The “smart society” of the future doesn’t look like science fiction
BODY:

					
		
				

	
			
		

	
							Bhaskar Chakravorti
		
							Non-Resident Senior Fellow - Brookings India
		
			
		
			

	

	
			
			R
		
	

	
							Ravi Shankar Chaturvedi
		
							Associate Director for Research - Fletcher’s Institute for Business in the Global Context, Tufts University					Doctoral Research Fellow for Innovation - Fletcher’s Institute for Business in the Global Context, Tufts University
		
			
		
			

			
	
	
What is a “smart” society? While flights of imagination from science-fiction writers, filmmakers, and techno-futurists involve things like flying cars and teleportation, in practice smart technology is making inroads in a piecemeal fashion, often in rather banal circumstances. In Chicago, for example, predictive analytics is improving health inspections schedules in restaurants, while in Boston city officials are collaborating with Waze, the traffic navigation app company, combining its data with inputs from street cameras and sensors to improve road conditions across the city. A city-state such as Singapore has a more holistic idea of a “smart nation,” where the vision includes initiatives from self-driving vehicles to cashless and contactless payments, robotics and assistive technologies, data-empowered urban environments, and technology-enabled homes.
To read more, please click here.


	Related
	
			
			
	
		
			
																									
		
	
	
		
					U.S. Economy
				Trump’s formula for growing the U.S. economy—what will work and what won’t
		
							Martin Neil Baily
																Friday, February 16, 2018
					
		
		
			

		
			
			
	
		
			
																									
		
	
	
		
					The Avenue
				How historic would a $1 trillion infrastructure program be?
		
							Adie Tomer, Joseph W. Kane, and Robert Puentes
																Friday, May 12, 2017
					
		
		
			

		
			
			
	
		
			
				I																					
		
	
	
		
					Report
				It is time to get past the “single story” about Africa
		
							Bhaskar Chakravorti
																Tuesday, October 20, 2015
					
		
		
			

		
		



	Related Books
	
			
			

	
		
							
									
					
	
	
						Terms of Disservice
					
									
						By Dipayan Ghosh					
													2020
							
				

		
			
			

	
		
							
									
					
	
	
						Growth in a Time of Change
					
									
						Edited by Hyeon-Wook Kim and Zia Qureshi					
													2020
							
				

		
			
			

	
		
							
									
					
	
	
						Behavioral Science & Policy, Volume 1, Number 2
					
									
						Edited by Craig Fox and Sim B. Sitkin					
													2016
							
				

		
		

			

--------------------------------------------------------------------------------

New article found for topic: data science
URL: https://www.brookings.edu/blog/techtank/2016/03/10/idea-to-retire-viewing-data-as-an-output/
TITLE: Idea to retire: Viewing data as an output
Idea to retire: Viewing data as an output
BODY:

				It is rare to talk to federal administrators and not hear something about “data-driven decision making.” Administrators dedicate significant time and information technology resources to decisions about their agency’s viability and to defending their standing in appropriations discussions. However, the questions asked of the data by senior officials for these purposes are dramatically different than the questions asked by a front line case manager for a social service organization.  Unfortunately, it is the former that has the loudest voice in procurement and system design conversations.  The prevailing top-down approach to data collection and use also puts little actionable information in the hands of the people responsible for gathering data. As a result, the incentive to collect data is diminished while opportunities for informed program improvements and abilities to measure outcomes against costs are missed. A small shift in our approach to data collection and visualization could lead to big improvements in the effectiveness of social service delivery.
While the effort to collect and analyze data gives the impression of transparency, we are actually looking at our institutions through a fun house mirror. Frameworks are geared toward only answering a small set of high-level questions, creating a false sense of openness that limits the policy discourse around those agencies to their funding—not their effectiveness. 
It is time for this “good on the surface” notion to die, and in its place we must re-orient our data collection strategies to inform our work as it happens. 
Make no mistake—for the vast majority of agencies this is a wholesale paradigm shift with significant implications for design, implementation, and evaluation of social service delivery.
Collecting the wrong data
Take the case of homelessness organizations. Social workers across the country bring homeless families into a network of wrap-around services available through several state and local agencies. With every meeting, they collect qualitative and quantitative data about clients and update the case management system that loosely connects to that network of providers. If effectively analyzed and visualized, this information would likely help spot warning signs of future trouble for the family and would give social workers a chance to make a meaningful early intervention.  Visualizing the data could also inform the network of providers of interventions that are working so they can put resources to the most effective programs.
However, the data entered in the system is often not used to produce the types of insights needed to help best manage cases. Instead, the data is sent to analysts at The Department of Housing and Urban Development (HUD) who, once a year, release large, static reports including the Annual Homeless Assessment Report (AHAR), Annual Progress Report (APR), and HEARTH Performance Measures. HUD has strict guidelines and complex coding systems for how data can be entered, stored, and used by community service providers. The mandated once-yearly reports are the primary form of feedback case managers receive from the information they’ve entered and contain very little useful information for day-to-day client service. 
Where this backward looking systemic analysis is valued is among the policymakers making budgetary decisions. Forensic analysis of program effectiveness is usually measured along broad performance indicators like individuals served, cost-per-beneficiary, number of people who have exited from system dependence. But these reactive measures do very little to help social workers ensure that client families make progress and don’t slide back into poverty. Without that direct connection to their work, it is inevitable that data collection and analysis becomes an infrequent and low-value administrative task on top of an already large workload.
Shifting mentalities
As Andrew Means describes it, we need to “upgrade from an output mentality to an input plus output mentality” when it comes to using data in this context. Means, the founder of Data Analysts for Social Good and The Impact Lab, works with organizations to update data collection techniques and use data science to solve longstanding social problems. He argues that too many public institutions view data exclusively an output of a program to show whether or not the original design was effective. 

  


	Related Books
	
			
			

	
		
							
									
					
	
	
						Constitution 3.0
					
									
						Edited by Jeffrey Rosen and Benjamin Wittes					
													2013
							
				

		
			
			

	
		
							
									
					
	
	
						The Need for Speed
					
									
						By Robert E. Litan and Hal J. Singer					
													2013
							
				

		
			
			

	
		
							
									
					
	
	
						After the Breakup
					
									
						By Robert W. Crandall					
													2010
							
				

		
		


                                               Output mentality                                


  


  

	
		
				

	
			
			N
		
	

	
							Neal Myrick
		
							Neal Myrick is director of social impact at Tableau Software and director of Tableau Foundation, which encourages the use of facts and analytical reasoning to solve the world’s problems. Neal has served in both private and nonprofit senior leadership positions at intersection of information technology and social change.
		
			
		
			

	

	
			
			S
		
	

	
							Steve Schwartz
		
							Steve Schwartz is the marketing manager for Tableau Software’s Social Impact efforts and supports the Tableau Foundation’s work. He is also a co-founder of Upaya Social Ventures, a nonprofit that is building businesses and creating jobs in India’s poorest communities. 
		
			
		
			

			
	
	
  
    


                                              Input mentality

Instead, Means advocates for primarily viewing data as an input—information that establishes a baseline of understanding and makes predictive modeling possible alongside forensic analysis. He has seen how individuals who see a far greater value in the data are more likely collect data faithfully and accurately. Better data leads to better analysis helping inform programs, making more targeted, efficient, and effective program work is possible. 
Drawing on our previous example, using data as an input allows agencies to estimate which beneficiaries are most likely to drop out of a homeless support program or which program they would be best served by. This is clearly in an agency’s best interest. Not only is the agency able to focus human and financial resources where they are most effective, but overall improvements in outcomes will reflect positively in conventional budgetary discussions. 
Higher value data = better transparency
This brings us back to the question of transparency and value. Data that is not valued is not rigorously collected or validated. Moreover, because the analysis is only interesting to a small group of administrators, reporting is an afterthought. That is why agencies often dump a 25 column, comma-delimited file with 10,000 rows of data on their website and assume they are meeting their disclosure obligations. This information is virtually inaccessible to the average person in that form. It exists to validate past decisions instead of informing future ones, and the by-product of that approach is a policy discourse that cannot leverage data to advance our service delivery beyond the status quo. 
For social workers, agency directors, and funders, the challenge is not to simply make the data available but to make it useful for everyone in the decision chain. By using contextual resources like data visualization, individuals are empowered to act and ask more, better questions of the data. For a social worker, this could be an opportunity to use data in their work with individuals or to coordinate with partners who provide additional services to clients. Further upstream, institutions have an opportunity to articulate their value, showcase their progress, and advocate for a broader understanding of issues critical to their work. 
Not only will this increase public understanding of key issues, but also increase the accountability of agencies and decision makers to their constituencies to the benefit of our broader society.
By taking a grassroots approach to data system design, we can put analytical power back into the hands of the people who can most effectively use it for change. For example, the Community Technology Alliance partnered with our organization to put the HUD data discussed above into a series of interactive reports that will help homelessness agencies nationwide and give them an opportunity to combine that data with their proprietary data to draw out more valuable insights. Modeled as a nationwide Data Fellowship, the program trains a group of 20 homeless services data enthusiasts across the country on analytical tools and visualization best practices. In addition to building a first set of field-usable visualizations, the fellows will also train their peers on the value of collecting and using data in their work.
This is not just an effort to tackle homelessness and create change within one social service organization’s reporting system. It is an effort to create a new standard for information utility and transparency that all organizations can follow based on a few simple principles:

Data must be useful to those responsible for collecting it and acting on it.
More useful data for local decision-makers means better data for federal and state funding agencies as well as taxpaying citizens
Data and the story it tells must be accessible and easily understood at all levels

This is our challenge to government information technology leaders—let the old output-only notion of data driven decision making die. Build systems from where the information is most valuable, easily understood, and actionable and let them yield better data. Encourage transparent institutions using better data, and enrich the public discussion about an agency’s value to society. 

  Read more essays in the Ideas to Retire series here.
			

--------------------------------------------------------------------------------

New article found for topic: data science
URL: https://www.brookings.edu/blog/future-development/2019/04/18/a-new-alphabet-for-europe-algorithms-big-data-and-the-computer-chip/
TITLE: A new alphabet for Europe: Algorithms, big data, and the computer chip
A new alphabet for Europe: Algorithms, big data, and the computer chip
BODY:

				If the biggest disrupter of the last few decades was Deng Xiaoping—the father of modern China—the big disrupter of the next few decades may well be John McCarthy. McCarthy, an American professor of Computer Science, is believed by many to be the father of artificial intelligence. Interestingly, the two have an epiphany in common. In 1979, Deng, a lifelong communist, visited the United States and came back a believer in market capitalism. In 1968, after a two-day visit to Czechoslovakia, McCarthy, who was raised as a communist by his immigrant parents, became a free-market Republican.
The ideas of computer scientists and mathematicians like McCarthy are radically transforming the way we communicate, and the way we make, buy, and sell goods and services. The changes will likely be so great that societies will have to reorganize government policy—rethinking how to regulate, what to subsidize, and whom to tax.	
		
				

	
			
		

	
							Wolfgang Fengler
		
							Lead Economist, Finance, Competitiveness and Innovation - World Bank
					
						Twitter
		wolfgangfengler
			
		
			
		
			

	

	
			
		

	
							Indermit Gill
		
							Nonresident Senior Fellow - Global Economy and Development
		
			
		
			

			
	
	
Change comes to Europe, again
Nowhere are these changes being considered more seriously than in Europe, where the technological transformations collectively called “Industry 4.0” are not unprecedented. At the end of the 19th century, as the Second Industrial Revolution unfolded in Western Europe, it brought mass unemployment in the countryside and squalor in the cities. Machines replaced peasants, who fled to cities and became the new urban poor. As they found new employment, mostly in the urban industrial sector, they joined a growing middle class. They would eventually transform Europe into a continent with the highest standards of living in the world.
A century later, Industry 4.0 technologies again threaten to replace their descendants with machines. But this time, Europe is a very different place. Europeans have changed from colonizers to traders, and the European Union replaced a clutch of warring nations. So even though technological change has precedent, it brings unprecedented imperatives. The European economic model puts a premium on social solidarity, and the European Union on political integration. Finding a way to remain competitive while juggling these social and political objectives has not been easy. Now Europe faces new questions. Will the new wave of digital technologies make this balancing act easier or impossible? Are Europeans better equipped than Asians and Americans to manage the trade-offs, or are these technologies coming when Europe is least prepared for them?  
In Europe 4.0, a World Bank research and policy project that we started earlier this year, we hope to answer these questions. Based on the work that we’ve completed, Europe’s workers, consumers, businesses, and policymakers will have to learn a new alphabet that will allow them to cohabit with these technologies. The ABC of the new world is Algorithms, Big data, and the Computer chip.
All technologies are not created equal
Today, there are more bits of data on Earth than grains of sand. Even though less than 1 percent of available data is actually being used, the leading companies are all data (or “tech”) companies. This is new. In 2000, there were three tech companies among the top 10 most valuable companies in the world. Today, the top seven are all tech companies: Five are American, two Chinese, none are European.
“So what?” you might ask. For one, it means that Europe is missing out on big profits. Data companies have significantly higher operating margins than traditional companies—20 percent higher, by one estimate. Higher profit margins inevitably mean bigger market shares. Europe risks being locked out of this new economy. With that will come a decline in economic power, and declining global influence. Most Europeans will not take this well. Nor should they.

			
			Related Content
		
		
		
	
		
			
																									
		
	
	
		
					Future Development
				Do digital technologies widen productivity gaps?
		
							Mary Hallward-Driemeier and Gaurav Nayyar
																Tuesday, April 23, 2019
					
		
		
			


	
		
			
																									
		
	
	
		
					Future Development
				Thriving with technology in Greece
		
							Christian Bodewig and Wolfgang Fengler
																Thursday, April 25, 2019
					
		
		
			

	
	

Like most things in economics, the drivers of this new data economy are costs. A closer look reveals that the new technologies are cutting three types of costs. The first is the cost of computing which has fallen so dramatically that almost everyone in Europe now has an affordable supercomputer in their pockets. The second is the decline in the cost of matching demand and supply through low-cost transaction platforms, such as Uber and Airbnb, or Fintech innovations like Kenya’s MPESA. The third is the cost of substituting workers with (much less fussy) robots. Robots are already common in parts of manufacturing such as automobiles, and becoming ever more ubiquitous in logistics and transportation.
Europe 4.0 organizes these technologies into three types (Figure 1):

Informational technologies that exploit the exponential growth of data. Examples include the internet of things, big data analytics, and cloud computing. The fundamental driver is the falling cost of computing. The main effect is to lower coordination costs.
Transactional technologies that digitize business models. Examples include the sharing economy, gig economy, digital platforms, and blockchain. The fundamental driver is the falling cost of matching demand and supply. The main effect is to reduce information asymmetries.
Operational technologies that combine data with automation. Examples include robotics, 3D printing, artificial intelligence, and machine learning. The fundamental driver is the falling cost of routine functions. The main effect is to reduce labor costs by automating activities.

Figure 1: Industry 4.0 technologies

It is not clear whether these technologies will lead to greater concentration of production in leading regions or countries, or in larger enterprises that use more capital-intensive forms of production. For Europe, it matters a lot whether they do.
A simple framework for Europe (and other places)
At present, Europe has a moderately strong presence in informational and operational technologies, but it is lagging in transactional technologies. Figure 2 summarizes our working hypotheses regarding the effects of Industry 4.0 technologies on Europe’s objectives of economic competitiveness, social inclusion, and political integration. Policies to encourage the adoption of informational technologies are likely to help with all three policy goals. Adopting new transactional technologies such as digital platforms will aid competitiveness and possibly help with social inclusion, but will likely lead to economic concentration in leading countries and regions. Operational technologies may be even more socially and spatially polarizing.  
Figure 2: Industry 4.0 technologies and Europe’s policy objectives

Industry 4.0 technologies are likely to be viewed quite differently in developing countries that care mostly about economic competitiveness (think China) compared with advanced economies like the United States where economic competitiveness and social inclusion are both important policy objectives. Spare a thought for the European Union, which must juggle the three objectives all at once.
			

--------------------------------------------------------------------------------


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

